{"id": "arxiv:2510.11719v1", "source": "arxiv", "title": "BayeSN-TD: Time Delay and $H_0$ Estimation for Lensed SN H0pe", "authors": ["M. Grayling", "S. Thorp", "K. S. Mandel", "M. Pascale", "J. D. R", " Pierel", "E. E. Hayes", "C. Larison", "A. Agrawal", "G. Narayan"], "year": 2025, "url": "http://arxiv.org/abs/2510.11719v1", "pdf_url": "http://arxiv.org/pdf/2510.11719v1", "summary": "We present BayeSN-TD, an enhanced implementation of the probabilistic type Ia\nsupernova (SN Ia) BayeSN SED model, designed for fitting multiply-imaged,\ngravitationally lensed type Ia supernovae (glSNe Ia). BayeSN-TD fits for\nmagnifications and time-delays across multiple images while marginalising over\nan achromatic, Gaussian process-based treatment of microlensing, to allow for\ntime-dependent deviations from a typical SN Ia SED caused by gravitational\nlensing by stars in the lensing system. BayeSN-TD is able to robustly infer\ntime delays and produce well-calibrated uncertainties, even when applied to\nsimulations based on a different SED model and incorporating chromatic\nmicrolensing, strongly validating its suitability for time-delay cosmography.\nWe then apply BayeSN-TD to publicly available photometry of the glSN Ia SN\nH0pe, inferring time delays between images BA and BC of $\\Delta\nT_{BA}=121.9^{+9.5}_{-7.5}$ days and $\\Delta T_{BC}=63.2^{+3.2}_{-3.3}$ days\nalong with absolute magnifications $\\beta$ for each image, $\\beta_A =\n2.38^{+0.72}_{-0.54}$, $\\beta_B=5.27^{+1.25}_{-1.02}$ and\n$\\beta_C=3.93^{+1.00}_{-0.75}$. Combining our constraints on time-delays and\nmagnifications with existing lens models of this system, we infer\n$H_0=69.3^{+12.6}_{-7.8}$ km s$^{-1}$ Mpc$^{-1}$, consistent with previous\nanalysis of this system; incorporating additional constraints based on\nspectroscopy yields $H_0=66.8^{+13.4}_{-5.4}$ km s$^{-1}$ Mpc$^{-1}$. While\nthis is not yet precise enough to draw a meaningful conclusion with regard to\nthe `Hubble tension', upcoming analysis of SN H0pe with more accurate\nphotometry enabled by template images, and other glSNe, will provide stronger\nconstraints on $H_0$; BayeSN-TD will be a valuable tool for these analyses."}
{"id": "arxiv:2510.11717v1", "source": "arxiv", "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams", "authors": ["Takuya Nakabayashi", "Navami Kairanda", "Hideo Saito", "Vladislav Golyanik"], "year": 2025, "url": "http://arxiv.org/abs/2510.11717v1", "pdf_url": "http://arxiv.org/pdf/2510.11717v1", "summary": "Event cameras offer various advantages for novel view rendering compared to\nsynchronously operating RGB cameras, and efficient event-based techniques\nsupporting rigid scenes have been recently demonstrated in the literature. In\nthe case of non-rigid objects, however, existing approaches additionally\nrequire sparse RGB inputs, which can be a substantial practical limitation; it\nremains unknown if similar models could be learned from event streams only.\nThis paper sheds light on this challenging open question and introduces Ev4DGS,\ni.e., the first approach for novel view rendering of non-rigidly deforming\nobjects in the explicit observation space (i.e., as RGB or greyscale images)\nfrom monocular event streams. Our method regresses a deformable 3D Gaussian\nSplatting representation through 1) a loss relating the outputs of the\nestimated model with the 2D event observation space, and 2) a coarse 3D\ndeformation model trained from binary masks generated from events. We perform\nexperimental comparisons on existing synthetic and newly recorded real datasets\nwith non-rigid objects. The results demonstrate the validity of Ev4DGS and its\nsuperior performance compared to multiple naive baselines that can be applied\nin our setting. We will release our models and the datasets used in the\nevaluation for research purposes; see the project webpage:\nhttps://4dqv.mpi-inf.mpg.de/Ev4DGS/."}
{"id": "arxiv:2510.11718v1", "source": "arxiv", "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images", "authors": ["Chengqi Duan", "Kaiyue Sun", "Rongyao Fang", "Manyuan Zhang", "Yan Feng", "Ying Luo", "Yufang Liu", "Ke Wang", "Peng Pei", "Xunliang Cai", "Hongsheng Li", "Yi Ma", "Xihui Liu"], "year": 2025, "url": "http://arxiv.org/abs/2510.11718v1", "pdf_url": "http://arxiv.org/pdf/2510.11718v1", "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT."}
{"id": "arxiv:2510.11716v1", "source": "arxiv", "title": "Meson-antimeson mixing", "authors": ["Ulrich Nierste"], "year": 2025, "url": "http://arxiv.org/abs/2510.11716v1", "pdf_url": "http://arxiv.org/pdf/2510.11716v1", "summary": "Meson-antimeson transitions are flavor-changing neutral current processes in\nwhich the strangeness, charm, or beauty quantum number changes by two units. In\nthe Standard Model (SM) these transitions originate from box diagrams with two\nW bosons. They permit the preparation of time-dependent, oscillating quantum\nstates which are superpositions of a meson and its antimeson. By studying their\ndecays one gains information on both the meson-antimeson mixing amplitude and\nthe decay amplitude involved and one can measure complex phases quantifying the\nviolation of charge-parity (CP) violation. I present a comprehensive overview\non the topic, starting with phenomenological presentations of $K$-$\\bar K$,\n$B_d$-$\\bar B_d$, $B_s$-$\\bar B_s$, and $D$-$\\bar D$ mixing. Highlights are the\ndiscovery of the violation of CP and other discrete symmetries, the predictions\nof the charm quark and its mass and a heavy top quark, and the confirmation of\nthe Kobayashi-Maskawa mechanism of CP violation. Further sections cover the\ntheoretical formalism needed to describe meson-antimeson mixing and to\ncalculate observables in terms of the fundamental parameters of the SM. I\ndiscuss the unitarity triangle of the Cabibbo-Kobayashi-Maskawa matrix, which\nis used to visualize how various CP-violating and CP-conserving quantities\ncombine to probe the SM. I describe the emergence of precision flavor physics\nand the role of reliable theory calculations to link $K$-$\\bar K$ mixing to\n$B_d$-$\\bar B_d$ mixing, which was essential to confirm the Kobayashi-Maskawa\nmechanism, and present the current status of theory predictions. Today, the\nfocus of the field is on physics beyond the SM, because meson-antimeson mixing\namplitudes are sensitive to virtual effects of heavy particles with masses\nwhich are several orders of magnitude above the reach of current particle\ncolliders."}
{"id": "arxiv:2510.11714v1", "source": "arxiv", "title": "Stochastic Homogenization of the Hamilton-Jacobi Equation on Manifolds", "authors": ["Marco Pozza", "Alfonso Sorrentino"], "year": 2025, "url": "http://arxiv.org/abs/2510.11714v1", "pdf_url": "http://arxiv.org/pdf/2510.11714v1", "summary": "This article establishes a stochastic homogenization result for the first\norder Hamilton-Jacobi equation on a Riemannian manifold $M$, in the context of\na stationary ergodic random environment. The setting involves a finitely\ngenerated abelian group $ \\mathtt{G}$ of rank $b$ acting on $M$ by isometries\nin a free, totally discontinuous, and co-compact manner, and a family of\nHamiltonians $H: T^*M \\times \\Omega \\to \\mathbb{R}$, parametrized over a\nprobability space $(\\Omega, \\mathbb{P})$, which are stationary with respect to\na $\\mathbb{P}$-ergodic action of $\\mathtt{G}$ on $\\Omega$. Under standard\nassumptions, including strict convexity and coercivity in the momentum\nvariable, we prove that as the scaling parameter $\\varepsilon$ goes to $0$, the\nviscosity solutions to the rescaled equation converge almost surely and locally\nuniformly to the solution to a deterministic homogenized Hamilton-Jacobi\nequation posed on $\\mathbb{R}^b$, which corresponds to the asymptotic cone of\n$\\mathtt{G}$. In particular, this approach sheds light on the relation between\nthe limit problem, the limit space, and the complexity of the acting group. The\nclassical periodic case corresponds to a randomness set $\\Omega$ that reduces\nto a singleton; other interesting examples of this setting are also described.\n  We remark that the effective Hamiltonian $\\overline{H}$ is obtained as the\nconvex conjugate of an effective Lagrangian $\\overline{L}$, which generalizes\nMather's $\\beta$-function to the stochastic setting; this represents a first\nstep towards the development of a stationary-ergodic version of Aubry-Mather\ntheory. As a geometric application, we introduce a notion of stable-like norm\nfor stationary ergodic families of Riemannian metrics on $M$, which generalizes\nthe classical Federer-Gromov's stable norm for closed manifolds."}
{"id": "arxiv:2510.11713v1", "source": "arxiv", "title": "Are Large Reasoning Models Interruptible?", "authors": ["Tsung-Han Wu", "Mihran Miroyan", "David M. Chan", "Trevor Darrell", "Narges Norouzi", "Joseph E. Gonzalez"], "year": 2025, "url": "http://arxiv.org/abs/2510.11713v1", "pdf_url": "http://arxiv.org/pdf/2510.11713v1", "summary": "Large Reasoning Models (LRMs) excel at complex reasoning but are\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\nassumed to be instantaneous, and the context of a request is presumed to be\nimmutable over the duration of the response. While generally true for\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\ntasks such as assistive programming, where models may take hours to think\nthrough problems and code may change dramatically from the time the model\nstarts thinking to the model's final output. In this work, we challenge the\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\nscenarios: interruptions, which test the quality of the model's partial outputs\non a limited budget, and dynamic context, which tests model adaptation to\nin-flight changes. Across mathematics and programming benchmarks that require\nlong-form reasoning, static evaluations consistently overestimate robustness:\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\nfail unpredictably when interrupted or exposed to changing context, with\nperformance dropping by up to 60% when updates are introduced late in the\nreasoning process. Our analysis further reveals several novel failure modes,\nincluding reasoning leakage, where models fold the reasoning into their final\nanswer when interrupted; panic, where under time pressure models abandon\nreasoning entirely and return incorrect answers; and self-doubt, where\nperformance degrades while incorporating updated information."}
{"id": "arxiv:2510.11712v1", "source": "arxiv", "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training", "authors": ["Haoran Feng", "Dizhe Zhang", "Xiangtai Li", "Bo Du", "Lu Qi"], "year": 2025, "url": "http://arxiv.org/abs/2510.11712v1", "pdf_url": "http://arxiv.org/pdf/2510.11712v1", "summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data, where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and\npanoramic refinement, which enhance perceptual quality while regularizing\ndiversity and photorealism. At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama, inpainting, and\noutpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/DiT360."}
{"id": "arxiv:2510.11711v1", "source": "arxiv", "title": "Reinforced sequential Monte Carlo for amortised sampling", "authors": ["Sanghyeok Choi", "Sarthak Mittal", "Víctor Elvira", "Jinkyoo Park", "Nikolay Malkin"], "year": 2025, "url": "http://arxiv.org/abs/2510.11711v1", "pdf_url": "http://arxiv.org/pdf/2510.11711v1", "summary": "This paper proposes a synergy of amortised and particle-based methods for\nsampling from distributions defined by unnormalised density functions. We state\na connection between sequential Monte Carlo (SMC) and neural sequential\nsamplers trained by maximum-entropy reinforcement learning (MaxEnt RL), wherein\nlearnt sampling policies and value functions define proposal kernels and twist\nfunctions. Exploiting this connection, we introduce an off-policy RL training\nprocedure for the sampler that uses samples from SMC -- using the learnt\nsampler as a proposal -- as a behaviour policy that better explores the target\ndistribution. We describe techniques for stable joint training of proposals and\ntwist functions and an adaptive weight tempering scheme to reduce training\nsignal variance. Furthermore, building upon past attempts to use experience\nreplay to guide the training of neural samplers, we derive a way to combine\nhistorical samples with annealed importance sampling weights within a replay\nbuffer. On synthetic multi-modal targets (in both continuous and discrete\nspaces) and the Boltzmann distribution of alanine dipeptide conformations, we\ndemonstrate improvements in approximating the true distribution as well as\ntraining stability compared to both amortised and Monte Carlo methods."}
{"id": "arxiv:2510.11710v1", "source": "arxiv", "title": "Comparing Symmetrized Determinant Neural Quantum States for the Hubbard Model", "authors": ["Louis Sharma", "Ahmedeo Shokry", "Rajah Nutakki", "Olivier Simard", "Michel Ferrero", "Filippo Vicentini"], "year": 2025, "url": "http://arxiv.org/abs/2510.11710v1", "pdf_url": "http://arxiv.org/pdf/2510.11710v1", "summary": "Accurate simulations of the Hubbard model are crucial to understanding\nstrongly correlated phenomena, where small energy differences between competing\norders demand high numerical precision. In this work, Neural Quantum States are\nused to probe the strongly coupled and underdoped regime of the square-lattice\nHubbard model. We systematically compare the Hidden Fermion Determinant State\nand the Jastrow-Backflow ansatz, parametrized by a Vision Transformer, finding\nthat in practice, their accuracy is similar. We also test different\nsymmetrization strategies, finding that output averaging yields the lowest\nenergies, though it becomes costly for larger system sizes. On cylindrical\nsystems, we consistently observe filled stripes. On the torus, our calculations\ndisplay features consistent with a doped Mott insulator, including\nantiferromagnetic correlations and suppressed density fluctuations. Our results\ndemonstrate both the promise and current challenges of neural quantum states\nfor correlated fermions."}
{"id": "arxiv:2510.11709v1", "source": "arxiv", "title": "Adversarial Attacks Leverage Interference Between Features in Superposition", "authors": ["Edward Stevinson", "Lucas Prieto", "Melih Barsbey", "Tolga Birdal"], "year": 2025, "url": "http://arxiv.org/abs/2510.11709v1", "pdf_url": "http://arxiv.org/pdf/2510.11709v1", "summary": "Fundamental questions remain about when and why adversarial examples arise in\nneural networks, with competing views characterising them either as artifacts\nof the irregularities in the decision landscape or as products of sensitivity\nto non-robust input features. In this paper, we instead argue that adversarial\nvulnerability can stem from efficient information encoding in neural networks.\nSpecifically, we show how superposition - where networks represent more\nfeatures than they have dimensions - creates arrangements of latent\nrepresentations that adversaries can exploit. We demonstrate that adversarial\nperturbations leverage interference between superposed features, making attack\npatterns predictable from feature arrangements. Our framework provides a\nmechanistic explanation for two known phenomena: adversarial attack\ntransferability between models with similar training regimes and class-specific\nvulnerability patterns. In synthetic settings with precisely controlled\nsuperposition, we establish that superposition suffices to create adversarial\nvulnerability. We then demonstrate that these findings persist in a ViT trained\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\nof networks' representational compression, rather than flaws in the learning\nprocess or non-robust inputs."}
{"id": "arxiv:2510.11719v1", "source": "arxiv", "title": "BayeSN-TD: Time Delay and $H_0$ Estimation for Lensed SN H0pe", "authors": ["M. Grayling", "S. Thorp", "K. S. Mandel", "M. Pascale", "J. D. R", " Pierel", "E. E. Hayes", "C. Larison", "A. Agrawal", "G. Narayan"], "year": 2025, "url": "http://arxiv.org/abs/2510.11719v1", "pdf_url": "http://arxiv.org/pdf/2510.11719v1", "summary": "We present BayeSN-TD, an enhanced implementation of the probabilistic type Ia\nsupernova (SN Ia) BayeSN SED model, designed for fitting multiply-imaged,\ngravitationally lensed type Ia supernovae (glSNe Ia). BayeSN-TD fits for\nmagnifications and time-delays across multiple images while marginalising over\nan achromatic, Gaussian process-based treatment of microlensing, to allow for\ntime-dependent deviations from a typical SN Ia SED caused by gravitational\nlensing by stars in the lensing system. BayeSN-TD is able to robustly infer\ntime delays and produce well-calibrated uncertainties, even when applied to\nsimulations based on a different SED model and incorporating chromatic\nmicrolensing, strongly validating its suitability for time-delay cosmography.\nWe then apply BayeSN-TD to publicly available photometry of the glSN Ia SN\nH0pe, inferring time delays between images BA and BC of $\\Delta\nT_{BA}=121.9^{+9.5}_{-7.5}$ days and $\\Delta T_{BC}=63.2^{+3.2}_{-3.3}$ days\nalong with absolute magnifications $\\beta$ for each image, $\\beta_A =\n2.38^{+0.72}_{-0.54}$, $\\beta_B=5.27^{+1.25}_{-1.02}$ and\n$\\beta_C=3.93^{+1.00}_{-0.75}$. Combining our constraints on time-delays and\nmagnifications with existing lens models of this system, we infer\n$H_0=69.3^{+12.6}_{-7.8}$ km s$^{-1}$ Mpc$^{-1}$, consistent with previous\nanalysis of this system; incorporating additional constraints based on\nspectroscopy yields $H_0=66.8^{+13.4}_{-5.4}$ km s$^{-1}$ Mpc$^{-1}$. While\nthis is not yet precise enough to draw a meaningful conclusion with regard to\nthe `Hubble tension', upcoming analysis of SN H0pe with more accurate\nphotometry enabled by template images, and other glSNe, will provide stronger\nconstraints on $H_0$; BayeSN-TD will be a valuable tool for these analyses."}
{"id": "arxiv:2510.11717v1", "source": "arxiv", "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams", "authors": ["Takuya Nakabayashi", "Navami Kairanda", "Hideo Saito", "Vladislav Golyanik"], "year": 2025, "url": "http://arxiv.org/abs/2510.11717v1", "pdf_url": "http://arxiv.org/pdf/2510.11717v1", "summary": "Event cameras offer various advantages for novel view rendering compared to\nsynchronously operating RGB cameras, and efficient event-based techniques\nsupporting rigid scenes have been recently demonstrated in the literature. In\nthe case of non-rigid objects, however, existing approaches additionally\nrequire sparse RGB inputs, which can be a substantial practical limitation; it\nremains unknown if similar models could be learned from event streams only.\nThis paper sheds light on this challenging open question and introduces Ev4DGS,\ni.e., the first approach for novel view rendering of non-rigidly deforming\nobjects in the explicit observation space (i.e., as RGB or greyscale images)\nfrom monocular event streams. Our method regresses a deformable 3D Gaussian\nSplatting representation through 1) a loss relating the outputs of the\nestimated model with the 2D event observation space, and 2) a coarse 3D\ndeformation model trained from binary masks generated from events. We perform\nexperimental comparisons on existing synthetic and newly recorded real datasets\nwith non-rigid objects. The results demonstrate the validity of Ev4DGS and its\nsuperior performance compared to multiple naive baselines that can be applied\nin our setting. We will release our models and the datasets used in the\nevaluation for research purposes; see the project webpage:\nhttps://4dqv.mpi-inf.mpg.de/Ev4DGS/."}
{"id": "arxiv:2510.11718v1", "source": "arxiv", "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images", "authors": ["Chengqi Duan", "Kaiyue Sun", "Rongyao Fang", "Manyuan Zhang", "Yan Feng", "Ying Luo", "Yufang Liu", "Ke Wang", "Peng Pei", "Xunliang Cai", "Hongsheng Li", "Yi Ma", "Xihui Liu"], "year": 2025, "url": "http://arxiv.org/abs/2510.11718v1", "pdf_url": "http://arxiv.org/pdf/2510.11718v1", "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT."}
{"id": "arxiv:2510.11716v1", "source": "arxiv", "title": "Meson-antimeson mixing", "authors": ["Ulrich Nierste"], "year": 2025, "url": "http://arxiv.org/abs/2510.11716v1", "pdf_url": "http://arxiv.org/pdf/2510.11716v1", "summary": "Meson-antimeson transitions are flavor-changing neutral current processes in\nwhich the strangeness, charm, or beauty quantum number changes by two units. In\nthe Standard Model (SM) these transitions originate from box diagrams with two\nW bosons. They permit the preparation of time-dependent, oscillating quantum\nstates which are superpositions of a meson and its antimeson. By studying their\ndecays one gains information on both the meson-antimeson mixing amplitude and\nthe decay amplitude involved and one can measure complex phases quantifying the\nviolation of charge-parity (CP) violation. I present a comprehensive overview\non the topic, starting with phenomenological presentations of $K$-$\\bar K$,\n$B_d$-$\\bar B_d$, $B_s$-$\\bar B_s$, and $D$-$\\bar D$ mixing. Highlights are the\ndiscovery of the violation of CP and other discrete symmetries, the predictions\nof the charm quark and its mass and a heavy top quark, and the confirmation of\nthe Kobayashi-Maskawa mechanism of CP violation. Further sections cover the\ntheoretical formalism needed to describe meson-antimeson mixing and to\ncalculate observables in terms of the fundamental parameters of the SM. I\ndiscuss the unitarity triangle of the Cabibbo-Kobayashi-Maskawa matrix, which\nis used to visualize how various CP-violating and CP-conserving quantities\ncombine to probe the SM. I describe the emergence of precision flavor physics\nand the role of reliable theory calculations to link $K$-$\\bar K$ mixing to\n$B_d$-$\\bar B_d$ mixing, which was essential to confirm the Kobayashi-Maskawa\nmechanism, and present the current status of theory predictions. Today, the\nfocus of the field is on physics beyond the SM, because meson-antimeson mixing\namplitudes are sensitive to virtual effects of heavy particles with masses\nwhich are several orders of magnitude above the reach of current particle\ncolliders."}
{"id": "arxiv:2510.11714v1", "source": "arxiv", "title": "Stochastic Homogenization of the Hamilton-Jacobi Equation on Manifolds", "authors": ["Marco Pozza", "Alfonso Sorrentino"], "year": 2025, "url": "http://arxiv.org/abs/2510.11714v1", "pdf_url": "http://arxiv.org/pdf/2510.11714v1", "summary": "This article establishes a stochastic homogenization result for the first\norder Hamilton-Jacobi equation on a Riemannian manifold $M$, in the context of\na stationary ergodic random environment. The setting involves a finitely\ngenerated abelian group $ \\mathtt{G}$ of rank $b$ acting on $M$ by isometries\nin a free, totally discontinuous, and co-compact manner, and a family of\nHamiltonians $H: T^*M \\times \\Omega \\to \\mathbb{R}$, parametrized over a\nprobability space $(\\Omega, \\mathbb{P})$, which are stationary with respect to\na $\\mathbb{P}$-ergodic action of $\\mathtt{G}$ on $\\Omega$. Under standard\nassumptions, including strict convexity and coercivity in the momentum\nvariable, we prove that as the scaling parameter $\\varepsilon$ goes to $0$, the\nviscosity solutions to the rescaled equation converge almost surely and locally\nuniformly to the solution to a deterministic homogenized Hamilton-Jacobi\nequation posed on $\\mathbb{R}^b$, which corresponds to the asymptotic cone of\n$\\mathtt{G}$. In particular, this approach sheds light on the relation between\nthe limit problem, the limit space, and the complexity of the acting group. The\nclassical periodic case corresponds to a randomness set $\\Omega$ that reduces\nto a singleton; other interesting examples of this setting are also described.\n  We remark that the effective Hamiltonian $\\overline{H}$ is obtained as the\nconvex conjugate of an effective Lagrangian $\\overline{L}$, which generalizes\nMather's $\\beta$-function to the stochastic setting; this represents a first\nstep towards the development of a stationary-ergodic version of Aubry-Mather\ntheory. As a geometric application, we introduce a notion of stable-like norm\nfor stationary ergodic families of Riemannian metrics on $M$, which generalizes\nthe classical Federer-Gromov's stable norm for closed manifolds."}
{"id": "arxiv:2510.11713v1", "source": "arxiv", "title": "Are Large Reasoning Models Interruptible?", "authors": ["Tsung-Han Wu", "Mihran Miroyan", "David M. Chan", "Trevor Darrell", "Narges Norouzi", "Joseph E. Gonzalez"], "year": 2025, "url": "http://arxiv.org/abs/2510.11713v1", "pdf_url": "http://arxiv.org/pdf/2510.11713v1", "summary": "Large Reasoning Models (LRMs) excel at complex reasoning but are\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\nassumed to be instantaneous, and the context of a request is presumed to be\nimmutable over the duration of the response. While generally true for\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\ntasks such as assistive programming, where models may take hours to think\nthrough problems and code may change dramatically from the time the model\nstarts thinking to the model's final output. In this work, we challenge the\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\nscenarios: interruptions, which test the quality of the model's partial outputs\non a limited budget, and dynamic context, which tests model adaptation to\nin-flight changes. Across mathematics and programming benchmarks that require\nlong-form reasoning, static evaluations consistently overestimate robustness:\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\nfail unpredictably when interrupted or exposed to changing context, with\nperformance dropping by up to 60% when updates are introduced late in the\nreasoning process. Our analysis further reveals several novel failure modes,\nincluding reasoning leakage, where models fold the reasoning into their final\nanswer when interrupted; panic, where under time pressure models abandon\nreasoning entirely and return incorrect answers; and self-doubt, where\nperformance degrades while incorporating updated information."}
{"id": "arxiv:2510.11712v1", "source": "arxiv", "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training", "authors": ["Haoran Feng", "Dizhe Zhang", "Xiangtai Li", "Bo Du", "Lu Qi"], "year": 2025, "url": "http://arxiv.org/abs/2510.11712v1", "pdf_url": "http://arxiv.org/pdf/2510.11712v1", "summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data, where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and\npanoramic refinement, which enhance perceptual quality while regularizing\ndiversity and photorealism. At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama, inpainting, and\noutpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/DiT360."}
{"id": "arxiv:2510.11711v1", "source": "arxiv", "title": "Reinforced sequential Monte Carlo for amortised sampling", "authors": ["Sanghyeok Choi", "Sarthak Mittal", "Víctor Elvira", "Jinkyoo Park", "Nikolay Malkin"], "year": 2025, "url": "http://arxiv.org/abs/2510.11711v1", "pdf_url": "http://arxiv.org/pdf/2510.11711v1", "summary": "This paper proposes a synergy of amortised and particle-based methods for\nsampling from distributions defined by unnormalised density functions. We state\na connection between sequential Monte Carlo (SMC) and neural sequential\nsamplers trained by maximum-entropy reinforcement learning (MaxEnt RL), wherein\nlearnt sampling policies and value functions define proposal kernels and twist\nfunctions. Exploiting this connection, we introduce an off-policy RL training\nprocedure for the sampler that uses samples from SMC -- using the learnt\nsampler as a proposal -- as a behaviour policy that better explores the target\ndistribution. We describe techniques for stable joint training of proposals and\ntwist functions and an adaptive weight tempering scheme to reduce training\nsignal variance. Furthermore, building upon past attempts to use experience\nreplay to guide the training of neural samplers, we derive a way to combine\nhistorical samples with annealed importance sampling weights within a replay\nbuffer. On synthetic multi-modal targets (in both continuous and discrete\nspaces) and the Boltzmann distribution of alanine dipeptide conformations, we\ndemonstrate improvements in approximating the true distribution as well as\ntraining stability compared to both amortised and Monte Carlo methods."}
{"id": "arxiv:2510.11710v1", "source": "arxiv", "title": "Comparing Symmetrized Determinant Neural Quantum States for the Hubbard Model", "authors": ["Louis Sharma", "Ahmedeo Shokry", "Rajah Nutakki", "Olivier Simard", "Michel Ferrero", "Filippo Vicentini"], "year": 2025, "url": "http://arxiv.org/abs/2510.11710v1", "pdf_url": "http://arxiv.org/pdf/2510.11710v1", "summary": "Accurate simulations of the Hubbard model are crucial to understanding\nstrongly correlated phenomena, where small energy differences between competing\norders demand high numerical precision. In this work, Neural Quantum States are\nused to probe the strongly coupled and underdoped regime of the square-lattice\nHubbard model. We systematically compare the Hidden Fermion Determinant State\nand the Jastrow-Backflow ansatz, parametrized by a Vision Transformer, finding\nthat in practice, their accuracy is similar. We also test different\nsymmetrization strategies, finding that output averaging yields the lowest\nenergies, though it becomes costly for larger system sizes. On cylindrical\nsystems, we consistently observe filled stripes. On the torus, our calculations\ndisplay features consistent with a doped Mott insulator, including\nantiferromagnetic correlations and suppressed density fluctuations. Our results\ndemonstrate both the promise and current challenges of neural quantum states\nfor correlated fermions."}
{"id": "arxiv:2510.11709v1", "source": "arxiv", "title": "Adversarial Attacks Leverage Interference Between Features in Superposition", "authors": ["Edward Stevinson", "Lucas Prieto", "Melih Barsbey", "Tolga Birdal"], "year": 2025, "url": "http://arxiv.org/abs/2510.11709v1", "pdf_url": "http://arxiv.org/pdf/2510.11709v1", "summary": "Fundamental questions remain about when and why adversarial examples arise in\nneural networks, with competing views characterising them either as artifacts\nof the irregularities in the decision landscape or as products of sensitivity\nto non-robust input features. In this paper, we instead argue that adversarial\nvulnerability can stem from efficient information encoding in neural networks.\nSpecifically, we show how superposition - where networks represent more\nfeatures than they have dimensions - creates arrangements of latent\nrepresentations that adversaries can exploit. We demonstrate that adversarial\nperturbations leverage interference between superposed features, making attack\npatterns predictable from feature arrangements. Our framework provides a\nmechanistic explanation for two known phenomena: adversarial attack\ntransferability between models with similar training regimes and class-specific\nvulnerability patterns. In synthetic settings with precisely controlled\nsuperposition, we establish that superposition suffices to create adversarial\nvulnerability. We then demonstrate that these findings persist in a ViT trained\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\nof networks' representational compression, rather than flaws in the learning\nprocess or non-robust inputs."}
{"id": "arxiv:2510.07910v1", "source": "arxiv", "title": "MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation", "authors": ["Chongmyung Kwon", "Yujin Kim", "Seoeun Park", "Yunji Lee", "Charmgil Hong"], "year": 2025, "url": "http://arxiv.org/abs/2510.07910v1", "pdf_url": "http://arxiv.org/pdf/2510.07910v1", "summary": "Drug recommendation is an essential task in machine learning-based clinical\ndecision support systems. However, the risk of drug-drug interactions (DDI)\nbetween co-prescribed medications remains a significant challenge. Previous\nstudies have used graph neural networks (GNNs) to represent drug structures.\nRegardless, their simplified discrete forms cannot fully capture the molecular\nbinding affinity and reactivity. Therefore, we propose Multimodal DDI\nPrediction with Molecular Electron Localization Function (ELF) Maps (MMM), a\nnovel framework that integrates three-dimensional (3D) quantum-chemical\ninformation into drug representation learning. It generates 3D electron density\nmaps using the ELF. To capture both therapeutic relevance and interaction\nrisks, MMM combines ELF-derived features that encode global electronic\nproperties with a bipartite graph encoder that models local substructure\ninteractions. This design enables learning complementary characteristics of\ndrug molecules. We evaluate MMM in the MIMIC-III dataset (250 drugs, 442\nsubstructures), comparing it with several baseline models. In particular, a\ncomparison with the GNN-based SafeDrug model demonstrates statistically\nsignificant improvements in the F1-score (p = 0.0387), Jaccard (p = 0.0112),\nand the DDI rate (p = 0.0386). These results demonstrate the potential of\nELF-based 3D representations to enhance prediction accuracy and support safer\ncombinatorial drug prescribing in clinical practice."}
{"id": "arxiv:2509.22468v1", "source": "arxiv", "title": "Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised Molecular Graph Pretraining", "authors": ["Boshra Ariguib", "Mathias Niepert", "Andrei Manolache"], "year": 2025, "url": "http://arxiv.org/abs/2509.22468v1", "pdf_url": "http://arxiv.org/pdf/2509.22468v1", "summary": "High-quality molecular representations are essential for property prediction\nand molecular design, yet large labeled datasets remain scarce. While\nself-supervised pretraining on molecular graphs has shown promise, many\nexisting approaches either depend on hand-crafted augmentations or complex\ngenerative objectives, and often rely solely on 2D topology, leaving valuable\n3D structural information underutilized. To address this gap, we introduce\nC-FREE (Contrast-Free Representation learning on Ego-nets), a simple framework\nthat integrates 2D graphs with ensembles of 3D conformers. C-FREE learns\nmolecular representations by predicting subgraph embeddings from their\ncomplementary neighborhoods in the latent space, using fixed-radius ego-nets as\nmodeling units across different conformers. This design allows us to integrate\nboth geometric and topological information within a hybrid Graph Neural Network\n(GNN)-Transformer backbone, without negatives, positional encodings, or\nexpensive pre-processing. Pretraining on the GEOM dataset, which provides rich\n3D conformational diversity, C-FREE achieves state-of-the-art results on\nMoleculeNet, surpassing contrastive, generative, and other multimodal\nself-supervised methods. Fine-tuning across datasets with diverse sizes and\nmolecule types further demonstrates that pretraining transfers effectively to\nnew chemical domains, highlighting the importance of 3D-informed molecular\nrepresentations."}
{"id": "arxiv:2509.22028v1", "source": "arxiv", "title": "MCGM: Multi-stage Clustered Global Modeling for Long-range Interactions in Molecules", "authors": ["Haodong Pan", "Yusong Wang", "Nanning Zheng", "Caijui Jiang"], "year": 2025, "url": "http://arxiv.org/abs/2509.22028v1", "pdf_url": "http://arxiv.org/pdf/2509.22028v1", "summary": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance."}
{"id": "arxiv:2509.17018v1", "source": "arxiv", "title": "DeepEOSNet: Capturing the dependency on thermodynamic state in property prediction tasks", "authors": ["Jan Pavšek", "Alexander Mitsos", "Manuel Dahmen", "Tai Xuan Tan", "Jan G. Rittig"], "year": 2025, "url": "http://arxiv.org/abs/2509.17018v1", "pdf_url": "http://arxiv.org/pdf/2509.17018v1", "summary": "We propose a machine learning (ML) architecture to better capture the\ndependency of thermodynamic properties on the independent states. When\npredicting state-dependent thermodynamic properties, ML models need to account\nfor both molecular structure and the thermodynamic state, described by\nindependent variables, typically temperature, pressure, and composition. Modern\nmolecular ML models typically include state information by adding it to\nmolecular fingerprint vectors or by embedding explicit (semi-empirical)\nthermodynamic relations. Here, we propose to rather split the information\nprocessing on the molecular structure and the dependency on states into two\nseparate network channels: a graph neural network and a multilayer perceptron,\nwhose output is combined by a dot product. We refer to our approach as\nDeepEOSNet, as this idea is based on the DeepONet architecture [Lu et al.\n(2021), Nat. Mach. Intell.]: instead of operators, we learn state dependencies,\nwith the possibility to predict equation of states (EOS). We investigate the\npredictive performance of DeepEOSNet by means of three case studies, which\ninclude the prediction of vapor pressure as a function of temperature, and\nmixture molar volume as a function of composition, temperature, and pressure.\nOur results show superior performance of DeepEOSNet for predicting vapor\npressure and comparable performance for predicting mixture molar volume\ncompared to state-of-research graph-based thermodynamic prediction models from\nour earlier works. In fact, we see large potential of DeepEOSNet in cases where\ndata is sparse in the state domain and the output function is structurally\nsimilar across different molecules. The concept of DeepEOSNet can easily be\ntransferred to other ML architectures in molecular context, and thus provides a\nviable option for property prediction."}
{"id": "arxiv:2509.11782v1", "source": "arxiv", "title": "Multimodal Regression for Enzyme Turnover Rates Prediction", "authors": ["Bozhen Hu", "Cheng Tan", "Siyuan Li", "Jiangbin Zheng", "Sizhe Qiu", "Jun Xia", "Stan Z. Li"], "year": 2025, "url": "http://arxiv.org/abs/2509.11782v1", "pdf_url": "http://arxiv.org/pdf/2509.11782v1", "summary": "The enzyme turnover rate is a fundamental parameter in enzyme kinetics,\nreflecting the catalytic efficiency of enzymes. However, enzyme turnover rates\nremain scarce across most organisms due to the high cost and complexity of\nexperimental measurements. To address this gap, we propose a multimodal\nframework for predicting the enzyme turnover rate by integrating enzyme\nsequences, substrate structures, and environmental factors. Our model combines\na pre-trained language model and a convolutional neural network to extract\nfeatures from protein sequences, while a graph neural network captures\ninformative representations from substrate molecules. An attention mechanism is\nincorporated to enhance interactions between enzyme and substrate\nrepresentations. Furthermore, we leverage symbolic regression via\nKolmogorov-Arnold Networks to explicitly learn mathematical formulas that\ngovern the enzyme turnover rate, enabling interpretable and accurate\npredictions. Extensive experiments demonstrate that our framework outperforms\nboth traditional and state-of-the-art deep learning approaches. This work\nprovides a robust tool for studying enzyme kinetics and holds promise for\napplications in enzyme engineering, biotechnology, and industrial biocatalysis."}
{"id": "arxiv:2509.07887v1", "source": "arxiv", "title": "A Survey of Graph Neural Networks for Drug Discovery: Recent Developments and Challenges", "authors": ["Katherine Berry", "Liang Cheng"], "year": 2025, "url": "http://arxiv.org/abs/2509.07887v1", "pdf_url": "http://arxiv.org/pdf/2509.07887v1", "summary": "Graph Neural Networks (GNNs) have gained traction in the complex domain of\ndrug discovery because of their ability to process graph-structured data such\nas drug molecule models. This approach has resulted in a myriad of methods and\nmodels in published literature across several categories of drug discovery\nresearch. This paper covers the research categories comprehensively with recent\npapers, namely molecular property prediction, including drug-target binding\naffinity prediction, drug-drug interaction study, microbiome interaction\nprediction, drug repositioning, retrosynthesis, and new drug design, and\nprovides guidance for future work on GNNs for drug discovery."}
{"id": "arxiv:2503.07397v1", "source": "arxiv", "title": "Q-MARL: A quantum-inspired algorithm using neural message passing for large-scale multi-agent reinforcement learning", "authors": ["Kha Vo", "Chin-Teng Lin"], "year": 2025, "url": "http://arxiv.org/abs/2503.07397v1", "pdf_url": "http://arxiv.org/pdf/2503.07397v1", "summary": "Inspired by a graph-based technique for predicting molecular properties in\nquantum chemistry -- atoms' position within molecules in three-dimensional\nspace -- we present Q-MARL, a completely decentralised learning architecture\nthat supports very large-scale multi-agent reinforcement learning scenarios\nwithout the need for strong assumptions like common rewards or agent order. The\nkey is to treat each agent as relative to its surrounding agents in an\nenvironment that is presumed to change dynamically. Hence, in each time step,\nan agent is the centre of its own neighbourhood and also a neighbour to many\nother agents. Each role is formulated as a sub-graph, and each sub-graph is\nused as a training sample. A message-passing neural network supports full-scale\nvertex and edge interaction within a local neighbourhood, while a parameter\ngoverning the depth of the sub-graphs eases the training burden. During\ntesting, an agent's actions are locally ensembled across all the sub-graphs\nthat contain it, resulting in robust decisions. Where other approaches struggle\nto manage 50 agents, Q-MARL can easily marshal thousands. A detailed\ntheoretical analysis proves improvement and convergence, and simulations with\nthe typical collaborative and competitive scenarios show dramatically faster\ntraining speeds and reduced training losses."}
{"id": "arxiv:2501.18876v1", "source": "arxiv", "title": "QMe14S, A Comprehensive and Efficient Spectral Dataset for Small Organic Molecules", "authors": ["Mingzhi Yuan", "Zihan Zou", "Wei Hu"], "year": 2025, "url": "http://arxiv.org/abs/2501.18876v1", "pdf_url": "http://arxiv.org/pdf/2501.18876v1", "summary": "Developing machine learning protocols for molecular simulations requires\ncomprehensive and efficient datasets. Here we introduce the QMe14S dataset,\ncomprising 186,102 small organic molecules featuring 14 elements (H, B, C, N,\nO, F, Al, Si, P, S, Cl, As, Se, Br) and 47 functional groups. Using density\nfunctional theory at the B3LYP/TZVP level, we optimized the geometries and\ncalculated properties including energy, atomic charge, atomic force, dipole\nmoment, quadrupole moment, polarizability, octupole moment, first\nhyperpolarizability, and Hessian. At the same level, we obtained the harmonic\nIR, Raman and NMR spectra. Furthermore, we conducted ab initio molecular\ndynamics simulations to generate dynamic configurations and extract\nnonequilibrium properties, including energy, forces, and Hessians. By\nleveraging our E(3)-equivariant message-passing neural network (DetaNet), we\ndemonstrated that models trained on QMe14S outperform those trained on the\npreviously developed QM9S dataset in simulating molecular spectra. The QMe14S\ndataset thus serves as a comprehensive benchmark for molecular simulations,\noffering valuable insights into structure-property relationships."}
{"id": "arxiv:2412.01982v2", "source": "arxiv", "title": "Pooling Solvent Mixtures for Solvation Free Energy Predictions", "authors": ["Roel J. Leenhouts", "Nathan Morgan", "Emad Al Ibrahim", "William H. Green", "Florence H. Vermeire"], "year": 2024, "url": "http://arxiv.org/abs/2412.01982v2", "pdf_url": "http://arxiv.org/pdf/2412.01982v2", "summary": "Solvation free energy is an important design parameter in reaction kinetics\nand separation processes, making it a critical property to predict during\nprocess development. In previous research, directed message passing neural\nnetworks (D-MPNN) have successfully been used to predict solvation free\nenergies and enthalpies in organic solvents. However, solvent mixtures provide\ngreater flexibility for optimizing solvent interactions than monosolvents. This\nwork aims to extend our previous models to mixtures. To handle mixtures in a\npermutation invariant manner we propose a pooling function; MolPool. With this\npooling function, the machine learning models can learn and predict properties\nfor an arbitrary number of molecules. The novel SolProp-mix software that\napplies MolPool to D-MPNN was compared to state-of-the-art architectures for\npredicting mixture properties and validated with our new database of COSMOtherm\ncalculations; BinarySolv-QM. To improve predictions towards experimental\naccuracy, the network was then fine-tuned on experimental data in monosolvents.\nTo demonstrate the benefit of this transfer learning methodology, experimental\ndatasets of solvation free energies in binary (BinarySolv-Exp) and ternary\n(TernarySolv-Exp) solvent mixtures were compiled from data on vapor-liquid\nequilibria and activity coefficients. The neural network performed better than\nCOSMOtherm calculations with an MAE of 0.25 kcal/mol and an RMSE of 0.37\nkcal/mol for non-aqueous mixed solvents. Additionally, the ability to capture\ntrends for a varying mixture composition was validated successfully. Our\nmodel's ability to accurately predict mixture properties from the combination\nof in silico data and pure component experimental data is promising given the\nscarcity of experimental data for mixtures in many fields."}
{"id": "arxiv:2410.16975v1", "source": "arxiv", "title": "Publishing Neural Networks in Drug Discovery Might Compromise Training Data Privacy", "authors": ["Fabian P. Krüger", "Johan Östman", "Lewis Mervin", "Igor V. Tetko", "Ola Engkvist"], "year": 2024, "url": "http://arxiv.org/abs/2410.16975v1", "pdf_url": "http://arxiv.org/pdf/2410.16975v1", "summary": "This study investigates the risks of exposing confidential chemical\nstructures when machine learning models trained on these structures are made\npublicly available. We use membership inference attacks, a common method to\nassess privacy that is largely unexplored in the context of drug discovery, to\nexamine neural networks for molecular property prediction in a black-box\nsetting. Our results reveal significant privacy risks across all evaluated\ndatasets and neural network architectures. Combining multiple attacks increases\nthese risks. Molecules from minority classes, often the most valuable in drug\ndiscovery, are particularly vulnerable. We also found that representing\nmolecules as graphs and using message-passing neural networks may mitigate\nthese risks. We provide a framework to assess privacy risks of classification\nmodels and molecular representations. Our findings highlight the need for\ncareful consideration when sharing neural networks trained on proprietary\nchemical structures, informing organisations and researchers about the\ntrade-offs between data confidentiality and model openness."}
{"id": "arxiv:2410.07981v2", "source": "arxiv", "title": "MolMix: A Simple Yet Effective Baseline for Multimodal Molecular Representation Learning", "authors": ["Andrei Manolache", "Dragos Tantaru", "Mathias Niepert"], "year": 2024, "url": "http://arxiv.org/abs/2410.07981v2", "pdf_url": "http://arxiv.org/pdf/2410.07981v2", "summary": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning."}
{"id": "arxiv:2409.18070v1", "source": "arxiv", "title": "Prediction of the Infrared Absorbance Intensities and Frequencies of Hydrocarbons:A Message Passing Neural Network Approach", "authors": ["Maliheh Shaban Tameh", "Veaceslav Coropceanu", "Thomas A. R. Purcell", "Jean-Luc Brédas"], "year": 2024, "url": "http://arxiv.org/abs/2409.18070v1", "pdf_url": "http://arxiv.org/pdf/2409.18070v1", "summary": "Accurately and efficiently predicting the infrared (IR) spectra of a molecule\ncan provide insights into the structure-properties relationships of molecular\nspecies, which has led to a proliferation of machine learning tools designed\nfor this purpose. However, earlier studies have focused primarily on obtaining\nnormalized IR spectra, which limits their potential for a comprehensive\nanalysis of molecular behavior in the IR range. For instance, to fully\nunderstand and predict the optical properties, such as the transparency\ncharacteristics, it is necessary to predict the molar absorptivity IR spectra\ninstead. Here, we propose a graph-based communicative message passing neural\nnetwork (CMPNN) algorithm that can predict both the peak positions and absolute\nintensities corresponding to density functional theory (DFT) calculated molar\nabsorptivities in the IR domain. By modifying existing spectral loss functions,\nwe show that our method is able to predict with DFT-accuracy level the IR molar\nabsorptivities of a series of hydrocarbons containing up to ten carbon atoms\nand apply the model to a set of larger molecules. We also compare the predicted\nspectra with those generated by the direct message passing neural network\n(DMPNN). The results suggest that both algorithms demonstrate similar\npredictive capabilities for hydrocarbons, indicating that either model could be\neffectively used in future research on spectral prediction for such systems."}
{"id": "arxiv:2510.11286v1", "source": "arxiv", "title": "Edge-to-Cloud Computations-as-a-Service in Software-Defined Energy Networks for Smart Grids", "authors": ["Jack Jackman", "David Ryan", "Arun Narayanan", "Pedro Nardelli", "Indrakshi Dey"], "year": 2025, "url": "http://arxiv.org/abs/2510.11286v1", "pdf_url": "http://arxiv.org/pdf/2510.11286v1", "summary": "Modern power grids face an acute mismatch between where data is generated and\nwhere it can be processed: protection relays, EV (Electric Vehicle) charging,\nand distributed renewables demand millisecond analytics at the edge, while\nenergy-hungry workloads often sit in distant clouds leading to missed real-time\ndeadlines and wasted power. We address this by proposing, to our knowledge, the\nfirst-ever SDEN (Software Defined Energy Network) for CaaS\n(Computations-as-a-Service) that unifies edge, fog, and cloud compute with 5G\nURLLC (Ultra-Reliable Low-Latency Communications), SDN (Software Defined\nNetworking), and NFV (Network Functions Virtualization) to co-optimize energy,\nlatency, and reliability end-to-end. Our contributions are threefold: (i) a\njoint task offloading formulation that couples computation placement with\nnetwork capacity under explicit URLLC constraints; (ii) a feasibility\npreserving, lightweight greedy heuristic that scales while closely tracking\noptimal energy and latency trade-offs; and (iii) a tiered AI (Artificial\nIntelligence) pipeline-reactive at the edge, predictive in the fog, strategic\nin the cloud-featuring privacy-preserving, federated GNNs (Graph Neural\nNetworks) for fault detection and microgrid coordination. Unlike prior\nedge-only or cloud-only schemes, SDEN turns fragmented grid compute into a\nsingle, programmable substrate that delivers dependable, energy-aware, real\ntime analytics establishing a first-ever, software defined path to practical,\ngrid-scale CaaS."}
{"id": "arxiv:2510.10954v1", "source": "arxiv", "title": "Comparative Evaluation of Neural Network Architectures for Generalizable Human Spatial Preference Prediction in Unseen Built Environments", "authors": ["Maral Doctorarastoo", "Katherine A. Flanigan", "Mario Bergés", "Christopher McComb"], "year": 2025, "url": "http://arxiv.org/abs/2510.10954v1", "pdf_url": "http://arxiv.org/pdf/2510.10954v1", "summary": "The capacity to predict human spatial preferences within built environments\nis instrumental for developing Cyber-Physical-Social Infrastructure Systems\n(CPSIS). A significant challenge in this domain is the generalizability of\npreference models, particularly their efficacy in predicting preferences within\nenvironmental configurations not encountered during training. While deep\nlearning models have shown promise in learning complex spatial and contextual\ndependencies, it remains unclear which neural network architectures are most\neffective at generalizing to unseen layouts. To address this, we conduct a\ncomparative study of Graph Neural Networks, Convolutional Neural Networks, and\nstandard feedforward Neural Networks using synthetic data generated from a\nsimplified and synthetic pocket park environment. Beginning with this\nillustrative case study, allows for controlled analysis of each model's ability\nto transfer learned preference patterns to unseen spatial scenarios. The models\nare evaluated based on their capacity to predict preferences influenced by\nheterogeneous physical, environmental, and social features. Generalizability\nscore is calculated using the area under the precision-recall curve for the\nseen and unseen layouts. This generalizability score is appropriate for\nimbalanced data, providing insights into the suitability of each neural network\narchitecture for preference-aware human behavior modeling in unseen built\nenvironments."}
{"id": "arxiv:2510.10775v1", "source": "arxiv", "title": "Structure Over Signal: A Globalized Approach to Multi-relational GNNs for Stock Prediction", "authors": ["Amber Li", "Aruzhan Abil", "Juno Marques Oda"], "year": 2025, "url": "http://arxiv.org/abs/2510.10775v1", "pdf_url": "http://arxiv.org/pdf/2510.10775v1", "summary": "In financial markets, Graph Neural Networks have been successfully applied to\nmodeling relational data, effectively capturing nonlinear inter-stock\ndependencies. Yet, existing models often fail to efficiently propagate messages\nduring macroeconomic shocks. In this paper, we propose OmniGNN, an\nattention-based multi-relational dynamic GNN that integrates macroeconomic\ncontext via heterogeneous node and edge types for robust message passing.\nCentral to OmniGNN is a sector node acting as a global intermediary, enabling\nrapid shock propagation across the graph without relying on long-range\nmulti-hop diffusion. The model leverages Graph Attention Networks (GAT) to\nweigh neighbor contributions and employs Transformers to capture temporal\ndynamics across multiplex relations. Experiments show that OmniGNN outperforms\nexisting stock prediction models on public datasets, particularly demonstrating\nstrong robustness during the COVID-19 period."}
{"id": "arxiv:2510.10341v1", "source": "arxiv", "title": "Multi-View Graph Learning with Graph-Tuple", "authors": ["Shiyu Chen", " Ningyuan", " Huang", "Soledad Villar"], "year": 2025, "url": "http://arxiv.org/abs/2510.10341v1", "pdf_url": "http://arxiv.org/pdf/2510.10341v1", "summary": "Graph Neural Networks (GNNs) typically scale with the number of graph edges,\nmaking them well suited for sparse graphs but less efficient on dense graphs,\nsuch as point clouds or molecular interactions. A common remedy is to sparsify\nthe graph via similarity thresholding or distance pruning, but this forces an\narbitrary choice of a single interaction scale and discards crucial information\nfrom other scales. To overcome this limitation, we introduce a multi-view\ngraph-tuple framework. Instead of a single graph, our graph-tuple framework\npartitions the graph into disjoint subgraphs, capturing primary local\ninteractions and weaker, long-range connections. We then learn multi-view\nrepresentations from the graph-tuple via a heterogeneous message-passing\narchitecture inspired by the theory of non-commuting operators, which we\nformally prove is strictly more expressive and guarantees a lower oracle risk\ncompared to single-graph message-passing models. We instantiate our framework\non two scientific domains: molecular property prediction from feature-scarce\nCoulomb matrices and cosmological parameter inference from geometric point\nclouds. On both applications, our multi-view graph-tuple models demonstrate\nbetter performance than single-graph baselines, highlighting the power and\nversatility of our multi-view approach."}
{"id": "arxiv:2510.10327v1", "source": "arxiv", "title": "Mapping the Urban Mobility Intelligence Frontier: A Scientometric Analysis of Data-Driven Pedestrian Trajectory Prediction and Simulation", "authors": ["Junhao Xu", "Hui Zeng"], "year": 2025, "url": "http://arxiv.org/abs/2510.10327v1", "pdf_url": "http://arxiv.org/pdf/2510.10327v1", "summary": "Understanding and predicting pedestrian dynamics has become essential for\nshaping safer, more responsive, and human-centered urban environments. This\nstudy conducts a comprehensive scientometric analysis of research on\ndata-driven pedestrian trajectory prediction and crowd simulation, mapping its\nintellectual evolution and interdisciplinary structure. Using bibliometric data\nfrom the Web of Science Core Collection, we employ SciExplorer and Bibliometrix\nto identify major trends, influential contributors, and emerging frontiers.\nResults reveal a strong convergence between artificial intelligence, urban\ninformatics, and crowd behavior modeling--driven by graph neural networks,\ntransformers, and generative models. Beyond technical advances, the field\nincreasingly informs urban mobility design, public safety planning, and digital\ntwin development for smart cities. However, challenges remain in ensuring\ninterpretability, inclusivity, and cross-domain transferability. By connecting\nmethodological trajectories with urban applications, this work highlights how\ndata-driven approaches can enrich urban governance and pave the way for\nadaptive, socially responsible mobility intelligence in future cities."}
{"id": "arxiv:2510.10116v1", "source": "arxiv", "title": "Preference-driven Knowledge Distillation for Few-shot Node Classification", "authors": ["Xing Wei", "Chunchun Chen", "Rui Fan", "Xiaofeng Cao", "Sourav Medya", "Wei Ye"], "year": 2025, "url": "http://arxiv.org/abs/2510.10116v1", "pdf_url": "http://arxiv.org/pdf/2510.10116v1", "summary": "Graph neural networks (GNNs) can efficiently process text-attributed graphs\n(TAGs) due to their message-passing mechanisms, but their training heavily\nrelies on the human-annotated labels. Moreover, the complex and diverse local\ntopologies of nodes of real-world TAGs make it challenging for a single\nmechanism to handle. Large language models (LLMs) perform well in\nzero-/few-shot learning on TAGs but suffer from a scalability challenge.\nTherefore, we propose a preference-driven knowledge distillation (PKD)\nframework to synergize the complementary strengths of LLMs and various GNNs for\nfew-shot node classification. Specifically, we develop a GNN-preference-driven\nnode selector that effectively promotes prediction distillation from LLMs to\nteacher GNNs. To further tackle nodes' intricate local topologies, we develop a\nnode-preference-driven GNN selector that identifies the most suitable teacher\nGNN for each node, thereby facilitating tailored knowledge distillation from\nteacher GNNs to the student GNN. Extensive experiments validate the efficacy of\nour proposed framework in few-shot node classification on real-world TAGs."}
{"id": "arxiv:2509.10871v1", "source": "arxiv", "title": "Optimal message passing for molecular prediction is simple, attentive and spatial", "authors": ["Alma C. Castaneda-Leautaud", "Rommie E. Amaro"], "year": 2025, "url": "http://arxiv.org/abs/2509.10871v1", "pdf_url": "http://arxiv.org/pdf/2509.10871v1", "summary": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns."}
{"id": "arxiv:2506.15792v1", "source": "arxiv", "title": "Descriptor-based Foundation Models for Molecular Property Prediction", "authors": ["Jackson Burns", "Akshat Zalte", "William Green"], "year": 2025, "url": "http://arxiv.org/abs/2506.15792v1", "pdf_url": "http://arxiv.org/pdf/2506.15792v1", "summary": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets."}
{"id": "arxiv:2505.18728v1", "source": "arxiv", "title": "Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling", "authors": ["Andrea Ceni", "Alessio Gravina", "Claudio Gallicchio", "Davide Bacciu", "Carola-Bibiane Schonlieb", "Moshe Eliasof"], "year": 2025, "url": "http://arxiv.org/abs/2505.18728v1", "pdf_url": "http://arxiv.org/pdf/2505.18728v1", "summary": "The recent success of State-Space Models (SSMs) in sequence modeling has\nmotivated their adaptation to graph learning, giving rise to Graph State-Space\nModels (GSSMs). However, existing GSSMs operate by applying SSM modules to\nsequences extracted from graphs, often compromising core properties such as\npermutation equivariance, message-passing compatibility, and computational\nefficiency. In this paper, we introduce a new perspective by embedding the key\nprinciples of modern SSM computation directly into the Message-Passing Neural\nNetwork framework, resulting in a unified methodology for both static and\ntemporal graphs. Our approach, MP-SSM, enables efficient,\npermutation-equivariant, and long-range information propagation while\npreserving the architectural simplicity of message passing. Crucially, MP-SSM\nenables an exact sensitivity analysis, which we use to theoretically\ncharacterize information flow and evaluate issues like vanishing gradients and\nover-squashing in the deep regime. Furthermore, our design choices allow for a\nhighly optimized parallel implementation akin to modern SSMs. We validate\nMP-SSM across a wide range of tasks, including node classification, graph\nproperty prediction, long-range benchmarks, and spatiotemporal forecasting,\ndemonstrating both its versatility and strong empirical performance."}
{"id": "arxiv:2503.07397v1", "source": "arxiv", "title": "Q-MARL: A quantum-inspired algorithm using neural message passing for large-scale multi-agent reinforcement learning", "authors": ["Kha Vo", "Chin-Teng Lin"], "year": 2025, "url": "http://arxiv.org/abs/2503.07397v1", "pdf_url": "http://arxiv.org/pdf/2503.07397v1", "summary": "Inspired by a graph-based technique for predicting molecular properties in\nquantum chemistry -- atoms' position within molecules in three-dimensional\nspace -- we present Q-MARL, a completely decentralised learning architecture\nthat supports very large-scale multi-agent reinforcement learning scenarios\nwithout the need for strong assumptions like common rewards or agent order. The\nkey is to treat each agent as relative to its surrounding agents in an\nenvironment that is presumed to change dynamically. Hence, in each time step,\nan agent is the centre of its own neighbourhood and also a neighbour to many\nother agents. Each role is formulated as a sub-graph, and each sub-graph is\nused as a training sample. A message-passing neural network supports full-scale\nvertex and edge interaction within a local neighbourhood, while a parameter\ngoverning the depth of the sub-graphs eases the training burden. During\ntesting, an agent's actions are locally ensembled across all the sub-graphs\nthat contain it, resulting in robust decisions. Where other approaches struggle\nto manage 50 agents, Q-MARL can easily marshal thousands. A detailed\ntheoretical analysis proves improvement and convergence, and simulations with\nthe typical collaborative and competitive scenarios show dramatically faster\ntraining speeds and reduced training losses."}
{"id": "arxiv:2503.07378v5", "source": "arxiv", "title": "A Materials Map Integrating Experimental and Computational Data via Graph-Based Machine Learning for Enhanced Materials Discovery", "authors": ["Yusuke Hashimoto", "Xue Jia", "Hao Li", "Takaaki Tomai"], "year": 2025, "url": "http://arxiv.org/abs/2503.07378v5", "pdf_url": "http://arxiv.org/pdf/2503.07378v5", "summary": "Materials informatics (MI), emerging from the integration of materials\nscience and data science, is expected to significantly accelerate material\ndevelopment and discovery. The data used in MI are derived from both\ncomputational and experimental studies; however, their integration remains\nchallenging. In our previous study, we reported the integration of these\ndatasets by applying a machine learning model that is trained on the\nexperimental dataset to the compositional data stored in the computational\ndatabase. In this study, we use the obtained datasets to construct materials\nmaps, which visualize the relationships between material properties and\nstructural features, aiming to support experimental researchers. The materials\nmap is constructed using the MatDeepLearn (MDL) framework, which implements\nmaterials property prediction using graph-based representations of material\nstructure and deep learning modeling. Through statistical analysis, we find\nthat the MDL framework using the message passing neural network (MPNN)\narchitecture efficiently extracts features reflecting the structural complexity\nof materials. Moreover, we find that this advantage does not necessarily\ntranslate into improved accuracy in the prediction of material properties. We\nattribute this unexpected outcome to the high learning performance inherent in\nMPNN, which can contribute to the structuring of data points within the\nmaterials map."}
{"id": "arxiv:2501.18876v1", "source": "arxiv", "title": "QMe14S, A Comprehensive and Efficient Spectral Dataset for Small Organic Molecules", "authors": ["Mingzhi Yuan", "Zihan Zou", "Wei Hu"], "year": 2025, "url": "http://arxiv.org/abs/2501.18876v1", "pdf_url": "http://arxiv.org/pdf/2501.18876v1", "summary": "Developing machine learning protocols for molecular simulations requires\ncomprehensive and efficient datasets. Here we introduce the QMe14S dataset,\ncomprising 186,102 small organic molecules featuring 14 elements (H, B, C, N,\nO, F, Al, Si, P, S, Cl, As, Se, Br) and 47 functional groups. Using density\nfunctional theory at the B3LYP/TZVP level, we optimized the geometries and\ncalculated properties including energy, atomic charge, atomic force, dipole\nmoment, quadrupole moment, polarizability, octupole moment, first\nhyperpolarizability, and Hessian. At the same level, we obtained the harmonic\nIR, Raman and NMR spectra. Furthermore, we conducted ab initio molecular\ndynamics simulations to generate dynamic configurations and extract\nnonequilibrium properties, including energy, forces, and Hessians. By\nleveraging our E(3)-equivariant message-passing neural network (DetaNet), we\ndemonstrated that models trained on QMe14S outperform those trained on the\npreviously developed QM9S dataset in simulating molecular spectra. The QMe14S\ndataset thus serves as a comprehensive benchmark for molecular simulations,\noffering valuable insights into structure-property relationships."}
{"id": "arxiv:2508.20527v2", "source": "arxiv", "title": "Molecular Machine Learning in Chemical Process Design", "authors": ["Jan G. Rittig", "Manuel Dahmen", "Martin Grohe", "Philippe Schwaller", "Alexander Mitsos"], "year": 2025, "url": "http://arxiv.org/abs/2508.20527v2", "pdf_url": "http://arxiv.org/pdf/2508.20527v2", "summary": "We present a perspective on molecular machine learning (ML) in the field of\nchemical process engineering. Recently, molecular ML has demonstrated great\npotential in (i) providing highly accurate predictions for properties of pure\ncomponents and their mixtures, and (ii) exploring the chemical space for new\nmolecular structures. We review current state-of-the-art molecular ML models\nand discuss research directions that promise further advancements. This\nincludes ML methods, such as graph neural networks and transformers, which can\nbe further advanced through the incorporation of physicochemical knowledge in a\nhybrid or physics-informed fashion. Then, we consider leveraging molecular ML\nat the chemical process scale, which is highly desirable yet rather unexplored.\nWe discuss how molecular ML can be integrated into process design and\noptimization formulations, promising to accelerate the identification of novel\nmolecules and processes. To this end, it will be essential to create molecule\nand process design benchmarks and practically validate proposed candidates,\npossibly in collaboration with the chemical industry."}
{"id": "arxiv:2508.15015v1", "source": "arxiv", "title": "Fragment-Wise Interpretability in Graph Neural Networks via Molecule Decomposition and Contribution Analysis", "authors": ["Sebastian Musiał", "Bartosz Zieliński", "Tomasz Danel"], "year": 2025, "url": "http://arxiv.org/abs/2508.15015v1", "pdf_url": "http://arxiv.org/pdf/2508.15015v1", "summary": "Graph neural networks have demonstrated remarkable success in predicting\nmolecular properties by leveraging the rich structural information encoded in\nmolecular graphs. However, their black-box nature reduces interpretability,\nwhich limits trust in their predictions for important applications such as drug\ndiscovery and materials design. Furthermore, existing explanation techniques\noften fail to reliably quantify the contribution of individual atoms or\nsubstructures due to the entangled message-passing dynamics. We introduce SEAL\n(Substructure Explanation via Attribution Learning), a new interpretable graph\nneural network that attributes model predictions to meaningful molecular\nsubgraphs. SEAL decomposes input graphs into chemically relevant fragments and\nestimates their causal influence on the output. The strong alignment between\nfragment contributions and model predictions is achieved by explicitly reducing\ninter-fragment message passing in our proposed model architecture. Extensive\nevaluations on synthetic benchmarks and real-world molecular datasets\ndemonstrate that SEAL outperforms other explainability methods in both\nquantitative attribution metrics and human-aligned interpretability. A user\nstudy further confirms that SEAL provides more intuitive and trustworthy\nexplanations to domain experts. By bridging the gap between predictive\nperformance and interpretability, SEAL offers a promising direction for more\ntransparent and actionable molecular modeling."}
{"id": "arxiv:2508.07807v2", "source": "arxiv", "title": "Topological Feature Compression for Molecular Graph Neural Networks", "authors": ["Rahul Khorana"], "year": 2025, "url": "http://arxiv.org/abs/2508.07807v2", "pdf_url": "http://arxiv.org/pdf/2508.07807v2", "summary": "Recent advances in molecular representation learning have produced highly\neffective encodings of molecules for numerous cheminformatics and\nbioinformatics tasks. However, extracting general chemical insight while\nbalancing predictive accuracy, interpretability, and computational efficiency\nremains a major challenge. In this work, we introduce a novel Graph Neural\nNetwork (GNN) architecture that combines compressed higher-order topological\nsignals with standard molecular features. Our approach captures global\ngeometric information while preserving computational tractability and\nhuman-interpretable structure. We evaluate our model across a range of\nbenchmarks, from small-molecule datasets to complex material datasets, and\ndemonstrate superior performance using a parameter-efficient architecture. We\nachieve the best performing results in both accuracy and robustness across\nalmost all benchmarks. We open source all code \\footnote{All code and results\ncan be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}."}
{"id": "arxiv:2507.03474v1", "source": "arxiv", "title": "Molecular Machine Learning Using Euler Characteristic Transforms", "authors": ["Victor Toscano-Duran", "Florian Rottach", "Bastian Rieck"], "year": 2025, "url": "http://arxiv.org/abs/2507.03474v1", "pdf_url": "http://arxiv.org/pdf/2507.03474v1", "summary": "The shape of a molecule determines its physicochemical and biological\nproperties. However, it is often underrepresented in standard molecular\nrepresentation learning approaches. Here, we propose using the Euler\nCharacteristic Transform (ECT) as a geometrical-topological descriptor.\nComputed directly on a molecular graph derived from handcrafted atomic\nfeatures, the ECT enables the extraction of multiscale structural features,\noffering a novel way to represent and encode molecular shape in the feature\nspace. We assess the predictive performance of this representation across nine\nbenchmark regression datasets, all centered around predicting the inhibition\nconstant $K_i$. In addition, we compare our proposed ECT-based representation\nagainst traditional molecular representations and methods, such as molecular\nfingerprints/descriptors and graph neural networks (GNNs). Our results show\nthat our ECT-based representation achieves competitive performance, ranking\namong the best-performing methods on several datasets. More importantly, its\ncombination with traditional representations, particularly with the AVALON\nfingerprint, significantly \\emph{enhances predictive performance},\noutperforming other methods on most datasets. These findings highlight the\ncomplementary value of multiscale topological information and its potential for\nbeing combined with established techniques. Our study suggests that hybrid\napproaches incorporating explicit shape information can lead to more\ninformative and robust molecular representations, enhancing and opening new\navenues in molecular machine learning tasks. To support reproducibility and\nfoster open biomedical research, we provide open access to all experiments and\ncode used in this work."}
{"id": "arxiv:2507.03430v2", "source": "arxiv", "title": "Multi-Level Fusion Graph Neural Network for Molecule Property Prediction", "authors": ["XiaYu Liu", "Chao Fan", "Yang Liu", "Hou-biao Li"], "year": 2025, "url": "http://arxiv.org/abs/2507.03430v2", "pdf_url": "http://arxiv.org/pdf/2507.03430v2", "summary": "Accurate prediction of molecular properties is essential in drug discovery\nand related fields. However, existing graph neural networks (GNNs) often\nstruggle to simultaneously capture both local and global molecular structures.\nIn this work, we propose a Multi-Level Fusion Graph Neural Network (MLFGNN)\nthat integrates Graph Attention Networks and a novel Graph Transformer to\njointly model local and global dependencies. In addition, we incorporate\nmolecular fingerprints as a complementary modality and introduce a mechanism of\ninteraction between attention to adaptively fuse information across\nrepresentations. Extensive experiments on multiple benchmark datasets\ndemonstrate that MLFGNN consistently outperforms state-of-the-art methods in\nboth classification and regression tasks. Interpretability analysis further\nreveals that the model effectively captures task-relevant chemical patterns,\nsupporting the usefulness of multi-level and multi-modal fusion in molecular\nrepresentation learning."}
{"id": "arxiv:2506.19862v1", "source": "arxiv", "title": "DualEquiNet: A Dual-Space Hierarchical Equivariant Network for Large Biomolecules", "authors": ["Junjie Xu", "Jiahao Zhang", "Mangal Prakash", "Xiang Zhang", "Suhang Wang"], "year": 2025, "url": "http://arxiv.org/abs/2506.19862v1", "pdf_url": "http://arxiv.org/pdf/2506.19862v1", "summary": "Geometric graph neural networks (GNNs) that respect E(3) symmetries have\nachieved strong performance on small molecule modeling, but they face\nscalability and expressiveness challenges when applied to large biomolecules\nsuch as RNA and proteins. These systems require models that can simultaneously\ncapture fine-grained atomic interactions, long-range dependencies across\nspatially distant components, and biologically relevant hierarchical structure,\nsuch as atoms forming residues, which in turn form higher-order domains.\nExisting geometric GNNs, which typically operate exclusively in either\nEuclidean or Spherical Harmonics space, are limited in their ability to capture\nboth the fine-scale atomic details and the long-range, symmetry-aware\ndependencies required for modeling the multi-scale structure of large\nbiomolecules. We introduce DualEquiNet, a Dual-Space Hierarchical Equivariant\nNetwork that constructs complementary representations in both Euclidean and\nSpherical Harmonics spaces to capture local geometry and global symmetry-aware\nfeatures. DualEquiNet employs bidirectional cross-space message passing and a\nnovel Cross-Space Interaction Pooling mechanism to hierarchically aggregate\natomic features into biologically meaningful units, such as residues, enabling\nefficient and expressive multi-scale modeling for large biomolecular systems.\nDualEquiNet achieves state-of-the-art performance on multiple existing\nbenchmarks for RNA property prediction and protein modeling, and outperforms\nprior methods on two newly introduced 3D structural benchmarks demonstrating\nits broad effectiveness across a range of large biomolecule modeling tasks."}
{"id": "arxiv:2509.10871v1", "source": "arxiv", "title": "Optimal message passing for molecular prediction is simple, attentive and spatial", "authors": ["Alma C. Castaneda-Leautaud", "Rommie E. Amaro"], "year": 2025, "url": "http://arxiv.org/abs/2509.10871v1", "pdf_url": "http://arxiv.org/pdf/2509.10871v1", "summary": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns."}
{"id": "arxiv:2506.15792v1", "source": "arxiv", "title": "Descriptor-based Foundation Models for Molecular Property Prediction", "authors": ["Jackson Burns", "Akshat Zalte", "William Green"], "year": 2025, "url": "http://arxiv.org/abs/2506.15792v1", "pdf_url": "http://arxiv.org/pdf/2506.15792v1", "summary": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets."}
{"id": "arxiv:2505.18728v1", "source": "arxiv", "title": "Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling", "authors": ["Andrea Ceni", "Alessio Gravina", "Claudio Gallicchio", "Davide Bacciu", "Carola-Bibiane Schonlieb", "Moshe Eliasof"], "year": 2025, "url": "http://arxiv.org/abs/2505.18728v1", "pdf_url": "http://arxiv.org/pdf/2505.18728v1", "summary": "The recent success of State-Space Models (SSMs) in sequence modeling has\nmotivated their adaptation to graph learning, giving rise to Graph State-Space\nModels (GSSMs). However, existing GSSMs operate by applying SSM modules to\nsequences extracted from graphs, often compromising core properties such as\npermutation equivariance, message-passing compatibility, and computational\nefficiency. In this paper, we introduce a new perspective by embedding the key\nprinciples of modern SSM computation directly into the Message-Passing Neural\nNetwork framework, resulting in a unified methodology for both static and\ntemporal graphs. Our approach, MP-SSM, enables efficient,\npermutation-equivariant, and long-range information propagation while\npreserving the architectural simplicity of message passing. Crucially, MP-SSM\nenables an exact sensitivity analysis, which we use to theoretically\ncharacterize information flow and evaluate issues like vanishing gradients and\nover-squashing in the deep regime. Furthermore, our design choices allow for a\nhighly optimized parallel implementation akin to modern SSMs. We validate\nMP-SSM across a wide range of tasks, including node classification, graph\nproperty prediction, long-range benchmarks, and spatiotemporal forecasting,\ndemonstrating both its versatility and strong empirical performance."}
{"id": "arxiv:2503.07378v5", "source": "arxiv", "title": "A Materials Map Integrating Experimental and Computational Data via Graph-Based Machine Learning for Enhanced Materials Discovery", "authors": ["Yusuke Hashimoto", "Xue Jia", "Hao Li", "Takaaki Tomai"], "year": 2025, "url": "http://arxiv.org/abs/2503.07378v5", "pdf_url": "http://arxiv.org/pdf/2503.07378v5", "summary": "Materials informatics (MI), emerging from the integration of materials\nscience and data science, is expected to significantly accelerate material\ndevelopment and discovery. The data used in MI are derived from both\ncomputational and experimental studies; however, their integration remains\nchallenging. In our previous study, we reported the integration of these\ndatasets by applying a machine learning model that is trained on the\nexperimental dataset to the compositional data stored in the computational\ndatabase. In this study, we use the obtained datasets to construct materials\nmaps, which visualize the relationships between material properties and\nstructural features, aiming to support experimental researchers. The materials\nmap is constructed using the MatDeepLearn (MDL) framework, which implements\nmaterials property prediction using graph-based representations of material\nstructure and deep learning modeling. Through statistical analysis, we find\nthat the MDL framework using the message passing neural network (MPNN)\narchitecture efficiently extracts features reflecting the structural complexity\nof materials. Moreover, we find that this advantage does not necessarily\ntranslate into improved accuracy in the prediction of material properties. We\nattribute this unexpected outcome to the high learning performance inherent in\nMPNN, which can contribute to the structuring of data points within the\nmaterials map."}
{"id": "arxiv:2501.18876v1", "source": "arxiv", "title": "QMe14S, A Comprehensive and Efficient Spectral Dataset for Small Organic Molecules", "authors": ["Mingzhi Yuan", "Zihan Zou", "Wei Hu"], "year": 2025, "url": "http://arxiv.org/abs/2501.18876v1", "pdf_url": "http://arxiv.org/pdf/2501.18876v1", "summary": "Developing machine learning protocols for molecular simulations requires\ncomprehensive and efficient datasets. Here we introduce the QMe14S dataset,\ncomprising 186,102 small organic molecules featuring 14 elements (H, B, C, N,\nO, F, Al, Si, P, S, Cl, As, Se, Br) and 47 functional groups. Using density\nfunctional theory at the B3LYP/TZVP level, we optimized the geometries and\ncalculated properties including energy, atomic charge, atomic force, dipole\nmoment, quadrupole moment, polarizability, octupole moment, first\nhyperpolarizability, and Hessian. At the same level, we obtained the harmonic\nIR, Raman and NMR spectra. Furthermore, we conducted ab initio molecular\ndynamics simulations to generate dynamic configurations and extract\nnonequilibrium properties, including energy, forces, and Hessians. By\nleveraging our E(3)-equivariant message-passing neural network (DetaNet), we\ndemonstrated that models trained on QMe14S outperform those trained on the\npreviously developed QM9S dataset in simulating molecular spectra. The QMe14S\ndataset thus serves as a comprehensive benchmark for molecular simulations,\noffering valuable insights into structure-property relationships."}
{"id": "arxiv:2410.18676v2", "source": "arxiv", "title": "Homomorphism Counts as Structural Encodings for Graph Learning", "authors": ["Linus Bao", "Emily Jin", "Michael Bronstein", "İsmail İlkan Ceylan", "Matthias Lanzinger"], "year": 2024, "url": "http://arxiv.org/abs/2410.18676v2", "pdf_url": "http://arxiv.org/pdf/2410.18676v2", "summary": "Graph Transformers are popular neural networks that extend the well-known\nTransformer architecture to the graph domain. These architectures operate by\napplying self-attention on graph nodes and incorporating graph structure\nthrough the use of positional encodings (e.g., Laplacian positional encoding)\nor structural encodings (e.g., random-walk structural encoding). The quality of\nsuch encodings is critical, since they provide the necessary $\\textit{graph\ninductive biases}$ to condition the model on graph structure. In this work, we\npropose $\\textit{motif structural encoding}$ (MoSE) as a flexible and powerful\nstructural encoding framework based on counting graph homomorphisms.\nTheoretically, we compare the expressive power of MoSE to random-walk\nstructural encoding and relate both encodings to the expressive power of\nstandard message passing neural networks. Empirically, we observe that MoSE\noutperforms other well-known positional and structural encodings across a range\nof architectures, and it achieves state-of-the-art performance on a widely\nstudied molecular property prediction dataset."}
{"id": "arxiv:2510.09854v1", "source": "arxiv", "title": "NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering", "authors": ["Kaiwen Shi", "Zheyuan Zhang", "Zhengqing Yuan", "Keerthiram Murugesan", "Vincent Galass", "Chuxu Zhang", "Yanfang Ye"], "year": 2025, "url": "http://arxiv.org/abs/2510.09854v1", "pdf_url": "http://arxiv.org/pdf/2510.09854v1", "summary": "Diet plays a central role in human health, and Nutrition Question Answering\n(QA) offers a promising path toward personalized dietary guidance and the\nprevention of diet-related chronic diseases. However, existing methods face two\nfundamental challenges: the limited reasoning capacity of single-agent systems\nand the complexity of designing effective multi-agent architectures, as well as\ncontextual overload that hinders accurate decision-making. We introduce\nNutritional-Graph Router (NG-Router), a novel framework that formulates\nnutritional QA as a supervised, knowledge-graph-guided multi-agent\ncollaboration problem. NG-Router integrates agent nodes into heterogeneous\nknowledge graphs and employs a graph neural network to learn task-aware routing\ndistributions over agents, leveraging soft supervision derived from empirical\nagent performance. To further address contextual overload, we propose a\ngradient-based subgraph retrieval mechanism that identifies salient evidence\nduring training, thereby enhancing multi-hop and relational reasoning.\nExtensive experiments across multiple benchmarks and backbone models\ndemonstrate that NG-Router consistently outperforms both single-agent and\nensemble baselines, offering a principled approach to domain-aware multi-agent\nreasoning for complex nutritional health tasks."}
{"id": "arxiv:2510.08966v1", "source": "arxiv", "title": "Semantic-Condition Tuning: Fusing Graph Context with Large Language Models for Knowledge Graph Completion", "authors": ["Ruitong Liu", "Yan Wen", "Te Sun", "Yunjia Wu", "Pingyang Huang", "Zihang Yu", "Siyuan Li"], "year": 2025, "url": "http://arxiv.org/abs/2510.08966v1", "pdf_url": "http://arxiv.org/pdf/2510.08966v1", "summary": "Fusing Knowledge Graphs with Large Language Models is crucial for\nknowledge-intensive tasks like knowledge graph completion. The prevailing\nparadigm, prefix-tuning, simply concatenates knowledge embeddings with text\ninputs. However, this shallow fusion overlooks the rich relational semantics\nwithin KGs and imposes a significant implicit reasoning burden on the LLM to\ncorrelate the prefix with the text. To address these, we propose\nSemantic-condition Tuning (SCT), a new knowledge injection paradigm comprising\ntwo key modules. First, a Semantic Graph Module employs a Graph Neural Network\nto extract a context-aware semantic condition from the local graph\nneighborhood, guided by knowledge-enhanced relations. Subsequently, this\ncondition is passed to a Condition-Adaptive Fusion Module, which, in turn,\nadaptively modulates the textual embedding via two parameterized projectors,\nenabling a deep, feature-wise, and knowledge-aware interaction. The resulting\npre-fused embedding is then fed into the LLM for fine-tuning. Extensive\nexperiments on knowledge graph benchmarks demonstrate that SCT significantly\noutperforms prefix-tuning and other strong baselines. Our analysis confirms\nthat by modulating the input representation with semantic graph context before\nLLM inference, SCT provides a more direct and potent signal, enabling more\naccurate and robust knowledge reasoning."}
{"id": "arxiv:2510.08450v1", "source": "arxiv", "title": "gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity", "authors": ["Hugh Blayney", "Álvaro Arroyo", "Xiaowen Dong", "Michael M. Bronstein"], "year": 2025, "url": "http://arxiv.org/abs/2510.08450v1", "pdf_url": "http://arxiv.org/pdf/2510.08450v1", "summary": "Graph Neural Networks (GNNs) leverage the graph structure to transmit\ninformation between nodes, typically through the message-passing mechanism.\nWhile these models have found a wide variety of applications, they are known to\nsuffer from over-squashing, where information from a large receptive field of\nnode representations is collapsed into a single fixed sized vector, resulting\nin an information bottleneck. In this paper, we re-examine the over-squashing\nphenomenon through the lens of model storage and retrieval capacity, which we\ndefine as the amount of information that can be stored in a node's\nrepresentation for later use. We study some of the limitations of existing\ntasks used to measure over-squashing and introduce a new synthetic task to\ndemonstrate that an information bottleneck can saturate this capacity.\nFurthermore, we adapt ideas from the sequence modeling literature on\nassociative memories, fast weight programmers, and the xLSTM model to develop a\nnovel GNN architecture with improved capacity. We demonstrate strong\nperformance of this architecture both on our capacity synthetic task, as well\nas a range of real-world graph benchmarks."}
{"id": "arxiv:2510.07910v1", "source": "arxiv", "title": "MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation", "authors": ["Chongmyung Kwon", "Yujin Kim", "Seoeun Park", "Yunji Lee", "Charmgil Hong"], "year": 2025, "url": "http://arxiv.org/abs/2510.07910v1", "pdf_url": "http://arxiv.org/pdf/2510.07910v1", "summary": "Drug recommendation is an essential task in machine learning-based clinical\ndecision support systems. However, the risk of drug-drug interactions (DDI)\nbetween co-prescribed medications remains a significant challenge. Previous\nstudies have used graph neural networks (GNNs) to represent drug structures.\nRegardless, their simplified discrete forms cannot fully capture the molecular\nbinding affinity and reactivity. Therefore, we propose Multimodal DDI\nPrediction with Molecular Electron Localization Function (ELF) Maps (MMM), a\nnovel framework that integrates three-dimensional (3D) quantum-chemical\ninformation into drug representation learning. It generates 3D electron density\nmaps using the ELF. To capture both therapeutic relevance and interaction\nrisks, MMM combines ELF-derived features that encode global electronic\nproperties with a bipartite graph encoder that models local substructure\ninteractions. This design enables learning complementary characteristics of\ndrug molecules. We evaluate MMM in the MIMIC-III dataset (250 drugs, 442\nsubstructures), comparing it with several baseline models. In particular, a\ncomparison with the GNN-based SafeDrug model demonstrates statistically\nsignificant improvements in the F1-score (p = 0.0387), Jaccard (p = 0.0112),\nand the DDI rate (p = 0.0386). These results demonstrate the potential of\nELF-based 3D representations to enhance prediction accuracy and support safer\ncombinatorial drug prescribing in clinical practice."}
{"id": "arxiv:2510.07285v1", "source": "arxiv", "title": "GTCN-G: A Residual Graph-Temporal Fusion Network for Imbalanced Intrusion Detection (Preprint)", "authors": ["Tianxiang Xu", "Zhichao Wen", "Xinyu Zhao", "Qi Hu", "Yan Li", "Chang Liu"], "year": 2025, "url": "http://arxiv.org/abs/2510.07285v1", "pdf_url": "http://arxiv.org/pdf/2510.07285v1", "summary": "The escalating complexity of network threats and the inherent class imbalance\nin traffic data present formidable challenges for modern Intrusion Detection\nSystems (IDS). While Graph Neural Networks (GNNs) excel in modeling topological\nstructures and Temporal Convolutional Networks (TCNs) are proficient in\ncapturing time-series dependencies, a framework that synergistically integrates\nboth while explicitly addressing data imbalance remains an open challenge. This\npaper introduces a novel deep learning framework, named Gated Temporal\nConvolutional Network and Graph (GTCN-G), engineered to overcome these\nlimitations. Our model uniquely fuses a Gated TCN (G-TCN) for extracting\nhierarchical temporal features from network flows with a Graph Convolutional\nNetwork (GCN) designed to learn from the underlying graph structure. The core\ninnovation lies in the integration of a residual learning mechanism,\nimplemented via a Graph Attention Network (GAT). This mechanism preserves\noriginal feature information through residual connections, which is critical\nfor mitigating the class imbalance problem and enhancing detection sensitivity\nfor rare malicious activities (minority classes). We conducted extensive\nexperiments on two public benchmark datasets, UNSW-NB15 and ToN-IoT, to\nvalidate our approach. The empirical results demonstrate that the proposed\nGTCN-G model achieves state-of-the-art performance, significantly outperforming\nexisting baseline models in both binary and multi-class classification tasks."}
{"id": "arxiv:2510.05995v2", "source": "arxiv", "title": "A comprehensive comparison of neural operators for 3D industry-scale engineering designs", "authors": ["Weiheng Zhong", "Qibang Liu", "Diab Abueidda", "Seid Koric", "Hadi Meidani"], "year": 2025, "url": "http://arxiv.org/abs/2510.05995v2", "pdf_url": "http://arxiv.org/pdf/2510.05995v2", "summary": "Neural operators have emerged as powerful tools for learning nonlinear\nmappings between function spaces, enabling real-time prediction of complex\ndynamics in diverse scientific and engineering applications. With their growing\nadoption in engineering design evaluation, a wide range of neural operator\narchitectures have been proposed for various problem settings. However, model\nselection remains challenging due to the absence of fair and comprehensive\ncomparisons. To address this, we propose and standardize six representative 3D\nindustry-scale engineering design datasets spanning thermal analysis, linear\nelasticity, elasto-plasticity, time-dependent plastic problems, and\ncomputational fluid dynamics. All datasets include fully preprocessed inputs\nand outputs for model training, making them directly usable across diverse\nneural operator architectures. Using these datasets, we conduct a systematic\ncomparison of four types of neural operator variants, including\nBranch-Trunk-based Neural Operators inspired by DeepONet, Graph-based Neural\nOperators inspired by Graph Neural Networks, Grid-based Neural Operators\ninspired by Fourier Neural Operators, and Point-based Neural Operators inspired\nby PointNet. We further introduce practical enhancements to adapt these models\nto different engineering settings, improving the fairness of the comparison.\nOur benchmarking study evaluates each model strengths and limitations in terms\nof predictive performance, computational efficiency, memory usage, and\ndeployment complexity. The findings provide actionable insights to guide future\nneural operator development."}
