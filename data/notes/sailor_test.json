{
  "topic": "graph neural networks for molecules",
  "session_id": "sailor_test",
  "created_at": 1760455598,
  "tasks": [
    {
      "id": "b65b6c5f",
      "title": "Search for candidate papers across arXiv and ML/chemistry conferences",
      "status": "done"
    },
    {
      "id": "f7ef17ef",
      "title": "Select ~8 high-impact papers (foundational + recent advances) to read",
      "status": "done"
    },
    {
      "id": "38cd7ff3",
      "title": "Read selected papers and extract model architectures, tasks, datasets, and performance",
      "status": "done"
    },
    {
      "id": "4a1ef531",
      "title": "Summarize and synthesize findings across the 8 papers",
      "status": "doing"
    },
    {
      "id": "df6f3fa5",
      "title": "Follow up on missing details (hyperparams, compute) and collect code links",
      "status": "doing"
    },
    {
      "id": "2311eaaf",
      "title": "Produce final recommendation: model choices, pretraining strategy, evaluation plan",
      "status": "todo"
    },
    {
      "id": "5fff4676",
      "title": "Synthesize cross-paper comparison and reproducibility matrix",
      "status": "doing"
    },
    {
      "id": "e3ec0445",
      "title": "Collect code/checkpoint links and generate repo-search queries for missing code",
      "status": "doing"
    },
    {
      "id": "e7594685",
      "title": "Produce final recommendation (model choices, pretraining, evaluation plan, compute budget)",
      "status": "todo"
    },
    {
      "id": "06a04cbd",
      "title": "Prepare minimal reproduction protocols for top-3 papers (deliverable-ready)",
      "status": "todo"
    },
    {
      "id": "1dbc6991",
      "title": "Assemble final deliverable (one-page cheat sheet + repo links + action plan)",
      "status": "todo"
    }
  ],
  "queries": [
    {
      "raw": "all:\"graph neural network\" AND all:molecule",
      "formatted": "all:\"graph neural network\" AND all:molecule"
    },
    {
      "raw": "all:\"message passing neural network\" AND all:molecule OR all:chemical OR all:chemistry",
      "formatted": "all:\"message passing neural network\" AND all:molecule OR all:chemical OR all:chemistry"
    },
    {
      "raw": "all:\"graph neural networks\" AND (all:drug OR all:quantum OR all:property prediction OR all:cheminformatics)",
      "formatted": "all:\"graph neural networks\" AND (all:drug OR all:quantum OR all:property prediction OR all:cheminformatics)"
    },
    {
      "raw": "all:\"neural message passing\" OR all:\"message passing neural network\" AND (all:QM9 OR all:quantum OR all:\"property prediction\" OR all:MoleculeNet)",
      "formatted": "all:\"neural message passing\" OR all:\"message passing neural network\" AND (all:QM9 OR all:quantum OR all:\"property prediction\" OR all:MoleculeNet)"
    },
    {
      "raw": "all:SchNet OR all:DimeNet OR all:PhysNet OR all:EGNN OR all:\"SE(3)-Transformer\" OR all:NequIP OR all:GemNet OR all:TorchMD-Net AND all:molecule",
      "formatted": "all:SchNet OR all:DimeNet OR all:PhysNet OR all:EGNN OR all:\"SE(3)-Transformer\" OR all:NequIP OR all:GemNet OR all:TorchMD-Net AND all:molecule"
    },
    {
      "raw": "all:MoleculeNet OR all:benchmark AND all:\"graph neural network\" AND all:molecule",
      "formatted": "all:MoleculeNet OR all:benchmark AND all:\"graph neural network\" AND all:molecule"
    },
    {
      "raw": "all:\"neural message passing\" OR all:\"message passing neural network\" AND (all:QM9 OR all:MoleculeNet OR all:\"property prediction\")",
      "formatted": "all:\"neural message passing\" OR all:\"message passing neural network\" AND (all:QM9 OR all:MoleculeNet OR all:\"property prediction\")"
    },
    {
      "raw": "all:SchNet OR all:DimeNet OR all:PhysNet OR all:EGNN OR all:\"SE(3)-Transformer\" OR all:NequIP OR all:GemNet OR all:TorchMD-Net AND all:molecule",
      "formatted": "all:SchNet OR all:DimeNet OR all:PhysNet OR all:EGNN OR all:\"SE(3)-Transformer\" OR all:NequIP OR all:GemNet OR all:TorchMD-Net AND all:molecule"
    },
    {
      "raw": "all:\"graph neural network\" AND (all:drug OR all:quantum OR all:cheminformatics OR all:benchmark OR all:OGB)",
      "formatted": "all:\"graph neural network\" AND (all:drug OR all:quantum OR all:cheminformatics OR all:benchmark OR all:OGB)"
    }
  ],
  "papers": [
    "arxiv:2510.07910v1",
    "arxiv:2509.22468v1",
    "arxiv:2509.22028v1",
    "arxiv:2509.17018v1",
    "arxiv:2509.11782v1",
    "arxiv:2509.07887v1",
    "arxiv:2503.07397v1",
    "arxiv:2501.18876v1",
    "arxiv:2412.01982v2",
    "arxiv:2410.16975v1",
    "arxiv:2410.07981v2",
    "arxiv:2409.18070v1",
    "arxiv:2510.11286v1",
    "arxiv:2510.10954v1",
    "arxiv:2510.10775v1",
    "arxiv:2510.10341v1",
    "arxiv:2510.10327v1",
    "arxiv:2510.10116v1",
    "arxiv:2509.10871v1",
    "arxiv:2506.15792v1",
    "arxiv:2505.18728v1",
    "arxiv:2503.07378v5",
    "arxiv:2508.20527v2",
    "arxiv:2508.15015v1",
    "arxiv:2508.07807v2",
    "arxiv:2507.03474v1",
    "arxiv:2507.03430v2",
    "arxiv:2506.19862v1",
    "arxiv:2410.18676v2",
    "arxiv:2510.09854v1",
    "arxiv:2510.08966v1",
    "arxiv:2510.08450v1",
    "arxiv:2510.07285v1",
    "arxiv:2510.05995v2"
  ],
  "history_steps": [
    {
      "step": 1,
      "action": {
        "action": "search",
        "notes": "Find foundational and recent papers on GNNs applied to molecular tasks (property prediction, quantum properties, drug discovery). Priority: MPNN/MPNN-derivatives, SchNet/DimeNet/PhysNet, GCN/GAT adaptations, and benchmarks (QM9, MoleculeNet). Search arXiv, NeurIPS/ICLR/ICML, and chemistry ML venues.",
        "queries": [
          "all:\"graph neural network\" AND all:molecule",
          "all:\"message passing neural network\" AND all:molecule OR all:chemical OR all:chemistry",
          "all:\"graph neural networks\" AND (all:drug OR all:quantum OR all:property prediction OR all:cheminformatics)"
        ],
        "papers": [],
        "focus": []
      },
      "result": "Search completed. Notes: Find foundational and recent papers on GNNs applied to molecular tasks (property prediction, quantum properties, drug discovery). Priority: MPNN/MPNN-derivatives, SchNet/DimeNet/PhysNet, GCN/GAT adaptations, and benchmarks (QM9, MoleculeNet). Search arXiv, NeurIPS/ICLR/ICML, and chemistry ML venues.\n- arxiv:2510.07910v1: MMM: Quantum-Chemical Molecular Representation Learning for Combinatorial Drug Recommendation\n- arxiv:2509.22468v1: Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised Molecular Graph Pretraining\n- arxiv:2509.22028v1: MCGM: Multi-stage Clustered Global Modeling for Long-range Interactions in Molecules\n- arxiv:2509.17018v1: DeepEOSNet: Capturing the dependency on thermodynamic state in property prediction tasks\n- arxiv:2509.11782v1: Multimodal Regression for Enzyme Turnover Rates Prediction\n... and 13 more",
      "planner_payload": {
        "action": "search",
        "queries": [
          "all:\"graph neural network\" AND all:molecule",
          "all:\"message passing neural network\" AND all:molecule OR all:chemical OR all:chemistry",
          "all:\"graph neural networks\" AND (all:drug OR all:quantum OR all:property prediction OR all:cheminformatics)"
        ],
        "notes": "Find foundational and recent papers on GNNs applied to molecular tasks (property prediction, quantum properties, drug discovery). Priority: MPNN/MPNN-derivatives, SchNet/DimeNet/PhysNet, GCN/GAT adaptations, and benchmarks (QM9, MoleculeNet). Search arXiv, NeurIPS/ICLR/ICML, and chemistry ML venues.",
        "todo": [
          {
            "title": "Search for candidate papers across arXiv and ML/chemistry conferences",
            "status": "doing"
          },
          {
            "title": "Select ~8 high-impact papers (foundational + recent advances) to read",
            "status": "todo"
          },
          {
            "title": "Read selected papers and extract model architectures, tasks, datasets, and performance",
            "status": "todo"
          }
        ]
      },
      "timestamp": 1760453492
    },
    {
      "step": 2,
      "action": {
        "action": "search",
        "notes": "Continue broad arXiv/conference search to capture foundational and high-impact GNNs for molecules (MPNN/Gilmer, SchNet, DimeNet, PhysNet, equivariant models like EGNN/SE(3)-Transformer/NequIP/GemNet, recent libraries like TorchMD-Net, and benchmarks/reviews such as MoleculeNet/OGB). Aim to assemble ~8 papers (foundational + recent advances) for reading/analysis.",
        "queries": [
          "all:\"neural message passing\" OR all:\"message passing neural network\" AND (all:QM9 OR all:quantum OR all:\"property prediction\" OR all:MoleculeNet)",
          "all:SchNet OR all:DimeNet OR all:PhysNet OR all:EGNN OR all:\"SE(3)-Transformer\" OR all:NequIP OR all:GemNet OR all:TorchMD-Net AND all:molecule",
          "all:MoleculeNet OR all:benchmark AND all:\"graph neural network\" AND all:molecule"
        ],
        "papers": [],
        "focus": []
      },
      "result": "Search completed. Notes: Continue broad arXiv/conference search to capture foundational and high-impact GNNs for molecules (MPNN/Gilmer, SchNet, DimeNet, PhysNet, equivariant models like EGNN/SE(3)-Transformer/NequIP/GemNet, recent libraries like TorchMD-Net, and benchmarks/reviews such as MoleculeNet/OGB). Aim to assemble ~8 papers (foundational + recent advances) for reading/analysis.\n- arxiv:2509.10871v1: Optimal message passing for molecular prediction is simple, attentive and spatial\n- arxiv:2506.15792v1: Descriptor-based Foundation Models for Molecular Property Prediction\n- arxiv:2505.18728v1: Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling\n- arxiv:2503.07397v1: Q-MARL: A quantum-inspired algorithm using neural message passing for large-scale multi-agent reinforcement learning\n- arxiv:2503.07378v5: A Materials Map Integrating Experimental and Computational Data via Graph-Based Machine Learning for Enhanced Materials Discovery\n... and 7 more",
      "planner_payload": {
        "action": "search",
        "queries": [
          "all:\"neural message passing\" OR all:\"message passing neural network\" AND (all:QM9 OR all:quantum OR all:\"property prediction\" OR all:MoleculeNet)",
          "all:SchNet OR all:DimeNet OR all:PhysNet OR all:EGNN OR all:\"SE(3)-Transformer\" OR all:NequIP OR all:GemNet OR all:TorchMD-Net AND all:molecule",
          "all:MoleculeNet OR all:benchmark AND all:\"graph neural network\" AND all:molecule"
        ],
        "notes": "Continue broad arXiv/conference search to capture foundational and high-impact GNNs for molecules (MPNN/Gilmer, SchNet, DimeNet, PhysNet, equivariant models like EGNN/SE(3)-Transformer/NequIP/GemNet, recent libraries like TorchMD-Net, and benchmarks/reviews such as MoleculeNet/OGB). Aim to assemble ~8 papers (foundational + recent advances) for reading/analysis.",
        "todo": [
          {
            "title": "Search for candidate papers across arXiv and ML/chemistry conferences",
            "status": "doing"
          },
          {
            "title": "Select ~8 high-impact papers (foundational + recent advances) to read",
            "status": "todo"
          },
          {
            "title": "Read selected papers and extract model architectures, tasks, datasets, and performance",
            "status": "todo"
          }
        ]
      },
      "timestamp": 1760453605
    },
    {
      "step": 3,
      "action": {
        "action": "search",
        "notes": "Continue broad arXiv/conference search targeted at foundational and high-impact molecular GNNs: MPNN/Gilmer, SchNet, DimeNet, PhysNet, equivariant architectures (EGNN, SE(3)-Transformer, NequIP, GemNet), TorchMD-Net, and benchmarks/reviews (QM9, MoleculeNet, OGB). Aim to recover classic papers plus recent advances so we can select ~8 to read.",
        "queries": [
          "all:\"neural message passing\" OR all:\"message passing neural network\" AND (all:QM9 OR all:MoleculeNet OR all:\"property prediction\")",
          "all:SchNet OR all:DimeNet OR all:PhysNet OR all:EGNN OR all:\"SE(3)-Transformer\" OR all:NequIP OR all:GemNet OR all:TorchMD-Net AND all:molecule",
          "all:\"graph neural network\" AND (all:drug OR all:quantum OR all:cheminformatics OR all:benchmark OR all:OGB)"
        ],
        "papers": [],
        "focus": []
      },
      "result": "Search completed. Notes: Continue broad arXiv/conference search targeted at foundational and high-impact molecular GNNs: MPNN/Gilmer, SchNet, DimeNet, PhysNet, equivariant architectures (EGNN, SE(3)-Transformer, NequIP, GemNet), TorchMD-Net, and benchmarks/reviews (QM9, MoleculeNet, OGB). Aim to recover classic papers plus recent advances so we can select ~8 to read.\n- arxiv:2509.10871v1: Optimal message passing for molecular prediction is simple, attentive and spatial\n- arxiv:2506.15792v1: Descriptor-based Foundation Models for Molecular Property Prediction\n- arxiv:2505.18728v1: Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling\n- arxiv:2503.07378v5: A Materials Map Integrating Experimental and Computational Data via Graph-Based Machine Learning for Enhanced Materials Discovery\n- arxiv:2501.18876v1: QMe14S, A Comprehensive and Efficient Spectral Dataset for Small Organic Molecules\n... and 7 more",
      "planner_payload": {
        "action": "search",
        "queries": [
          "all:\"neural message passing\" OR all:\"message passing neural network\" AND (all:QM9 OR all:MoleculeNet OR all:\"property prediction\")",
          "all:SchNet OR all:DimeNet OR all:PhysNet OR all:EGNN OR all:\"SE(3)-Transformer\" OR all:NequIP OR all:GemNet OR all:TorchMD-Net AND all:molecule",
          "all:\"graph neural network\" AND (all:drug OR all:quantum OR all:cheminformatics OR all:benchmark OR all:OGB)"
        ],
        "papers": [],
        "focus": [],
        "notes": "Continue broad arXiv/conference search targeted at foundational and high-impact molecular GNNs: MPNN/Gilmer, SchNet, DimeNet, PhysNet, equivariant architectures (EGNN, SE(3)-Transformer, NequIP, GemNet), TorchMD-Net, and benchmarks/reviews (QM9, MoleculeNet, OGB). Aim to recover classic papers plus recent advances so we can select ~8 to read.",
        "todo": [
          {
            "title": "Search for candidate papers across arXiv and ML/chemistry conferences",
            "status": "doing"
          },
          {
            "title": "Select ~8 high-impact papers (foundational + recent advances) to read",
            "status": "todo"
          },
          {
            "title": "Read selected papers and extract model architectures, tasks, datasets, and performance",
            "status": "todo"
          }
        ]
      },
      "timestamp": 1760453768
    },
    {
      "step": 4,
      "action": {
        "action": "read",
        "notes": "Selecting ~8 high-impact candidate papers to read next. I prioritized: (1) a recent survey for broad context (arXiv:2509.07887v1); (2) a paper on message passing best-practices (arXiv:2509.10871v1); (3) foundation-model/descriptor approaches (arXiv:2506.15792v1); (4) a simple multimodal baseline (MolMix arXiv:2410.07981v2); and (5–8) several recent specific-method papers addressing pretraining, long-range interactions, state-dependent properties, and multimodal regression (arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1). Goal for the executor: fetch full-texts and extract architecture descriptions, tasks/datasets used, metrics, and key results for each paper.",
        "queries": [],
        "papers": [
          "arxiv:2509.07887v1",
          "arxiv:2509.10871v1",
          "arxiv:2506.15792v1",
          "arxiv:2410.07981v2",
          "arxiv:2509.22468v1",
          "arxiv:2509.22028v1",
          "arxiv:2509.17018v1",
          "arxiv:2509.11782v1"
        ],
        "focus": []
      },
      "result": "arxiv:2509.07887v1: processed 1 chunks\narxiv:2509.10871v1: processed 1 chunks\narxiv:2506.15792v1: processed 1 chunks\narxiv:2410.07981v2: processed 1 chunks\narxiv:2509.22468v1: processed 1 chunks\narxiv:2509.22028v1: processed 1 chunks\narxiv:2509.17018v1: processed 1 chunks\narxiv:2509.11782v1: processed 1 chunks",
      "planner_payload": {
        "action": "read",
        "papers": [
          "arxiv:2509.07887v1",
          "arxiv:2509.10871v1",
          "arxiv:2506.15792v1",
          "arxiv:2410.07981v2",
          "arxiv:2509.22468v1",
          "arxiv:2509.22028v1",
          "arxiv:2509.17018v1",
          "arxiv:2509.11782v1"
        ],
        "focus": [],
        "notes": "Selecting ~8 high-impact candidate papers to read next. I prioritized: (1) a recent survey for broad context (arXiv:2509.07887v1); (2) a paper on message passing best-practices (arXiv:2509.10871v1); (3) foundation-model/descriptor approaches (arXiv:2506.15792v1); (4) a simple multimodal baseline (MolMix arXiv:2410.07981v2); and (5–8) several recent specific-method papers addressing pretraining, long-range interactions, state-dependent properties, and multimodal regression (arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1). Goal for the executor: fetch full-texts and extract architecture descriptions, tasks/datasets used, metrics, and key results for each paper.",
        "todo": [
          {
            "title": "Search for candidate papers across arXiv and ML/chemistry conferences",
            "status": "done"
          },
          {
            "title": "Select ~8 high-impact papers (foundational + recent advances) to read",
            "status": "done"
          },
          {
            "title": "Read selected papers and extract model architectures, tasks, datasets, and performance",
            "status": "doing"
          }
        ]
      },
      "timestamp": 1760453828
    },
    {
      "step": 5,
      "action": {
        "action": "summarize",
        "notes": "Now that full-text chunks were fetched for the 8 prioritized papers, summarize each paper into architecture, inputs, tasks/datasets, metrics/results, ablations, compute/code, limitations. Then synthesize cross-paper patterns and list any missing details to follow up. This will convert the raw reads into a unified comparison useful for downstream recommendations and experiments.",
        "queries": [],
        "papers": [],
        "focus": [
          "For each of the 8 papers (arXiv:2509.07887v1, arXiv:2509.10871v1, arXiv:2506.15792v1, arXiv:2410.07981v2, arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1): extract a concise description of the model architecture (layer types, message-passing style, equivariance/ invariances, attention, global pooling, multimodal fusion), input modalities (graphs, 3D coordinates, descriptors, spectra, sequences), and any novel algorithmic contributions.",
          "List the primary supervised/self-supervised tasks and benchmarks used (QM9, MoleculeNet tasks, OGB, custom datasets), including dataset sizes and train/val/test splits if reported.",
          "Record reported performance metrics and numbers for key benchmarks (e.g., MAE/RMSE/AUC/R2), and note the baseline models compared against.",
          "Summarize pretraining strategies (contrastive, masked prediction, descriptor prediction, multimodal), dataset(s) used for pretraining, and any transfer/finetuning results.",
          "Collect ablation studies and diagnostics reported (which components matter), runtime/compute info (GPUs, training time, model parameter counts) when available, and whether code/data are released (links).",
          "Note reported limitations, failure modes, or suggested future work from each paper.",
          "Synthesize cross-paper comparisons: common successful design patterns (e.g., attention + spatial features, equivariant layers), dataset gaps, evaluation inconsistencies, and practical recommendations for building GNNs for molecules.",
          "Flag any missing details or clarifications needed (e.g., ambiguous hyperparams, training schedules) so we can follow up by retrieving specific sections or contacting authors."
        ]
      },
      "result": "Summarized 'For each of the 8 papers (arXiv:2509.07887v1, arXiv:2509.10871v1, arXiv:2506.15792v1, arXiv:2410.07981v2, arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1): extract a concise description of the model architecture (layer types, message-passing style, equivariance/ invariances, attention, global pooling, multimodal fusion), input modalities (graphs, 3D coordinates, descriptors, spectra, sequences), and any novel algorithmic contributions.'\nSummarized 'List the primary supervised/self-supervised tasks and benchmarks used (QM9, MoleculeNet tasks, OGB, custom datasets), including dataset sizes and train/val/test splits if reported.'\nSummarized 'Record reported performance metrics and numbers for key benchmarks (e.g., MAE/RMSE/AUC/R2), and note the baseline models compared against.'\nSummarized 'Summarize pretraining strategies (contrastive, masked prediction, descriptor prediction, multimodal), dataset(s) used for pretraining, and any transfer/finetuning results.'\nSummarized 'Collect ablation studies and diagnostics reported (which components matter), runtime/compute info (GPUs, training time, model parameter counts) when available, and whether code/data are released (links).'\nSummarized 'Note reported limitations, failure modes, or suggested future work from each paper.'\nSummarized 'Synthesize cross-paper comparisons: common successful design patterns (e.g., attention + spatial features, equivariant layers), dataset gaps, evaluation inconsistencies, and practical recommendations for building GNNs for molecules.'\nSummarized 'Flag any missing details or clarifications needed (e.g., ambiguous hyperparams, training schedules) so we can follow up by retrieving specific sections or contacting authors.'",
      "planner_payload": {
        "action": "summarize",
        "queries": [],
        "papers": [],
        "focus": [
          "For each of the 8 papers (arXiv:2509.07887v1, arXiv:2509.10871v1, arXiv:2506.15792v1, arXiv:2410.07981v2, arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1): extract a concise description of the model architecture (layer types, message-passing style, equivariance/ invariances, attention, global pooling, multimodal fusion), input modalities (graphs, 3D coordinates, descriptors, spectra, sequences), and any novel algorithmic contributions.",
          "List the primary supervised/self-supervised tasks and benchmarks used (QM9, MoleculeNet tasks, OGB, custom datasets), including dataset sizes and train/val/test splits if reported.",
          "Record reported performance metrics and numbers for key benchmarks (e.g., MAE/RMSE/AUC/R2), and note the baseline models compared against.",
          "Summarize pretraining strategies (contrastive, masked prediction, descriptor prediction, multimodal), dataset(s) used for pretraining, and any transfer/finetuning results.",
          "Collect ablation studies and diagnostics reported (which components matter), runtime/compute info (GPUs, training time, model parameter counts) when available, and whether code/data are released (links).",
          "Note reported limitations, failure modes, or suggested future work from each paper.",
          "Synthesize cross-paper comparisons: common successful design patterns (e.g., attention + spatial features, equivariant layers), dataset gaps, evaluation inconsistencies, and practical recommendations for building GNNs for molecules.",
          "Flag any missing details or clarifications needed (e.g., ambiguous hyperparams, training schedules) so we can follow up by retrieving specific sections or contacting authors."
        ],
        "notes": "Now that full-text chunks were fetched for the 8 prioritized papers, summarize each paper into architecture, inputs, tasks/datasets, metrics/results, ablations, compute/code, limitations. Then synthesize cross-paper patterns and list any missing details to follow up. This will convert the raw reads into a unified comparison useful for downstream recommendations and experiments.",
        "todo": [
          {
            "title": "Search for candidate papers across arXiv and ML/chemistry conferences",
            "status": "done"
          },
          {
            "title": "Select ~8 high-impact papers (foundational + recent advances) to read",
            "status": "done"
          },
          {
            "title": "Read selected papers and extract model architectures, tasks, datasets, and performance",
            "status": "doing"
          },
          {
            "title": "Summarize and synthesize findings across the 8 papers",
            "status": "doing"
          },
          {
            "title": "Follow up on missing details (hyperparams, compute) and collect code links",
            "status": "todo"
          },
          {
            "title": "Produce final recommendation: model choices, pretraining strategy, evaluation plan",
            "status": "todo"
          }
        ]
      },
      "timestamp": 1760453978
    },
    {
      "step": 6,
      "action": {
        "action": "read",
        "notes": "Follow-up read pass: extract missing reproducibility and implementation details from the eight prioritized papers. For each paper, capture (1) exact hyperparameters and training schedules (optimizers, LR, batch size, epochs, weight decay, schedulers, warmup), (2) model sizes / parameter counts, (3) GPU/hardware used and wall-clock training time if reported, (4) precise dataset splits / preprocessing steps, (5) ablation table values and experiment settings needed to reproduce reported ablations, (6) links to code, checkpoints, and supplementary material, and (7) any authors' notes about randomness / seeds and license. If items are absent, flag them explicitly so we can request them from authors or search supplementary files/associated repos.",
        "queries": [],
        "papers": [
          "arxiv:2509.07887v1",
          "arxiv:2509.10871v1",
          "arxiv:2506.15792v1",
          "arxiv:2410.07981v2",
          "arxiv:2509.22468v1",
          "arxiv:2509.22028v1",
          "arxiv:2509.17018v1",
          "arxiv:2509.11782v1"
        ],
        "focus": []
      },
      "result": "arxiv:2509.07887v1: processed 1 chunks\narxiv:2509.10871v1: processed 1 chunks\narxiv:2506.15792v1: processed 1 chunks\narxiv:2410.07981v2: processed 1 chunks\narxiv:2509.22468v1: processed 1 chunks\narxiv:2509.22028v1: processed 1 chunks\narxiv:2509.17018v1: processed 1 chunks\narxiv:2509.11782v1: processed 1 chunks",
      "planner_payload": {
        "action": "read",
        "papers": [
          "arxiv:2509.07887v1",
          "arxiv:2509.10871v1",
          "arxiv:2506.15792v1",
          "arxiv:2410.07981v2",
          "arxiv:2509.22468v1",
          "arxiv:2509.22028v1",
          "arxiv:2509.17018v1",
          "arxiv:2509.11782v1"
        ],
        "notes": "Follow-up read pass: extract missing reproducibility and implementation details from the eight prioritized papers. For each paper, capture (1) exact hyperparameters and training schedules (optimizers, LR, batch size, epochs, weight decay, schedulers, warmup), (2) model sizes / parameter counts, (3) GPU/hardware used and wall-clock training time if reported, (4) precise dataset splits / preprocessing steps, (5) ablation table values and experiment settings needed to reproduce reported ablations, (6) links to code, checkpoints, and supplementary material, and (7) any authors' notes about randomness / seeds and license. If items are absent, flag them explicitly so we can request them from authors or search supplementary files/associated repos.",
        "todo": [
          {
            "title": "Search for candidate papers across arXiv and ML/chemistry conferences",
            "status": "done"
          },
          {
            "title": "Select ~8 high-impact papers (foundational + recent advances) to read",
            "status": "done"
          },
          {
            "title": "Read selected papers and extract model architectures, tasks, datasets, and performance",
            "status": "doing"
          },
          {
            "title": "Summarize and synthesize findings across the 8 papers",
            "status": "doing"
          },
          {
            "title": "Follow up on missing details (hyperparams, compute) and collect code links",
            "status": "doing"
          },
          {
            "title": "Produce final recommendation: model choices, pretraining strategy, evaluation plan",
            "status": "todo"
          }
        ]
      },
      "timestamp": 1760454359
    },
    {
      "step": 7,
      "action": {
        "action": "summarize",
        "notes": "Now synthesize the follow-up read pass: focus on extracting reproducibility and implementation details across the 8 prioritized papers, flag all missing items explicitly, and produce prioritized follow-ups and minimal reproduction plans for the top-performing works. This will prepare us to (a) request missing info or find repos and (b) estimate compute and timelines for reproductions.",
        "queries": [],
        "papers": [],
        "focus": [
          "For each of the 8 papers (arXiv:2509.07887v1, arXiv:2509.10871v1, arXiv:2506.15792v1, arXiv:2410.07981v2, arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1) produce a reproducibility checklist entry containing: optimizer, learning rate(s) and schedule, batch size, epochs, weight decay, scheduler/warmup details, gradient clipping, data augmentations, loss functions and weights, random seeds (if given), initialization, and any training tricks (mixed precision, gradient accumulation). If an item is not reported, mark it explicitly as \"missing\".",
          "Extract model sizes and parameter counts (total params, number of layers/heads where relevant), and report any architecture hyperparameters (embedding dims, MPNN steps, attention heads, equivariant block counts). If not reported, mark as \"missing\".",
          "Collect compute/hardware and runtime info: GPU types/count, effective batch size per GPU, wall-clock training time per run/epoch, and total FLOPS or estimated costs if reported. Mark missing items explicitly.",
          "List exact datasets and splits used for each experiment (train/val/test sizes, random split vs scaffold/time-split, any preprocessing or featurization steps like conformer generation parameters), and provide citations/links to dataset versions. Flag unknown or ambiguous splits.",
          "Extract ablation table values and the experimental settings for each ablation (which hyperparameters held fixed, which changed). If only qualitative statements provided, note that and capture exact quantitative entries where available.",
          "Enumerate code and checkpoint availability: provide GitHub/Zenodo/other links, branch/commit if available, reported license, and whether pretrained checkpoints are released. If link absent, record 'no link reported' and suggest likely search terms (repo name or project/title).",
          "Summarize reported randomness/reproducibility notes: seeds, number of runs (mean/std), deterministic settings, and any stated variance across seeds. Mark missing or single-run results.",
          "Produce a short cross-paper table (textual) highlighting which reproducibility items are consistently missing across papers (e.g., GPU hours, exact LR schedule), and prioritize follow-ups: (1) immediate — code links & hyperparams, (2) medium — dataset splits/ preprocessing, (3) optional — hardware/time estimates and exact seeds.",
          "For the top 3 papers that showed strongest empirical results (per our earlier summaries), propose precise next actions to reproduce their reported best results: which repository/branch to check, which hyperparameters to assume if missing, an estimated compute budget (GPU type, hours), and a minimal reproduction protocol (dataset split, seed choices, metric reporting).",
          "Generate a short template email / issue text to request missing details from authors (targeting hyperparams, seeds, and code availability) with a checklist so we can paste it when contacting authors or opening issues."
        ]
      },
      "result": "Summarized 'For each of the 8 papers (arXiv:2509.07887v1, arXiv:2509.10871v1, arXiv:2506.15792v1, arXiv:2410.07981v2, arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1) produce a reproducibility checklist entry containing: optimizer, learning rate(s) and schedule, batch size, epochs, weight decay, scheduler/warmup details, gradient clipping, data augmentations, loss functions and weights, random seeds (if given), initialization, and any training tricks (mixed precision, gradient accumulation). If an item is not reported, mark it explicitly as \"missing\".'\nSummarized 'Extract model sizes and parameter counts (total params, number of layers/heads where relevant), and report any architecture hyperparameters (embedding dims, MPNN steps, attention heads, equivariant block counts). If not reported, mark as \"missing\".'\nSummarized 'Collect compute/hardware and runtime info: GPU types/count, effective batch size per GPU, wall-clock training time per run/epoch, and total FLOPS or estimated costs if reported. Mark missing items explicitly.'\nSummarized 'List exact datasets and splits used for each experiment (train/val/test sizes, random split vs scaffold/time-split, any preprocessing or featurization steps like conformer generation parameters), and provide citations/links to dataset versions. Flag unknown or ambiguous splits.'\nSummarized 'Extract ablation table values and the experimental settings for each ablation (which hyperparameters held fixed, which changed). If only qualitative statements provided, note that and capture exact quantitative entries where available.'\nSummarized 'Enumerate code and checkpoint availability: provide GitHub/Zenodo/other links, branch/commit if available, reported license, and whether pretrained checkpoints are released. If link absent, record 'no link reported' and suggest likely search terms (repo name or project/title).'\nSummarized 'Summarize reported randomness/reproducibility notes: seeds, number of runs (mean/std), deterministic settings, and any stated variance across seeds. Mark missing or single-run results.'\nSummarized 'Produce a short cross-paper table (textual) highlighting which reproducibility items are consistently missing across papers (e.g., GPU hours, exact LR schedule), and prioritize follow-ups: (1) immediate — code links & hyperparams, (2) medium — dataset splits/ preprocessing, (3) optional — hardware/time estimates and exact seeds.'\nSummarized 'For the top 3 papers that showed strongest empirical results (per our earlier summaries), propose precise next actions to reproduce their reported best results: which repository/branch to check, which hyperparameters to assume if missing, an estimated compute budget (GPU type, hours), and a minimal reproduction protocol (dataset split, seed choices, metric reporting).'\nSummarized 'Generate a short template email / issue text to request missing details from authors (targeting hyperparams, seeds, and code availability) with a checklist so we can paste it when contacting authors or opening issues.'",
      "planner_payload": {
        "action": "summarize",
        "focus": [
          "For each of the 8 papers (arXiv:2509.07887v1, arXiv:2509.10871v1, arXiv:2506.15792v1, arXiv:2410.07981v2, arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1) produce a reproducibility checklist entry containing: optimizer, learning rate(s) and schedule, batch size, epochs, weight decay, scheduler/warmup details, gradient clipping, data augmentations, loss functions and weights, random seeds (if given), initialization, and any training tricks (mixed precision, gradient accumulation). If an item is not reported, mark it explicitly as \"missing\".",
          "Extract model sizes and parameter counts (total params, number of layers/heads where relevant), and report any architecture hyperparameters (embedding dims, MPNN steps, attention heads, equivariant block counts). If not reported, mark as \"missing\".",
          "Collect compute/hardware and runtime info: GPU types/count, effective batch size per GPU, wall-clock training time per run/epoch, and total FLOPS or estimated costs if reported. Mark missing items explicitly.",
          "List exact datasets and splits used for each experiment (train/val/test sizes, random split vs scaffold/time-split, any preprocessing or featurization steps like conformer generation parameters), and provide citations/links to dataset versions. Flag unknown or ambiguous splits.",
          "Extract ablation table values and the experimental settings for each ablation (which hyperparameters held fixed, which changed). If only qualitative statements provided, note that and capture exact quantitative entries where available.",
          "Enumerate code and checkpoint availability: provide GitHub/Zenodo/other links, branch/commit if available, reported license, and whether pretrained checkpoints are released. If link absent, record 'no link reported' and suggest likely search terms (repo name or project/title).",
          "Summarize reported randomness/reproducibility notes: seeds, number of runs (mean/std), deterministic settings, and any stated variance across seeds. Mark missing or single-run results.",
          "Produce a short cross-paper table (textual) highlighting which reproducibility items are consistently missing across papers (e.g., GPU hours, exact LR schedule), and prioritize follow-ups: (1) immediate — code links & hyperparams, (2) medium — dataset splits/ preprocessing, (3) optional — hardware/time estimates and exact seeds.",
          "For the top 3 papers that showed strongest empirical results (per our earlier summaries), propose precise next actions to reproduce their reported best results: which repository/branch to check, which hyperparameters to assume if missing, an estimated compute budget (GPU type, hours), and a minimal reproduction protocol (dataset split, seed choices, metric reporting).",
          "Generate a short template email / issue text to request missing details from authors (targeting hyperparams, seeds, and code availability) with a checklist so we can paste it when contacting authors or opening issues."
        ],
        "notes": "Now synthesize the follow-up read pass: focus on extracting reproducibility and implementation details across the 8 prioritized papers, flag all missing items explicitly, and produce prioritized follow-ups and minimal reproduction plans for the top-performing works. This will prepare us to (a) request missing info or find repos and (b) estimate compute and timelines for reproductions.",
        "todo": [
          {
            "title": "Search for candidate papers across arXiv and ML/chemistry conferences",
            "status": "done"
          },
          {
            "title": "Select ~8 high-impact papers (foundational + recent advances) to read",
            "status": "done"
          },
          {
            "title": "Read selected papers and extract model architectures, tasks, datasets, and performance",
            "status": "done"
          },
          {
            "title": "Summarize and synthesize findings across the 8 papers",
            "status": "doing"
          },
          {
            "title": "Follow up on missing details (hyperparams, compute) and collect code links",
            "status": "doing"
          },
          {
            "title": "Produce final recommendation: model choices, pretraining strategy, evaluation plan",
            "status": "todo"
          }
        ]
      },
      "timestamp": 1760454785
    },
    {
      "step": 8,
      "action": {
        "action": "summarize",
        "notes": "Synthesize the follow-up read pass into explicit reproducibility checklists and prioritized follow-ups so we can (a) locate code/checkpoints quickly, (b) decide which missing hyperparameters to assume vs request, and (c) prepare minimal reproduction runs for the strongest papers.",
        "queries": [],
        "papers": [],
        "focus": [
          "Per-paper reproducibility checklist (for each of the 8 papers): optimizer, initial LR and LR schedule, batch size, epochs, weight decay, scheduler/warmup, gradient clipping, data augmentations, loss(es) and loss weights, random seeds (or 'missing'), initialization scheme, mixed precision/accumulation, and any training tricks. Mark any item not reported as 'missing'.",
          "Per-paper model specification: total parameter count (or 'missing'), embedding dims, number of message-passing steps / equivariant blocks, attention heads, pooling method, and any architecture hyperparameters. Mark missing items.",
          "Per-paper compute & runtime: GPU type(s) and count, effective batch size per GPU, wall-clock training time (per epoch / total) or 'missing', and any reported FLOPs or cost estimates.",
          "Per-paper datasets & splits: exact dataset versions used, train/val/test sizes, split strategy (random/scaffold/time), preprocessing steps (conformer generation parameters, featurizers), and citations/links. Flag ambiguous or unreported splits.",
          "Extract ablation table entries and precise experimental settings (what was held fixed, what varied). If only qualitative statements present, record that and list exact quantitative values where available.",
          "Code and checkpoint availability: provide GitHub/Zenodo/other links, branch/commit if reported, license, and whether pretrained checkpoints are released. If no link is reported, list likely search terms to find repos.",
          "Randomness & reproducibility reporting: number of runs per experiment (mean/std) or single-run; any deterministic flags; reported variance across seeds or missing.",
          "Cross-paper summary table (textual): list reproducibility items most commonly missing (ranked), and produce prioritized follow-ups: (1) immediate — code & exact hyperparameters, (2) medium — dataset splits & preprocessing, (3) optional — hardware/time estimates and exact seeds.",
          "Identify the top-3 empirically strongest papers (based on our earlier summaries) and for each produce a minimal reproduction plan: repository/branch to check (or search hints), assumed hyperparameters if missing (conservative defaults), an estimated compute budget (GPU type + hours), and a minimal experiment protocol (split, seeds, metrics to report).",
          "Generate a short template email/issue to send to authors requesting missing reproducibility details (hyperparams, seeds, code/checkpoints, dataset splits), with a concise checklist so we can paste it into GitHub issues or emails."
        ]
      },
      "result": "Summarized 'Per-paper reproducibility checklist (for each of the 8 papers): optimizer, initial LR and LR schedule, batch size, epochs, weight decay, scheduler/warmup, gradient clipping, data augmentations, loss(es) and loss weights, random seeds (or 'missing'), initialization scheme, mixed precision/accumulation, and any training tricks. Mark any item not reported as 'missing'.'\nSummarized 'Per-paper model specification: total parameter count (or 'missing'), embedding dims, number of message-passing steps / equivariant blocks, attention heads, pooling method, and any architecture hyperparameters. Mark missing items.'\nSummarized 'Per-paper compute & runtime: GPU type(s) and count, effective batch size per GPU, wall-clock training time (per epoch / total) or 'missing', and any reported FLOPs or cost estimates.'\nSummarized 'Per-paper datasets & splits: exact dataset versions used, train/val/test sizes, split strategy (random/scaffold/time), preprocessing steps (conformer generation parameters, featurizers), and citations/links. Flag ambiguous or unreported splits.'\nSummarized 'Extract ablation table entries and precise experimental settings (what was held fixed, what varied). If only qualitative statements present, record that and list exact quantitative values where available.'\nSummarized 'Code and checkpoint availability: provide GitHub/Zenodo/other links, branch/commit if reported, license, and whether pretrained checkpoints are released. If no link is reported, list likely search terms to find repos.'\nSummarized 'Randomness & reproducibility reporting: number of runs per experiment (mean/std) or single-run; any deterministic flags; reported variance across seeds or missing.'\nSummarized 'Cross-paper summary table (textual): list reproducibility items most commonly missing (ranked), and produce prioritized follow-ups: (1) immediate — code & exact hyperparameters, (2) medium — dataset splits & preprocessing, (3) optional — hardware/time estimates and exact seeds.'\nSummarized 'Identify the top-3 empirically strongest papers (based on our earlier summaries) and for each produce a minimal reproduction plan: repository/branch to check (or search hints), assumed hyperparameters if missing (conservative defaults), an estimated compute budget (GPU type + hours), and a minimal experiment protocol (split, seeds, metrics to report).'\nSummarized 'Generate a short template email/issue to send to authors requesting missing reproducibility details (hyperparams, seeds, code/checkpoints, dataset splits), with a concise checklist so we can paste it into GitHub issues or emails.'",
      "planner_payload": {
        "action": "summarize",
        "focus": [
          "Per-paper reproducibility checklist (for each of the 8 papers): optimizer, initial LR and LR schedule, batch size, epochs, weight decay, scheduler/warmup, gradient clipping, data augmentations, loss(es) and loss weights, random seeds (or 'missing'), initialization scheme, mixed precision/accumulation, and any training tricks. Mark any item not reported as 'missing'.",
          "Per-paper model specification: total parameter count (or 'missing'), embedding dims, number of message-passing steps / equivariant blocks, attention heads, pooling method, and any architecture hyperparameters. Mark missing items.",
          "Per-paper compute & runtime: GPU type(s) and count, effective batch size per GPU, wall-clock training time (per epoch / total) or 'missing', and any reported FLOPs or cost estimates.",
          "Per-paper datasets & splits: exact dataset versions used, train/val/test sizes, split strategy (random/scaffold/time), preprocessing steps (conformer generation parameters, featurizers), and citations/links. Flag ambiguous or unreported splits.",
          "Extract ablation table entries and precise experimental settings (what was held fixed, what varied). If only qualitative statements present, record that and list exact quantitative values where available.",
          "Code and checkpoint availability: provide GitHub/Zenodo/other links, branch/commit if reported, license, and whether pretrained checkpoints are released. If no link is reported, list likely search terms to find repos.",
          "Randomness & reproducibility reporting: number of runs per experiment (mean/std) or single-run; any deterministic flags; reported variance across seeds or missing.",
          "Cross-paper summary table (textual): list reproducibility items most commonly missing (ranked), and produce prioritized follow-ups: (1) immediate — code & exact hyperparameters, (2) medium — dataset splits & preprocessing, (3) optional — hardware/time estimates and exact seeds.",
          "Identify the top-3 empirically strongest papers (based on our earlier summaries) and for each produce a minimal reproduction plan: repository/branch to check (or search hints), assumed hyperparameters if missing (conservative defaults), an estimated compute budget (GPU type + hours), and a minimal experiment protocol (split, seeds, metrics to report).",
          "Generate a short template email/issue to send to authors requesting missing reproducibility details (hyperparams, seeds, code/checkpoints, dataset splits), with a concise checklist so we can paste it into GitHub issues or emails."
        ],
        "notes": "Synthesize the follow-up read pass into explicit reproducibility checklists and prioritized follow-ups so we can (a) locate code/checkpoints quickly, (b) decide which missing hyperparameters to assume vs request, and (c) prepare minimal reproduction runs for the strongest papers.",
        "todo": [
          {
            "title": "Search for candidate papers across arXiv and ML/chemistry conferences",
            "status": "done"
          },
          {
            "title": "Select ~8 high-impact papers (foundational + recent advances) to read",
            "status": "done"
          },
          {
            "title": "Read selected papers and extract model architectures, tasks, datasets, and performance",
            "status": "done"
          },
          {
            "title": "Summarize and synthesize findings across the 8 papers",
            "status": "doing"
          },
          {
            "title": "Follow up on missing details (hyperparams, compute) and collect code links",
            "status": "doing"
          },
          {
            "title": "Produce final recommendation: model choices, pretraining strategy, evaluation plan",
            "status": "todo"
          }
        ]
      },
      "timestamp": 1760454946
    },
    {
      "step": 9,
      "action": {
        "action": "summarize",
        "notes": "Synthesize the follow-up read-pass outputs into explicit per-paper reproducibility checklists and a cross-paper prioritized follow-up plan. This will let us (a) rapidly find code/checkpoints, (b) decide safe default hyperparameters to use when authors omit details, and (c) prepare minimal, budgeted reproduction runs for the top-performing works.",
        "queries": [],
        "papers": [],
        "focus": [
          "For each of the 8 papers (arXiv:2509.07887v1, arXiv:2509.10871v1, arXiv:2506.15792v1, arXiv:2410.07981v2, arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1) produce a reproducibility checklist entry containing: optimizer, initial learning rate(s) and exact LR schedule (including warmup and decay), batch size (per-GPU and effective), epochs (or total training steps), weight decay, gradient clipping, data augmentations/augment strategies, loss functions and weights, random seed(s) or explicit 'missing', initialization scheme, mixed precision/gradient accumulation, and any other training tricks. Mark items not reported as 'missing'.",
          "Per-paper model specification: total parameter count (or 'missing'), embedding dimensions, number of message-passing steps / equivariant block counts, attention heads, pooling/readout method, and other architecture hyperparameters (e.g., cutoff radii, basis functions). If items are absent, mark 'missing'.",
          "Per-paper compute & runtime: reported GPU type(s) and count, effective batch size per device, wall-clock training time (per epoch or total) or 'missing', any reported FLOPs / cost estimates. If not reported, state 'missing' and provide conservative estimates where feasible.",
          "Per-paper datasets & splits: exact dataset names/versions, train/val/test sizes, split strategy (random/scaffold/time/fold), preprocessing steps (featurization, conformer generation parameters, augmentation), and citations/links. Flag ambiguous or unstated split strategies as 'missing/ambiguous'.",
          "Extract ablation table entries and experimental settings per ablation: record numerical results for each ablation row, and explicitly document which hyperparameters were held fixed vs varied. If ablations are only qualitative, mark as such and copy any reported qualitative conclusions.",
          "Code & checkpoint availability: list GitHub/Zenodo/other links and reported branch/commit and license (if provided). If no link is reported, return 'no link reported' and propose 2–3 likely search queries (repo name keywords, paper title tokens, author GitHub handles) to find the code.",
          "Randomness & reproducibility reporting: number of runs per experiment (mean/std) or single-run; any deterministic flags or seed-setting instructions; whether results averaged across seeds and what variance reported. Mark missing or underreported items.",
          "Cross-paper textual summary (compact table): rank reproducibility items by how often they are missing (e.g., GPU hours, exact LR schedule, seeds, total params). For each high-frequency missing item propose prioritized follow-ups: (1) immediate — find code & exact hyperparams, (2) medium — clarify dataset splits & preprocessing, (3) optional — request hardware/time estimates and exact seeds.",
          "Identify the top-3 empirically strongest papers (based on our earlier performance summaries). For each selected paper produce a minimal reproduction plan: (a) repository/branch search hints to check first, (b) assumed hyperparameters (conservative defaults) for any missing items, (c) estimated compute budget (GPU type and approximate hours), and (d) minimal experiment protocol (dataset split to use, 3 seeds to run, metrics to report, checkpointing schedule).",
          "Prepare a short template email / GitHub issue text (<=200 words) we can paste when contacting authors requesting missing reproducibility details: include a concise checklist (hyperparams, seeds, code/checkpoints, dataset splits, compute/hardware used) and polite closing asking for links or supplementary files."
        ]
      },
      "result": "Summarized 'For each of the 8 papers (arXiv:2509.07887v1, arXiv:2509.10871v1, arXiv:2506.15792v1, arXiv:2410.07981v2, arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1) produce a reproducibility checklist entry containing: optimizer, initial learning rate(s) and exact LR schedule (including warmup and decay), batch size (per-GPU and effective), epochs (or total training steps), weight decay, gradient clipping, data augmentations/augment strategies, loss functions and weights, random seed(s) or explicit 'missing', initialization scheme, mixed precision/gradient accumulation, and any other training tricks. Mark items not reported as 'missing'.'\nSummarized 'Per-paper model specification: total parameter count (or 'missing'), embedding dimensions, number of message-passing steps / equivariant block counts, attention heads, pooling/readout method, and other architecture hyperparameters (e.g., cutoff radii, basis functions). If items are absent, mark 'missing'.'\nSummarized 'Per-paper compute & runtime: reported GPU type(s) and count, effective batch size per device, wall-clock training time (per epoch or total) or 'missing', any reported FLOPs / cost estimates. If not reported, state 'missing' and provide conservative estimates where feasible.'\nSummarized 'Per-paper datasets & splits: exact dataset names/versions, train/val/test sizes, split strategy (random/scaffold/time/fold), preprocessing steps (featurization, conformer generation parameters, augmentation), and citations/links. Flag ambiguous or unstated split strategies as 'missing/ambiguous'.'\nSummarized 'Extract ablation table entries and experimental settings per ablation: record numerical results for each ablation row, and explicitly document which hyperparameters were held fixed vs varied. If ablations are only qualitative, mark as such and copy any reported qualitative conclusions.'\nSummarized 'Code & checkpoint availability: list GitHub/Zenodo/other links and reported branch/commit and license (if provided). If no link is reported, return 'no link reported' and propose 2–3 likely search queries (repo name keywords, paper title tokens, author GitHub handles) to find the code.'\nSummarized 'Randomness & reproducibility reporting: number of runs per experiment (mean/std) or single-run; any deterministic flags or seed-setting instructions; whether results averaged across seeds and what variance reported. Mark missing or underreported items.'\nSummarized 'Cross-paper textual summary (compact table): rank reproducibility items by how often they are missing (e.g., GPU hours, exact LR schedule, seeds, total params). For each high-frequency missing item propose prioritized follow-ups: (1) immediate — find code & exact hyperparams, (2) medium — clarify dataset splits & preprocessing, (3) optional — request hardware/time estimates and exact seeds.'\nSummarized 'Identify the top-3 empirically strongest papers (based on our earlier performance summaries). For each selected paper produce a minimal reproduction plan: (a) repository/branch search hints to check first, (b) assumed hyperparameters (conservative defaults) for any missing items, (c) estimated compute budget (GPU type and approximate hours), and (d) minimal experiment protocol (dataset split to use, 3 seeds to run, metrics to report, checkpointing schedule).'\nSummarized 'Prepare a short template email / GitHub issue text (<=200 words) we can paste when contacting authors requesting missing reproducibility details: include a concise checklist (hyperparams, seeds, code/checkpoints, dataset splits, compute/hardware used) and polite closing asking for links or supplementary files.'",
      "planner_payload": {
        "action": "summarize",
        "focus": [
          "For each of the 8 papers (arXiv:2509.07887v1, arXiv:2509.10871v1, arXiv:2506.15792v1, arXiv:2410.07981v2, arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1) produce a reproducibility checklist entry containing: optimizer, initial learning rate(s) and exact LR schedule (including warmup and decay), batch size (per-GPU and effective), epochs (or total training steps), weight decay, gradient clipping, data augmentations/augment strategies, loss functions and weights, random seed(s) or explicit 'missing', initialization scheme, mixed precision/gradient accumulation, and any other training tricks. Mark items not reported as 'missing'.",
          "Per-paper model specification: total parameter count (or 'missing'), embedding dimensions, number of message-passing steps / equivariant block counts, attention heads, pooling/readout method, and other architecture hyperparameters (e.g., cutoff radii, basis functions). If items are absent, mark 'missing'.",
          "Per-paper compute & runtime: reported GPU type(s) and count, effective batch size per device, wall-clock training time (per epoch or total) or 'missing', any reported FLOPs / cost estimates. If not reported, state 'missing' and provide conservative estimates where feasible.",
          "Per-paper datasets & splits: exact dataset names/versions, train/val/test sizes, split strategy (random/scaffold/time/fold), preprocessing steps (featurization, conformer generation parameters, augmentation), and citations/links. Flag ambiguous or unstated split strategies as 'missing/ambiguous'.",
          "Extract ablation table entries and experimental settings per ablation: record numerical results for each ablation row, and explicitly document which hyperparameters were held fixed vs varied. If ablations are only qualitative, mark as such and copy any reported qualitative conclusions.",
          "Code & checkpoint availability: list GitHub/Zenodo/other links and reported branch/commit and license (if provided). If no link is reported, return 'no link reported' and propose 2–3 likely search queries (repo name keywords, paper title tokens, author GitHub handles) to find the code.",
          "Randomness & reproducibility reporting: number of runs per experiment (mean/std) or single-run; any deterministic flags or seed-setting instructions; whether results averaged across seeds and what variance reported. Mark missing or underreported items.",
          "Cross-paper textual summary (compact table): rank reproducibility items by how often they are missing (e.g., GPU hours, exact LR schedule, seeds, total params). For each high-frequency missing item propose prioritized follow-ups: (1) immediate — find code & exact hyperparams, (2) medium — clarify dataset splits & preprocessing, (3) optional — request hardware/time estimates and exact seeds.",
          "Identify the top-3 empirically strongest papers (based on our earlier performance summaries). For each selected paper produce a minimal reproduction plan: (a) repository/branch search hints to check first, (b) assumed hyperparameters (conservative defaults) for any missing items, (c) estimated compute budget (GPU type and approximate hours), and (d) minimal experiment protocol (dataset split to use, 3 seeds to run, metrics to report, checkpointing schedule).",
          "Prepare a short template email / GitHub issue text (<=200 words) we can paste when contacting authors requesting missing reproducibility details: include a concise checklist (hyperparams, seeds, code/checkpoints, dataset splits, compute/hardware used) and polite closing asking for links or supplementary files."
        ],
        "notes": "Synthesize the follow-up read-pass outputs into explicit per-paper reproducibility checklists and a cross-paper prioritized follow-up plan. This will let us (a) rapidly find code/checkpoints, (b) decide safe default hyperparameters to use when authors omit details, and (c) prepare minimal, budgeted reproduction runs for the top-performing works.",
        "todo": [
          {
            "title": "Search for candidate papers across arXiv and ML/chemistry conferences",
            "status": "done"
          },
          {
            "title": "Select ~8 high-impact papers (foundational + recent advances) to read",
            "status": "done"
          },
          {
            "title": "Read selected papers and extract model architectures, tasks, datasets, and performance",
            "status": "done"
          },
          {
            "title": "Summarize and synthesize findings across the 8 papers",
            "status": "doing"
          },
          {
            "title": "Follow up on missing details (hyperparams, compute) and collect code links",
            "status": "doing"
          },
          {
            "title": "Produce final recommendation: model choices, pretraining strategy, evaluation plan",
            "status": "todo"
          }
        ]
      },
      "timestamp": 1760455170
    },
    {
      "step": 10,
      "action": {
        "action": "summarize",
        "notes": "We need a tight, actionable synthesis that merges the per-paper reproducibility checklists into a small set of deliverables: (1) code/checkpoint links or search queries to find them, (2) conservative default hyperparams for missing items, (3) minimal reproduction protocols and compute estimates for the top-3 papers, and (4) a short recommendation outline for model + pretraining + evaluation. This will let the team quickly run budgeted reproductions and decide which models to build upon.",
        "queries": [],
        "papers": [],
        "focus": [
          "Cross-paper comparison: produce a compact side-by-side summary (one-paragraph or bullet per paper) of architecture (message-passing / MPNN, SE(3)-equivariant, transformer), key inductive biases (edge/angle features, spherical harmonics, equivariance), input modalities (2D graph, 3D coordinates, conformers), parameter counts, primary tasks/datasets (QM9, OC20, MoleculeNet, etc.), and headline performance numbers (best metric and baseline).",
          "Reproducibility consolidation: merge per-paper checklists into a cross-paper matrix that flags which reproducibility items are present vs missing (optimizer, initial LR, LR schedule, batch size, epochs, weight decay, warmup, gradient clipping, augmentations, loss weights, seeds, init scheme, mixed precision, GPU hours). For each missing item, propose a conservative default and label it 'assume' vs 'request'.",
          "Code & checkpoint collection: for each of the 10 known paper IDs produce (a) reported code/checkpoint link(s) if present with license/commit when available, (b) if no link reported provide 2–3 concrete repo-search queries (keywords, likely repo names, author GitHub handles) to locate the implementation, and (c) a quick plausibility score (high/medium/low) for whether that repo corresponds to the reported results.",
          "Prioritized follow-ups: output an actionable prioritized list (Immediate / Medium / Optional) for missing reproducibility items across the corpus—Immediate = find code & exact hyperparams, Medium = clarify dataset splits & preprocessing, Optional = request hardware/time estimates and seed dumps. For Immediate items give the exact next search action (repo search query or author contact).",
          "Minimal reproduction plans (concise templates): for the top-3 empirically strongest papers produce a one-paragraph minimal reproduction protocol each covering: repo/branch search hints, assumed hyperparams (if missing), compute estimate (GPU type & hours), dataset split to use, 3 seeds, metrics to report, checkpointing schedule, and expected runtime per seed.",
          "Final recommendation sketch (outline): based on the synthesis, recommend (a) model family choice(s) for molecule property prediction vs force-field/energy tasks, (b) pretraining strategies to try (masked node/edge, denoising, contrastive, equivariant denoising), (c) a prioritized evaluation plan (benchmarks, metrics, baselines), and (d) a conservative compute/hardware plan for initial experiments."
        ]
      },
      "result": "Summarized 'Cross-paper comparison: produce a compact side-by-side summary (one-paragraph or bullet per paper) of architecture (message-passing / MPNN, SE(3)-equivariant, transformer), key inductive biases (edge/angle features, spherical harmonics, equivariance), input modalities (2D graph, 3D coordinates, conformers), parameter counts, primary tasks/datasets (QM9, OC20, MoleculeNet, etc.), and headline performance numbers (best metric and baseline).'\nSummarized 'Reproducibility consolidation: merge per-paper checklists into a cross-paper matrix that flags which reproducibility items are present vs missing (optimizer, initial LR, LR schedule, batch size, epochs, weight decay, warmup, gradient clipping, augmentations, loss weights, seeds, init scheme, mixed precision, GPU hours). For each missing item, propose a conservative default and label it 'assume' vs 'request'.'\nSummarized 'Code & checkpoint collection: for each of the 10 known paper IDs produce (a) reported code/checkpoint link(s) if present with license/commit when available, (b) if no link reported provide 2–3 concrete repo-search queries (keywords, likely repo names, author GitHub handles) to locate the implementation, and (c) a quick plausibility score (high/medium/low) for whether that repo corresponds to the reported results.'\nSummarized 'Prioritized follow-ups: output an actionable prioritized list (Immediate / Medium / Optional) for missing reproducibility items across the corpus—Immediate = find code & exact hyperparams, Medium = clarify dataset splits & preprocessing, Optional = request hardware/time estimates and seed dumps. For Immediate items give the exact next search action (repo search query or author contact).'\nSummarized 'Minimal reproduction plans (concise templates): for the top-3 empirically strongest papers produce a one-paragraph minimal reproduction protocol each covering: repo/branch search hints, assumed hyperparams (if missing), compute estimate (GPU type & hours), dataset split to use, 3 seeds, metrics to report, checkpointing schedule, and expected runtime per seed.'\nSummarized 'Final recommendation sketch (outline): based on the synthesis, recommend (a) model family choice(s) for molecule property prediction vs force-field/energy tasks, (b) pretraining strategies to try (masked node/edge, denoising, contrastive, equivariant denoising), (c) a prioritized evaluation plan (benchmarks, metrics, baselines), and (d) a conservative compute/hardware plan for initial experiments.'",
      "planner_payload": {
        "action": "summarize",
        "focus": [
          "Cross-paper comparison: produce a compact side-by-side summary (one-paragraph or bullet per paper) of architecture (message-passing / MPNN, SE(3)-equivariant, transformer), key inductive biases (edge/angle features, spherical harmonics, equivariance), input modalities (2D graph, 3D coordinates, conformers), parameter counts, primary tasks/datasets (QM9, OC20, MoleculeNet, etc.), and headline performance numbers (best metric and baseline).",
          "Reproducibility consolidation: merge per-paper checklists into a cross-paper matrix that flags which reproducibility items are present vs missing (optimizer, initial LR, LR schedule, batch size, epochs, weight decay, warmup, gradient clipping, augmentations, loss weights, seeds, init scheme, mixed precision, GPU hours). For each missing item, propose a conservative default and label it 'assume' vs 'request'.",
          "Code & checkpoint collection: for each of the 10 known paper IDs produce (a) reported code/checkpoint link(s) if present with license/commit when available, (b) if no link reported provide 2–3 concrete repo-search queries (keywords, likely repo names, author GitHub handles) to locate the implementation, and (c) a quick plausibility score (high/medium/low) for whether that repo corresponds to the reported results.",
          "Prioritized follow-ups: output an actionable prioritized list (Immediate / Medium / Optional) for missing reproducibility items across the corpus—Immediate = find code & exact hyperparams, Medium = clarify dataset splits & preprocessing, Optional = request hardware/time estimates and seed dumps. For Immediate items give the exact next search action (repo search query or author contact).",
          "Minimal reproduction plans (concise templates): for the top-3 empirically strongest papers produce a one-paragraph minimal reproduction protocol each covering: repo/branch search hints, assumed hyperparams (if missing), compute estimate (GPU type & hours), dataset split to use, 3 seeds, metrics to report, checkpointing schedule, and expected runtime per seed.",
          "Final recommendation sketch (outline): based on the synthesis, recommend (a) model family choice(s) for molecule property prediction vs force-field/energy tasks, (b) pretraining strategies to try (masked node/edge, denoising, contrastive, equivariant denoising), (c) a prioritized evaluation plan (benchmarks, metrics, baselines), and (d) a conservative compute/hardware plan for initial experiments."
        ],
        "notes": "We need a tight, actionable synthesis that merges the per-paper reproducibility checklists into a small set of deliverables: (1) code/checkpoint links or search queries to find them, (2) conservative default hyperparams for missing items, (3) minimal reproduction protocols and compute estimates for the top-3 papers, and (4) a short recommendation outline for model + pretraining + evaluation. This will let the team quickly run budgeted reproductions and decide which models to build upon.",
        "todo": [
          {
            "title": "Synthesize cross-paper comparison and reproducibility matrix",
            "status": "doing"
          },
          {
            "title": "Collect code/checkpoint links and generate repo-search queries for missing code",
            "status": "doing"
          },
          {
            "title": "Produce final recommendation (model choices, pretraining, evaluation plan, compute budget)",
            "status": "todo"
          }
        ]
      },
      "timestamp": 1760455385
    },
    {
      "step": 11,
      "action": {
        "action": "summarize",
        "notes": "We are in the synthesis phase: consolidate per-paper checklists and code links into a compact actionable package, propose conservative defaults for missing hyperparams, and produce minimal reproduction protocols and final recommendations so the team can begin budgeted replication and follow-up experiments.",
        "queries": [],
        "papers": [],
        "focus": [
          "Produce a compact per-paper bullet (1 para or 4 bullets) for the 8 known papers (ids in state) covering: model family/architecture, key inductive biases (equivariance, angles, spherical harmonics, edge features), input modality (2D graph, 3D coords, conformers), parameter count (if available), primary tasks/datasets and top reported metrics.",
          "Merge per-paper reproducibility checklists into a single cross-paper matrix that explicitly lists: optimizer, initial LR, LR schedule, batch size, epochs, weight decay, warmup, gradient clipping, augmentations, loss weights, seeds, init scheme, mixed precision, GPU hours. For each missing item propose a conservative default and label it 'assume' vs 'request'.",
          "For each of the 8 papers provide: (a) reported code/checkpoint link(s) with license/commit when present, (b) for missing links 2–3 repo-search queries (keywords, likely repo names, author GitHub handles), and (c) a plausibility score (high/medium/low) that a found repo matches reported results.",
          "Consolidate the prioritized follow-ups (Immediate / Medium / Optional) into a one-page action plan. For Immediate items include exact next search action (repo search query or author contact template) and target person (author handle if known).",
          "Deliver minimal reproduction protocols (one paragraph each) for the top-3 empirically strongest papers: repo/branch hints, assumed hyperparams (with 'assume' vs 'request' tags), compute estimate (GPU type & hours), dataset split/preprocessing, seeds, metrics to report, checkpointing schedule, runtime per seed.",
          "Produce a short final recommendation sketch (3–5 bullets) covering: model family(s) to prioritize for property prediction vs energy/force tasks, 2–3 pretraining strategies to try (with brief justification), prioritized evaluation plan (benchmarks, metrics, baselines), and a conservative compute/hardware plan for initial experiments (per-experiment GPU & hours)."
        ]
      },
      "result": "Summarized 'Produce a compact per-paper bullet (1 para or 4 bullets) for the 8 known papers (ids in state) covering: model family/architecture, key inductive biases (equivariance, angles, spherical harmonics, edge features), input modality (2D graph, 3D coords, conformers), parameter count (if available), primary tasks/datasets and top reported metrics.'\nSummarized 'Merge per-paper reproducibility checklists into a single cross-paper matrix that explicitly lists: optimizer, initial LR, LR schedule, batch size, epochs, weight decay, warmup, gradient clipping, augmentations, loss weights, seeds, init scheme, mixed precision, GPU hours. For each missing item propose a conservative default and label it 'assume' vs 'request'.'\nSummarized 'For each of the 8 papers provide: (a) reported code/checkpoint link(s) with license/commit when present, (b) for missing links 2–3 repo-search queries (keywords, likely repo names, author GitHub handles), and (c) a plausibility score (high/medium/low) that a found repo matches reported results.'\nSummarized 'Consolidate the prioritized follow-ups (Immediate / Medium / Optional) into a one-page action plan. For Immediate items include exact next search action (repo search query or author contact template) and target person (author handle if known).'\nSummarized 'Deliver minimal reproduction protocols (one paragraph each) for the top-3 empirically strongest papers: repo/branch hints, assumed hyperparams (with 'assume' vs 'request' tags), compute estimate (GPU type & hours), dataset split/preprocessing, seeds, metrics to report, checkpointing schedule, runtime per seed.'\nSummarized 'Produce a short final recommendation sketch (3–5 bullets) covering: model family(s) to prioritize for property prediction vs energy/force tasks, 2–3 pretraining strategies to try (with brief justification), prioritized evaluation plan (benchmarks, metrics, baselines), and a conservative compute/hardware plan for initial experiments (per-experiment GPU & hours).'",
      "planner_payload": {
        "action": "summarize",
        "focus": [
          "Produce a compact per-paper bullet (1 para or 4 bullets) for the 8 known papers (ids in state) covering: model family/architecture, key inductive biases (equivariance, angles, spherical harmonics, edge features), input modality (2D graph, 3D coords, conformers), parameter count (if available), primary tasks/datasets and top reported metrics.",
          "Merge per-paper reproducibility checklists into a single cross-paper matrix that explicitly lists: optimizer, initial LR, LR schedule, batch size, epochs, weight decay, warmup, gradient clipping, augmentations, loss weights, seeds, init scheme, mixed precision, GPU hours. For each missing item propose a conservative default and label it 'assume' vs 'request'.",
          "For each of the 8 papers provide: (a) reported code/checkpoint link(s) with license/commit when present, (b) for missing links 2–3 repo-search queries (keywords, likely repo names, author GitHub handles), and (c) a plausibility score (high/medium/low) that a found repo matches reported results.",
          "Consolidate the prioritized follow-ups (Immediate / Medium / Optional) into a one-page action plan. For Immediate items include exact next search action (repo search query or author contact template) and target person (author handle if known).",
          "Deliver minimal reproduction protocols (one paragraph each) for the top-3 empirically strongest papers: repo/branch hints, assumed hyperparams (with 'assume' vs 'request' tags), compute estimate (GPU type & hours), dataset split/preprocessing, seeds, metrics to report, checkpointing schedule, runtime per seed.",
          "Produce a short final recommendation sketch (3–5 bullets) covering: model family(s) to prioritize for property prediction vs energy/force tasks, 2–3 pretraining strategies to try (with brief justification), prioritized evaluation plan (benchmarks, metrics, baselines), and a conservative compute/hardware plan for initial experiments (per-experiment GPU & hours)."
        ],
        "notes": "We are in the synthesis phase: consolidate per-paper checklists and code links into a compact actionable package, propose conservative defaults for missing hyperparams, and produce minimal reproduction protocols and final recommendations so the team can begin budgeted replication and follow-up experiments.",
        "todo": [
          {
            "title": "Summarize and synthesize findings across the 8 papers",
            "status": "doing"
          },
          {
            "title": "Follow up on missing details (hyperparams, compute) and collect code links",
            "status": "doing"
          },
          {
            "title": "Produce final recommendation: model choices, pretraining strategy, evaluation plan",
            "status": "todo"
          },
          {
            "title": "Prepare minimal reproduction protocols for top-3 papers (deliverable-ready)",
            "status": "todo"
          },
          {
            "title": "Assemble final deliverable (one-page cheat sheet + repo links + action plan)",
            "status": "todo"
          }
        ]
      },
      "timestamp": 1760455488
    },
    {
      "step": 12,
      "action": {
        "action": "summarize",
        "notes": "Consolidate the per-paper checklists, code links, and conservative defaults into actionable deliverables so the team can start budgeted reproductions. The summary should produce ready-to-run reproduction paragraphs for the top-3 models plus a compact cross-paper hyperparameter matrix and precise next steps for obtaining missing artifacts.",
        "queries": [],
        "papers": [],
        "focus": [
          "Per-paper compact bullets (1 paragraph or 4 bullets) for each known paper id: architecture (MPNN / SE(3)-equivariant / Transformer), key inductive biases (edge/angle features, spherical harmonics, equivariance), input modality (2D graph, 3D coords, number of conformers), parameter count (or estimate), primary tasks/datasets, top reported metrics, code/checkpoint link or repo-search hints, and a short reproducibility checklist summary (optimizer, init LR, schedule, batch, epochs, weight decay, warmup, grad clipping, augmentations, loss weights, seeds, mixed precision, GPU hours) with each missing item tagged 'assume' or 'request'.",
          "Merge per-paper reproducibility checklists into a single cross-paper matrix: list each hyper/exp item as columns, fill reported values, and for missing entries propose a conservative default and tag 'assume' vs 'request'. Produce short justification for each chosen default (1 line).",
          "For papers lacking code/checkpoints: for each paper provide 2–3 targeted repo-search queries (keywords, likely repo name patterns, and probable author GitHub handles) and give a plausibility score (high/medium/low) that an identified repo matches reported results.",
          "Minimal reproduction protocol (one paragraph each) for the top-3 empirically strongest papers: include suggested repo/branch hints, assumed hyperparams with 'assume'/'request' tags, dataset splits/preprocessing, seeds, metrics to report, checkpointing schedule, estimated compute (GPU type & hours) and runtime per seed.",
          "Short final recommendation sketch (3–5 bullets) covering: model family(s) to prioritize for property prediction vs energy/force tasks, 2–3 pretraining strategies to try (with brief justification), prioritized evaluation plan (benchmarks, metrics, baselines), and a conservative compute/hardware plan for initial experiments (per-experiment GPU & hours).",
          "Prioritized follow-ups (Immediate / Medium / Optional): for each Immediate item include exact next search action (repo search query or author contact template) and a target person (author GitHub handle or contact if known)."
        ]
      },
      "result": "Summarized 'Per-paper compact bullets (1 paragraph or 4 bullets) for each known paper id: architecture (MPNN / SE(3)-equivariant / Transformer), key inductive biases (edge/angle features, spherical harmonics, equivariance), input modality (2D graph, 3D coords, number of conformers), parameter count (or estimate), primary tasks/datasets, top reported metrics, code/checkpoint link or repo-search hints, and a short reproducibility checklist summary (optimizer, init LR, schedule, batch, epochs, weight decay, warmup, grad clipping, augmentations, loss weights, seeds, mixed precision, GPU hours) with each missing item tagged 'assume' or 'request'.'\nSummarized 'Merge per-paper reproducibility checklists into a single cross-paper matrix: list each hyper/exp item as columns, fill reported values, and for missing entries propose a conservative default and tag 'assume' vs 'request'. Produce short justification for each chosen default (1 line).'\nSummarized 'For papers lacking code/checkpoints: for each paper provide 2–3 targeted repo-search queries (keywords, likely repo name patterns, and probable author GitHub handles) and give a plausibility score (high/medium/low) that an identified repo matches reported results.'\nSummarized 'Minimal reproduction protocol (one paragraph each) for the top-3 empirically strongest papers: include suggested repo/branch hints, assumed hyperparams with 'assume'/'request' tags, dataset splits/preprocessing, seeds, metrics to report, checkpointing schedule, estimated compute (GPU type & hours) and runtime per seed.'\nSummarized 'Short final recommendation sketch (3–5 bullets) covering: model family(s) to prioritize for property prediction vs energy/force tasks, 2–3 pretraining strategies to try (with brief justification), prioritized evaluation plan (benchmarks, metrics, baselines), and a conservative compute/hardware plan for initial experiments (per-experiment GPU & hours).'\nSummarized 'Prioritized follow-ups (Immediate / Medium / Optional): for each Immediate item include exact next search action (repo search query or author contact template) and a target person (author GitHub handle or contact if known).'",
      "planner_payload": {
        "action": "summarize",
        "focus": [
          "Per-paper compact bullets (1 paragraph or 4 bullets) for each known paper id: architecture (MPNN / SE(3)-equivariant / Transformer), key inductive biases (edge/angle features, spherical harmonics, equivariance), input modality (2D graph, 3D coords, number of conformers), parameter count (or estimate), primary tasks/datasets, top reported metrics, code/checkpoint link or repo-search hints, and a short reproducibility checklist summary (optimizer, init LR, schedule, batch, epochs, weight decay, warmup, grad clipping, augmentations, loss weights, seeds, mixed precision, GPU hours) with each missing item tagged 'assume' or 'request'.",
          "Merge per-paper reproducibility checklists into a single cross-paper matrix: list each hyper/exp item as columns, fill reported values, and for missing entries propose a conservative default and tag 'assume' vs 'request'. Produce short justification for each chosen default (1 line).",
          "For papers lacking code/checkpoints: for each paper provide 2–3 targeted repo-search queries (keywords, likely repo name patterns, and probable author GitHub handles) and give a plausibility score (high/medium/low) that an identified repo matches reported results.",
          "Minimal reproduction protocol (one paragraph each) for the top-3 empirically strongest papers: include suggested repo/branch hints, assumed hyperparams with 'assume'/'request' tags, dataset splits/preprocessing, seeds, metrics to report, checkpointing schedule, estimated compute (GPU type & hours) and runtime per seed.",
          "Short final recommendation sketch (3–5 bullets) covering: model family(s) to prioritize for property prediction vs energy/force tasks, 2–3 pretraining strategies to try (with brief justification), prioritized evaluation plan (benchmarks, metrics, baselines), and a conservative compute/hardware plan for initial experiments (per-experiment GPU & hours).",
          "Prioritized follow-ups (Immediate / Medium / Optional): for each Immediate item include exact next search action (repo search query or author contact template) and a target person (author GitHub handle or contact if known)."
        ],
        "notes": "Consolidate the per-paper checklists, code links, and conservative defaults into actionable deliverables so the team can start budgeted reproductions. The summary should produce ready-to-run reproduction paragraphs for the top-3 models plus a compact cross-paper hyperparameter matrix and precise next steps for obtaining missing artifacts.",
        "todo": [
          {
            "title": "Search for candidate papers across arXiv and ML/chemistry conferences",
            "status": "done"
          },
          {
            "title": "Select ~8 high-impact papers (foundational + recent advances) to read",
            "status": "done"
          },
          {
            "title": "Read selected papers and extract model architectures, tasks, datasets, and performance",
            "status": "done"
          },
          {
            "title": "Summarize and synthesize findings across the 8 papers",
            "status": "doing"
          },
          {
            "title": "Follow up on missing details (hyperparams, compute) and collect code links",
            "status": "doing"
          },
          {
            "title": "Produce final recommendation: model choices, pretraining strategy, evaluation plan",
            "status": "todo"
          }
        ]
      },
      "timestamp": 1760455598
    }
  ],
  "findings": [
    {
      "question": "For each of the 8 papers (arXiv:2509.07887v1, arXiv:2509.10871v1, arXiv:2506.15792v1, arXiv:2410.07981v2, arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1): extract a concise description of the model architecture (layer types, message-passing style, equivariance/ invariances, attention, global pooling, multimodal fusion), input modalities (graphs, 3D coordinates, descriptors, spectra, sequences), and any novel algorithmic contributions.",
      "answer": "1) arXiv:2410.07981v2\n- Model architecture: Multimodal transformer baseline. Modality-specific encoders: transformer for SMILES, message-passing neural network (MPNN) for 2D graphs, and an equivariant neural network for 3D conformers; the modality tokens are concatenated into a unified sequence and processed by a downstream transformer. Uses FlashAttention2 and bfloat16 for scaling.\n- Message passing / equivariance: 2D encoder is an MPNN; 3D encoder enforces equivariance (equivariant neural network).\n- Attention / pooling / fusion: Downstream transformer applies attention over the fused token sequence; aggregation of multiple 3D conformers is explicitly supported.\n- Input modalities: SMILES strings, 2D molecular graphs, 3D conformers (multiple conformations).\n- Novel contributions: Simple, modular multimodal fusion that aggregates multiple 3D conformers and demonstrates strong performance while being easy to swap encoders and efficiently scale (FlashAttention2, bfloat16) [arXiv:2410.07981v2].\n\n2) arXiv:2509.10871v1\n- Model architecture: Message-Passing Neural Network family with a minimalist message formulation (excluding self-perception), bidirectional message passing and an attention mechanism; experiments compare normalization/convolution factors.\n- Message passing / equivariance: Bidirectional MPNN message passing; not reported as equivariant.\n- Attention / pooling / fusion: Attention applied in message updates; both node-level and global outputs evaluated.\n- Input modalities: Molecular graphs (2D); optionally spatial features / 3D graphs were investigated.\n- Novel contributions: Empirical design findings showing that simpler MPNN messages (no self-perception) plus bidirectional attention yield SOTA performance on many datasets; analysis of dataset structural diversity and the (sometimes limited) benefit of normalization or adding spatial/3D features [arXiv:2509.10871v1].\n\n3) arXiv:2509.22028v1\n- Model architecture: Plug-and-play Multi-stage Clustered Global Modeling (MCGM) module that can be inserted into geometric GNN backbones. MCGM builds a multi-resolution hierarchy of atomic clusters, distills global information via dynamic hierarchical clustering, transforms and propagates global context back to atoms with residual connections.\n- Message passing / equivariance: Designed to augment locality-biased geometric GNNs (implies compatibility with existing geometric, likely equivariant, GNNs); core idea is hierarchical/global context rather than changing local equivariant operators.\n- Attention /",
      "citations": [
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.49170078041619186,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.4578290913437771,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.42241158902426174,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.07887v1:summary",
          "paper_id": "arxiv:2509.07887v1",
          "text": "Graph Neural Networks (GNNs) have gained traction in the complex domain of\ndrug discovery because of their ability to process graph-structured data such\nas drug molecule models. This approach has resulted in a myriad of methods and\nmodels in published literature across several categories of drug discovery\nresearch. This paper covers the research categories comprehensively with recent\npapers, namely molecular property prediction, including drug-target binding\naffinity prediction, drug-drug interaction study, microbiome interaction\nprediction, drug repositioning, retrosynthesis, and new drug design, and\nprovides guidance for future work on GNNs for drug discovery.",
          "score": 0.3841872722787258,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 5
    },
    {
      "question": "List the primary supervised/self-supervised tasks and benchmarks used (QM9, MoleculeNet tasks, OGB, custom datasets), including dataset sizes and train/val/test splits if reported.",
      "answer": "Below I list the primary supervised and self-supervised tasks / benchmarks mentioned in the provided excerpts, and report dataset sizes and train/val/test splits only where those were explicitly given in the excerpts.\n\n1) MoleculeNet (benchmark suite; supervised property prediction tasks)\n- Mentioned as an evaluation target / benchmark in multiple works (arxiv:2410.07981v2; arxiv:2509.22468v1).\n- Dataset sizes / train/val/test splits: not reported in the provided excerpts (standard MoleculeNet splits exist in the literature but are not stated here). [arxiv:2410.07981v2; arxiv:2509.22468v1]\n\n2) GEOM (ensemble of 3D conformers; used for pretraining)\n- Used as pretraining data for self-supervised methods that leverage conformer ensembles (C-FREE pretraining) (arxiv:2509.22468v1).\n- Dataset sizes / splits: not reported in the excerpt. [arxiv:2509.22468v1]\n\n3) Polaris and MoleculeACE benchmark collections (58 benchmark datasets referenced)\n- CheMeleon evaluated on 58 benchmark datasets from Polaris and MoleculeACE (supervised downstream evaluation), reporting win rates versus baselines (arxiv:2506.15792v1).\n- Dataset sizes / splits: not reported in the excerpt (only aggregate evaluation statistics like win rates are given). [arxiv:2506.15792v1]\n\n4) OE62 (energy prediction task / benchmark)\n- Used as an evaluation target for energy prediction accuracy improvements (MCGM paper reports reduction in OE62 energy prediction error) (arxiv:2509.22028v1).\n- Dataset sizes / splits: not reported in the excerpt. [arxiv:2509.22028v1]\n\n5",
      "citations": [
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.3957399634436476,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.3894007103312915,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.37016971843622926,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22468v1:summary",
          "paper_id": "arxiv:2509.22468v1",
          "text": "High-quality molecular representations are essential for property prediction\nand molecular design, yet large labeled datasets remain scarce. While\nself-supervised pretraining on molecular graphs has shown promise, many\nexisting approaches either depend on hand-crafted augmentations or complex\ngenerative objectives, and often rely solely on 2D topology, leaving valuable\n3D structural information underutilized. To address this gap, we introduce\nC-FREE (Contrast-Free Representation learning on Ego-nets), a simple framework\nthat integrates 2D graphs with ensembles of 3D conformers. C-FREE learns\nmolecular representations by predicting subgraph embeddings from their\ncomplementary neighborhoods in the latent space, using fixed-radius ego-nets as\nmodeling units across different conformers. This design allows us to integrate\nboth geometric and topological information within a hybrid Graph Neural Network\n(GNN)-Transformer backbone, without negatives, positional encodings, or\nexpensive pre-processing. Pretraining on the GEOM dataset, which provides rich\n3D conformational diversity, C-FREE achieves state-of-the-art results on\nMoleculeNet, surpassing contrastive, generative, and other multimodal\nself-supervised methods. Fine-tuning across datasets with diverse sizes and\nmolecule types further demonstrates that pretraining transfers effectively to\nnew chemical domains, highlighting the importance of 3D-informed molecular\nrepresentations.",
          "score": 0.35169970677236306,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 5
    },
    {
      "question": "Record reported performance metrics and numbers for key benchmarks (e.g., MAE/RMSE/AUC/R2), and note the baseline models compared against.",
      "answer": "- arXiv:2506.15792 (CheMeleon)\n  - Reported metrics:\n    - Win rate on 58 Polaris benchmark tasks: 79%\n    - Win rate on MoleculeACE assays: 97%\n  - Baselines compared against (percent win rates reported):\n    - Random Forest — 46% (Polaris), 63% (MoleculeACE)\n    - fastprop — 39% (Polaris)\n    - Chemprop — 36% (Polaris)\n    - \"other foundation models\" (CheMeleon reported to surpass them on MoleculeACE; no per-model numbers in excerpt)\n  - Notes: paper also reports that the model struggles with activity cliffs (no numeric metric for that in the excerpt) [arxiv:2506.15792v1].\n\n- arXiv:2509.10871\n  - Excerpt reports that the proposed MPNN architectures achieve state-of-the-art performance and outperform more complex/pretrained models, and describes architectural findings (bidirectional message passing + attention). No numeric metrics (MAE/RMSE/AUC/R2 or exact percentages) or explicit baseline performance numbers are provided in the excerpt [arxiv:2509.10871v1].\n\n- arXiv:2410.07981v2\n  - Excerpt claims state-of-the-art results across multiple datasets for a simple multimodal transformer baseline, but no numeric metrics or baseline numbers are included in the provided excerpt [arxiv:2410.07981v2].\n\n- arXiv:2509.11782\n  - Excerpt states the multimodal framework outperforms traditional and state-of-the-art deep learning approaches on enzyme turnover-rate prediction, but provides no numeric performance metrics or baseline numbers in the excerpt [arxiv:2509.11782v1].",
      "citations": [
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.36532749188925606,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.32971134997654095,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.31340180786955896,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.11782v1:summary",
          "paper_id": "arxiv:2509.11782v1",
          "text": "The enzyme turnover rate is a fundamental parameter in enzyme kinetics,\nreflecting the catalytic efficiency of enzymes. However, enzyme turnover rates\nremain scarce across most organisms due to the high cost and complexity of\nexperimental measurements. To address this gap, we propose a multimodal\nframework for predicting the enzyme turnover rate by integrating enzyme\nsequences, substrate structures, and environmental factors. Our model combines\na pre-trained language model and a convolutional neural network to extract\nfeatures from protein sequences, while a graph neural network captures\ninformative representations from substrate molecules. An attention mechanism is\nincorporated to enhance interactions between enzyme and substrate\nrepresentations. Furthermore, we leverage symbolic regression via\nKolmogorov-Arnold Networks to explicitly learn mathematical formulas that\ngovern the enzyme turnover rate, enabling interpretable and accurate\npredictions. Extensive experiments demonstrate that our framework outperforms\nboth traditional and state-of-the-art deep learning approaches. This work\nprovides a robust tool for studying enzyme kinetics and holds promise for\napplications in enzyme engineering, biotechnology, and industrial biocatalysis.",
          "score": 0.29448066498659264,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 5
    },
    {
      "question": "Summarize pretraining strategies (contrastive, masked prediction, descriptor prediction, multimodal), dataset(s) used for pretraining, and any transfer/finetuning results.",
      "answer": "Contrastive / contrast‑free\n- Strategy: C-FREE avoids contrastive negatives entirely and instead trains to predict subgraph (ego‑net) embeddings from their complementary neighborhood embeddings in latent space (no negatives, no positional encodings) using a hybrid GNN–Transformer backbone that fuses 2D topology and ensembles of 3D conformers (fixed‑radius ego‑nets across conformers) [arxiv:2509.22468v1].\n- Pretraining data: GEOM (rich 3D conformer ensembles).\n- Transfer / finetuning: Pretrained C‑FREE yields state‑of‑the‑art results on MoleculeNet, outperforming contrastive, generative, and other multimodal baselines [arxiv:2509.22468v1].\n\nMasked / masked‑prediction style\n- None of the provided excerpts describe a classical masked‑token/objective pretraining (e.g., masked SMILES/graph node masking) as the central method; the multimodal transformer paper and C‑FREE use prediction/aggregation of modality tokens or sub",
      "citations": [
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.4354734418343274,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.3921499971675592,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22468v1:summary",
          "paper_id": "arxiv:2509.22468v1",
          "text": "High-quality molecular representations are essential for property prediction\nand molecular design, yet large labeled datasets remain scarce. While\nself-supervised pretraining on molecular graphs has shown promise, many\nexisting approaches either depend on hand-crafted augmentations or complex\ngenerative objectives, and often rely solely on 2D topology, leaving valuable\n3D structural information underutilized. To address this gap, we introduce\nC-FREE (Contrast-Free Representation learning on Ego-nets), a simple framework\nthat integrates 2D graphs with ensembles of 3D conformers. C-FREE learns\nmolecular representations by predicting subgraph embeddings from their\ncomplementary neighborhoods in the latent space, using fixed-radius ego-nets as\nmodeling units across different conformers. This design allows us to integrate\nboth geometric and topological information within a hybrid Graph Neural Network\n(GNN)-Transformer backbone, without negatives, positional encodings, or\nexpensive pre-processing. Pretraining on the GEOM dataset, which provides rich\n3D conformational diversity, C-FREE achieves state-of-the-art results on\nMoleculeNet, surpassing contrastive, generative, and other multimodal\nself-supervised methods. Fine-tuning across datasets with diverse sizes and\nmolecule types further demonstrates that pretraining transfers effectively to\nnew chemical domains, highlighting the importance of 3D-informed molecular\nrepresentations.",
          "score": 0.3772658165421862,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.36536285681835096,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 5
    },
    {
      "question": "Collect ablation studies and diagnostics reported (which components matter), runtime/compute info (GPUs, training time, model parameter counts) when available, and whether code/data are released (links).",
      "answer": "Summary of ablation/diagnostic findings, runtime/compute, model sizes, and code/data release information from the provided excerpts\n\narXiv:2509.22028 (MCGM — Multi-stage Clustered Global Modeling)\n- Ablations / diagnostics (which components matter)\n  - MCGM is a plug‑in module that: (a) builds a multi‑resolution hierarchy of atomic clusters, (b) distills global information via dynamic hierarchical clustering, and (c) propagates context back through learned transformations with residual connections. Integration into four different backbone architectures yields consistent improvements (reported average 26.2% OE62 energy error reduction; SOTA on AQM) — implies hierarchical clustering + context propagation are the effective components. (arXiv:2509.22028v1)\n- Runtime / compute / model size\n  - Not reported in the provided excerpt (no GPU counts, training time, or parameter counts given). (arXiv:2509.22028v1)\n- Code / data release\n  - Not stated in the excerpt whether code or data are released. (arXiv:2509.22028v1)\n\narXiv:2506.15792 (CheMeleon — molecular foundation model pre‑trained on Mordred descriptors)\n- Ablations / diagnostics\n  - Diagnostic: t‑SNE of learned representations shows effective separation of chemical classes; model struggles to distinguish activity cliffs (a noted failure mode shared with other models). Comparisons on benchmark suites (Polaris, MoleculeACE) indicate strong empirical gains versus several baselines (win rates reported). No detailed component ablations described in the excerpt. (arXiv:2506.15792v1)\n- Runtime / compute / model size\n  - Not reported in the provided excerpt (no GPU/training time/parameter counts). (arXiv:2506.15792v1)\n- Code / data release\n  - The excerpt does not indicate whether code or",
      "citations": [
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.35971141921686506,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.3107065430992032,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.17018v1:summary",
          "paper_id": "arxiv:2509.17018v1",
          "text": "We propose a machine learning (ML) architecture to better capture the\ndependency of thermodynamic properties on the independent states. When\npredicting state-dependent thermodynamic properties, ML models need to account\nfor both molecular structure and the thermodynamic state, described by\nindependent variables, typically temperature, pressure, and composition. Modern\nmolecular ML models typically include state information by adding it to\nmolecular fingerprint vectors or by embedding explicit (semi-empirical)\nthermodynamic relations. Here, we propose to rather split the information\nprocessing on the molecular structure and the dependency on states into two\nseparate network channels: a graph neural network and a multilayer perceptron,\nwhose output is combined by a dot product. We refer to our approach as\nDeepEOSNet, as this idea is based on the DeepONet architecture [Lu et al.\n(2021), Nat. Mach. Intell.]: instead of operators, we learn state dependencies,\nwith the possibility to predict equation of states (EOS). We investigate the\npredictive performance of DeepEOSNet by means of three case studies, which\ninclude the prediction of vapor pressure as a function of temperature, and\nmixture molar volume as a function of composition, temperature, and pressure.\nOur results show superior performance of DeepEOSNet for predicting vapor\npressure and comparable performance for predicting mixture molar volume\ncompared to state-of-research graph-based thermodynamic prediction models from\nour earlier works. In fact, we see large potential of DeepEOSNet in cases where\ndata is sparse in the state domain and the output function is structurally\nsimilar across different molecules. The concept of DeepEOSNet can easily be\ntransferred to other ML architectures in molecular context, and thus provides a\nviable option for property prediction.",
          "score": 0.29517435332632197,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.281212805346834,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 5
    },
    {
      "question": "Note reported limitations, failure modes, or suggested future work from each paper.",
      "answer": "arXiv:2509.10871v1\n- Reported limitation / failure mode: structural diversity of datasets influences whether additional MPNN components or richer feature sets are needed; model choices that help on some datasets (e.g., bidirectional attention with a minimalist message) are not universally necessary or beneficial across all datasets. The authors also note that convolution normalization factors do not help uniformly across datasets and outputs (global and node-level) — i.e., some common architectural fixes may not generalize. (arXiv:2509.10871v1)\n\narXiv:2509.22028v1\n- Reported limitations of prior methods (motivation for work): (1) increasing cutoff radii for geometry-aware GNNs makes computation scale poorly (cubic in distance); (2) physics-inspired kernels are often system-specific and lack generality; (3) Fourier-space approaches need careful tuning of multiple parameters and add computational overhead. The paper proposes MCGM to address these failure modes. (arXiv:2509.22028v1)\n\narXiv:2410.07981v2\n- Suggested future/architecture flexibility (implicit): the framework is modular—modality-specific encoders (SMILES transformer, MPNN, equivariant 3D encoder) can be easily adapted or replaced, suggesting future work can explore alternative encoders or encoder combinations and scale to larger multimodal datasets (",
      "citations": [
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.3047902459256574,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.25847490819601343,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.2540177855547965,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.17018v1:summary",
          "paper_id": "arxiv:2509.17018v1",
          "text": "We propose a machine learning (ML) architecture to better capture the\ndependency of thermodynamic properties on the independent states. When\npredicting state-dependent thermodynamic properties, ML models need to account\nfor both molecular structure and the thermodynamic state, described by\nindependent variables, typically temperature, pressure, and composition. Modern\nmolecular ML models typically include state information by adding it to\nmolecular fingerprint vectors or by embedding explicit (semi-empirical)\nthermodynamic relations. Here, we propose to rather split the information\nprocessing on the molecular structure and the dependency on states into two\nseparate network channels: a graph neural network and a multilayer perceptron,\nwhose output is combined by a dot product. We refer to our approach as\nDeepEOSNet, as this idea is based on the DeepONet architecture [Lu et al.\n(2021), Nat. Mach. Intell.]: instead of operators, we learn state dependencies,\nwith the possibility to predict equation of states (EOS). We investigate the\npredictive performance of DeepEOSNet by means of three case studies, which\ninclude the prediction of vapor pressure as a function of temperature, and\nmixture molar volume as a function of composition, temperature, and pressure.\nOur results show superior performance of DeepEOSNet for predicting vapor\npressure and comparable performance for predicting mixture molar volume\ncompared to state-of-research graph-based thermodynamic prediction models from\nour earlier works. In fact, we see large potential of DeepEOSNet in cases where\ndata is sparse in the state domain and the output function is structurally\nsimilar across different molecules. The concept of DeepEOSNet can easily be\ntransferred to other ML architectures in molecular context, and thus provides a\nviable option for property prediction.",
          "score": 0.2527363101796984,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 5
    },
    {
      "question": "Synthesize cross-paper comparisons: common successful design patterns (e.g., attention + spatial features, equivariant layers), dataset gaps, evaluation inconsistencies, and practical recommendations for building GNNs for molecules.",
      "answer": "Below is a concise synthesis across the provided papers, focused on (1) recurring successful design patterns, (2) dataset gaps, (3) evaluation inconsistencies, and (4) practical recommendations for building GNNs for molecular tasks. Citations to the specific papers are given in-line.\n\n1) Common successful design patterns\n- Combine attention with message-passing / local features\n  - Bidirectional message-passing with attention improves class separability and outperforms more complex pretrained models on many datasets [arxiv:2509.10871v1]. Attention helps prioritize important edges/features during local aggregation.\n- Incorporate spatial / 3D information when available\n  - Using geometric information or 3D conformers boosts performance; hybrid architectures that fuse 2D topology and 3D conformers (e.g., ego-net ensembles) produce strong pretrained representations [arxiv:2509.22468v1]. Spatial features were found beneficial in many datasets, though impact varies by dataset diversity [arxiv:2509.10871v1].\n- Add global-context modules to counter locality bias\n  - Hierarchical global-context mechanisms (e.g., Multi-stage Clustered Global Modeling, MCGM) provide long-range interactions efficiently and substantially reduce energy-prediction error across backbones, addressing the message-passing locality limitation [arxiv:2509.22028v1].\n- Keep models simple where possible\n  - Simpler MPNN formulations (minimalist messages, exclusion of self-perception) combined with attention often match or exceed more complex alternatives—suggesting careful parsimony and feature design can beat complexity [arxiv:2509.10871v1].\n- Hybrid backbones and contrast-free pretraining\n  - Combining GNN encoders with Transformer-style components and contrast-free objectives (predicting complementary subgraph embeddings across conformers) yields strong generalizable representations without heavy augmentation or negative sampling [arxiv:2509.22468v1].\n\n2) Dataset gaps and limitations\n- Limited labeled data and conformational coverage\n  - Large labeled datasets remain scarce; methods that exploit GEOM-style conformer ensembles for pretraining help but many downstream datasets lack rich 3D conformational diversity [arxiv:2509.22468v1].\n- Structural diversity varies across benchmarks\n  - Performance benefits from added components (3D features, attention) depend on dataset structural diversity—some benchmarks are homogeneous and do not require complex spatial modeling [arxiv:2509.10871v1].\n- Domain-specific physics/data scarcity\n  - Physics-inspired kernels (Coulomb, dispersion) are often system-specific and not generally usable",
      "citations": [
        {
          "chunk_id": "arxiv:2509.07887v1:summary",
          "paper_id": "arxiv:2509.07887v1",
          "text": "Graph Neural Networks (GNNs) have gained traction in the complex domain of\ndrug discovery because of their ability to process graph-structured data such\nas drug molecule models. This approach has resulted in a myriad of methods and\nmodels in published literature across several categories of drug discovery\nresearch. This paper covers the research categories comprehensively with recent\npapers, namely molecular property prediction, including drug-target binding\naffinity prediction, drug-drug interaction study, microbiome interaction\nprediction, drug repositioning, retrosynthesis, and new drug design, and\nprovides guidance for future work on GNNs for drug discovery.",
          "score": 0.6152381085199791,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.564416329754983,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.5542069317154442,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22468v1:summary",
          "paper_id": "arxiv:2509.22468v1",
          "text": "High-quality molecular representations are essential for property prediction\nand molecular design, yet large labeled datasets remain scarce. While\nself-supervised pretraining on molecular graphs has shown promise, many\nexisting approaches either depend on hand-crafted augmentations or complex\ngenerative objectives, and often rely solely on 2D topology, leaving valuable\n3D structural information underutilized. To address this gap, we introduce\nC-FREE (Contrast-Free Representation learning on Ego-nets), a simple framework\nthat integrates 2D graphs with ensembles of 3D conformers. C-FREE learns\nmolecular representations by predicting subgraph embeddings from their\ncomplementary neighborhoods in the latent space, using fixed-radius ego-nets as\nmodeling units across different conformers. This design allows us to integrate\nboth geometric and topological information within a hybrid Graph Neural Network\n(GNN)-Transformer backbone, without negatives, positional encodings, or\nexpensive pre-processing. Pretraining on the GEOM dataset, which provides rich\n3D conformational diversity, C-FREE achieves state-of-the-art results on\nMoleculeNet, surpassing contrastive, generative, and other multimodal\nself-supervised methods. Fine-tuning across datasets with diverse sizes and\nmolecule types further demonstrates that pretraining transfers effectively to\nnew chemical domains, highlighting the importance of 3D-informed molecular\nrepresentations.",
          "score": 0.4897564014705405,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 5
    },
    {
      "question": "Flag any missing details or clarifications needed (e.g., ambiguous hyperparams, training schedules) so we can follow up by retrieving specific sections or contacting authors.",
      "answer": "Below are concise lists of missing details or clarifications to request for each paper so you can reproduce or compare results. I cite each item by the arXiv id from your excerpts.\n\narXiv:2509.22028v1 (MCGM)\n- Clustering specifics: algorithm used for dynamic hierarchical clustering, number of clusters per stage, criteria for merging/splitting clusters, and whether clustering is deterministic or learned.\n- Multi-resolution hierarchy: how many stages/levels used, cluster sizes at each level, and how levels are initialized.\n- Transformations / aggregation: exact neural architectures (layer types, widths, activation, normalization) used to distill and propagate cluster/global context back to atoms.\n- Integration details: how MCGM plugs into each backbone (where inserted, residual connection formulation, any backbone-specific tuning).\n- Hyperparameters: learning rate, optimizer, batch size, weight decay, dropout, training epochs, and any scheduler (warmup, LR decay).\n- Computational cost: complexity analysis and practical runtime / memory measurements vs baseline (including hardware used).\n- Datasets / splits: exact datasets and splits used for OE62 and AQM, preprocessing steps, and evaluation metrics (error definitions, units).\n- Ablations / baselines: ablation studies quantifying contribution of each module, and exact backbone configurations for fair comparison.\n- Implementation details: initialization, random seeds, reproducibility instructions, and code availability / license.\n\narXiv:2509.10871v1 (MPNN simplifications)\n- Model architecture: exact message formulation (mathematical definition), dimensions of embeddings, number of message-passing steps, and how bidirectional passing is implemented.\n- Attention: attention mechanism details (type, query/key/value design, multi-head count, softmax temperature, any normalization).\n- \"Excludes self-perception\": precise meaning and implementation (removing self-loops? how node features are treated).\n- Convolution normalization: which normalization factors were tested, how they were applied, and hyperparameters governing them.\n- Spatial / 3D features: exact spatial features used (distances, angles), how 3D graphs were constructed, and when 3D was used vs 2D.\n- Training details: optimizer, learning rate schedule, batch size, epochs, early stopping criteria, loss functions, and data augmentation (if any).\n- Dataset diversity analysis: metrics used to quantify diversity, thresholds, and how diversity influenced model choices.\n- Baseline comparisons: hyperparameters and training regime for compared models (to ensure fair comparison).\n- Code / reproducibility: availability of code, random seeds, and hardware used.\n\narXiv",
      "citations": [
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.2765294418246873,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.25162071795884183,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.23705663818726158,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.23346702292057753,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 5
    },
    {
      "question": "For each of the 8 papers (arXiv:2509.07887v1, arXiv:2509.10871v1, arXiv:2506.15792v1, arXiv:2410.07981v2, arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1) produce a reproducibility checklist entry containing: optimizer, learning rate(s) and schedule, batch size, epochs, weight decay, scheduler/warmup details, gradient clipping, data augmentations, loss functions and weights, random seeds (if given), initialization, and any training tricks (mixed precision, gradient accumulation). If an item is not reported, mark it explicitly as \"missing\".",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.2879099175279502,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.28695760782198376,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.2811516068114487,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.11782v1:summary",
          "paper_id": "arxiv:2509.11782v1",
          "text": "The enzyme turnover rate is a fundamental parameter in enzyme kinetics,\nreflecting the catalytic efficiency of enzymes. However, enzyme turnover rates\nremain scarce across most organisms due to the high cost and complexity of\nexperimental measurements. To address this gap, we propose a multimodal\nframework for predicting the enzyme turnover rate by integrating enzyme\nsequences, substrate structures, and environmental factors. Our model combines\na pre-trained language model and a convolutional neural network to extract\nfeatures from protein sequences, while a graph neural network captures\ninformative representations from substrate molecules. An attention mechanism is\nincorporated to enhance interactions between enzyme and substrate\nrepresentations. Furthermore, we leverage symbolic regression via\nKolmogorov-Arnold Networks to explicitly learn mathematical formulas that\ngovern the enzyme turnover rate, enabling interpretable and accurate\npredictions. Extensive experiments demonstrate that our framework outperforms\nboth traditional and state-of-the-art deep learning approaches. This work\nprovides a robust tool for studying enzyme kinetics and holds promise for\napplications in enzyme engineering, biotechnology, and industrial biocatalysis.",
          "score": 0.2718586293116819,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 7
    },
    {
      "question": "Extract model sizes and parameter counts (total params, number of layers/heads where relevant), and report any architecture hyperparameters (embedding dims, MPNN steps, attention heads, equivariant block counts). If not reported, mark as \"missing\".",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.39437161560408646,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.3876141741535421,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.3848187186984574,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.17018v1:summary",
          "paper_id": "arxiv:2509.17018v1",
          "text": "We propose a machine learning (ML) architecture to better capture the\ndependency of thermodynamic properties on the independent states. When\npredicting state-dependent thermodynamic properties, ML models need to account\nfor both molecular structure and the thermodynamic state, described by\nindependent variables, typically temperature, pressure, and composition. Modern\nmolecular ML models typically include state information by adding it to\nmolecular fingerprint vectors or by embedding explicit (semi-empirical)\nthermodynamic relations. Here, we propose to rather split the information\nprocessing on the molecular structure and the dependency on states into two\nseparate network channels: a graph neural network and a multilayer perceptron,\nwhose output is combined by a dot product. We refer to our approach as\nDeepEOSNet, as this idea is based on the DeepONet architecture [Lu et al.\n(2021), Nat. Mach. Intell.]: instead of operators, we learn state dependencies,\nwith the possibility to predict equation of states (EOS). We investigate the\npredictive performance of DeepEOSNet by means of three case studies, which\ninclude the prediction of vapor pressure as a function of temperature, and\nmixture molar volume as a function of composition, temperature, and pressure.\nOur results show superior performance of DeepEOSNet for predicting vapor\npressure and comparable performance for predicting mixture molar volume\ncompared to state-of-research graph-based thermodynamic prediction models from\nour earlier works. In fact, we see large potential of DeepEOSNet in cases where\ndata is sparse in the state domain and the output function is structurally\nsimilar across different molecules. The concept of DeepEOSNet can easily be\ntransferred to other ML architectures in molecular context, and thus provides a\nviable option for property prediction.",
          "score": 0.3520892351265578,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 7
    },
    {
      "question": "Collect compute/hardware and runtime info: GPU types/count, effective batch size per GPU, wall-clock training time per run/epoch, and total FLOPS or estimated costs if reported. Mark missing items explicitly.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.3500720743700138,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22468v1:summary",
          "paper_id": "arxiv:2509.22468v1",
          "text": "High-quality molecular representations are essential for property prediction\nand molecular design, yet large labeled datasets remain scarce. While\nself-supervised pretraining on molecular graphs has shown promise, many\nexisting approaches either depend on hand-crafted augmentations or complex\ngenerative objectives, and often rely solely on 2D topology, leaving valuable\n3D structural information underutilized. To address this gap, we introduce\nC-FREE (Contrast-Free Representation learning on Ego-nets), a simple framework\nthat integrates 2D graphs with ensembles of 3D conformers. C-FREE learns\nmolecular representations by predicting subgraph embeddings from their\ncomplementary neighborhoods in the latent space, using fixed-radius ego-nets as\nmodeling units across different conformers. This design allows us to integrate\nboth geometric and topological information within a hybrid Graph Neural Network\n(GNN)-Transformer backbone, without negatives, positional encodings, or\nexpensive pre-processing. Pretraining on the GEOM dataset, which provides rich\n3D conformational diversity, C-FREE achieves state-of-the-art results on\nMoleculeNet, surpassing contrastive, generative, and other multimodal\nself-supervised methods. Fine-tuning across datasets with diverse sizes and\nmolecule types further demonstrates that pretraining transfers effectively to\nnew chemical domains, highlighting the importance of 3D-informed molecular\nrepresentations.",
          "score": 0.2566872518615853,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.11782v1:summary",
          "paper_id": "arxiv:2509.11782v1",
          "text": "The enzyme turnover rate is a fundamental parameter in enzyme kinetics,\nreflecting the catalytic efficiency of enzymes. However, enzyme turnover rates\nremain scarce across most organisms due to the high cost and complexity of\nexperimental measurements. To address this gap, we propose a multimodal\nframework for predicting the enzyme turnover rate by integrating enzyme\nsequences, substrate structures, and environmental factors. Our model combines\na pre-trained language model and a convolutional neural network to extract\nfeatures from protein sequences, while a graph neural network captures\ninformative representations from substrate molecules. An attention mechanism is\nincorporated to enhance interactions between enzyme and substrate\nrepresentations. Furthermore, we leverage symbolic regression via\nKolmogorov-Arnold Networks to explicitly learn mathematical formulas that\ngovern the enzyme turnover rate, enabling interpretable and accurate\npredictions. Extensive experiments demonstrate that our framework outperforms\nboth traditional and state-of-the-art deep learning approaches. This work\nprovides a robust tool for studying enzyme kinetics and holds promise for\napplications in enzyme engineering, biotechnology, and industrial biocatalysis.",
          "score": 0.2541094554854461,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.24842397248180242,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 7
    },
    {
      "question": "List exact datasets and splits used for each experiment (train/val/test sizes, random split vs scaffold/time-split, any preprocessing or featurization steps like conformer generation parameters), and provide citations/links to dataset versions. Flag unknown or ambiguous splits.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.2925830854765676,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22468v1:summary",
          "paper_id": "arxiv:2509.22468v1",
          "text": "High-quality molecular representations are essential for property prediction\nand molecular design, yet large labeled datasets remain scarce. While\nself-supervised pretraining on molecular graphs has shown promise, many\nexisting approaches either depend on hand-crafted augmentations or complex\ngenerative objectives, and often rely solely on 2D topology, leaving valuable\n3D structural information underutilized. To address this gap, we introduce\nC-FREE (Contrast-Free Representation learning on Ego-nets), a simple framework\nthat integrates 2D graphs with ensembles of 3D conformers. C-FREE learns\nmolecular representations by predicting subgraph embeddings from their\ncomplementary neighborhoods in the latent space, using fixed-radius ego-nets as\nmodeling units across different conformers. This design allows us to integrate\nboth geometric and topological information within a hybrid Graph Neural Network\n(GNN)-Transformer backbone, without negatives, positional encodings, or\nexpensive pre-processing. Pretraining on the GEOM dataset, which provides rich\n3D conformational diversity, C-FREE achieves state-of-the-art results on\nMoleculeNet, surpassing contrastive, generative, and other multimodal\nself-supervised methods. Fine-tuning across datasets with diverse sizes and\nmolecule types further demonstrates that pretraining transfers effectively to\nnew chemical domains, highlighting the importance of 3D-informed molecular\nrepresentations.",
          "score": 0.26407149085982623,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.2620667159018048,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.24712383562231902,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 7
    },
    {
      "question": "Extract ablation table values and the experimental settings for each ablation (which hyperparameters held fixed, which changed). If only qualitative statements provided, note that and capture exact quantitative entries where available.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.25923741316759225,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.11782v1:summary",
          "paper_id": "arxiv:2509.11782v1",
          "text": "The enzyme turnover rate is a fundamental parameter in enzyme kinetics,\nreflecting the catalytic efficiency of enzymes. However, enzyme turnover rates\nremain scarce across most organisms due to the high cost and complexity of\nexperimental measurements. To address this gap, we propose a multimodal\nframework for predicting the enzyme turnover rate by integrating enzyme\nsequences, substrate structures, and environmental factors. Our model combines\na pre-trained language model and a convolutional neural network to extract\nfeatures from protein sequences, while a graph neural network captures\ninformative representations from substrate molecules. An attention mechanism is\nincorporated to enhance interactions between enzyme and substrate\nrepresentations. Furthermore, we leverage symbolic regression via\nKolmogorov-Arnold Networks to explicitly learn mathematical formulas that\ngovern the enzyme turnover rate, enabling interpretable and accurate\npredictions. Extensive experiments demonstrate that our framework outperforms\nboth traditional and state-of-the-art deep learning approaches. This work\nprovides a robust tool for studying enzyme kinetics and holds promise for\napplications in enzyme engineering, biotechnology, and industrial biocatalysis.",
          "score": 0.25745579654066536,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.24770169609937756,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.23170752186902585,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 7
    },
    {
      "question": "Enumerate code and checkpoint availability: provide GitHub/Zenodo/other links, branch/commit if available, reported license, and whether pretrained checkpoints are released. If link absent, record 'no link reported' and suggest likely search terms (repo name or project/title).",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.22453333134818002,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.07887v1:summary",
          "paper_id": "arxiv:2509.07887v1",
          "text": "Graph Neural Networks (GNNs) have gained traction in the complex domain of\ndrug discovery because of their ability to process graph-structured data such\nas drug molecule models. This approach has resulted in a myriad of methods and\nmodels in published literature across several categories of drug discovery\nresearch. This paper covers the research categories comprehensively with recent\npapers, namely molecular property prediction, including drug-target binding\naffinity prediction, drug-drug interaction study, microbiome interaction\nprediction, drug repositioning, retrosynthesis, and new drug design, and\nprovides guidance for future work on GNNs for drug discovery.",
          "score": 0.20194972039583242,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.19554154941858018,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.1864750040125874,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 7
    },
    {
      "question": "Summarize reported randomness/reproducibility notes: seeds, number of runs (mean/std), deterministic settings, and any stated variance across seeds. Mark missing or single-run results.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.2406524388716914,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.23395599345476806,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.2217356252961563,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.18466193173536855,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 7
    },
    {
      "question": "Produce a short cross-paper table (textual) highlighting which reproducibility items are consistently missing across papers (e.g., GPU hours, exact LR schedule), and prioritize follow-ups: (1) immediate — code links & hyperparams, (2) medium — dataset splits/ preprocessing, (3) optional — hardware/time estimates and exact seeds.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.28610187183382396,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.27460468196751203,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.11782v1:summary",
          "paper_id": "arxiv:2509.11782v1",
          "text": "The enzyme turnover rate is a fundamental parameter in enzyme kinetics,\nreflecting the catalytic efficiency of enzymes. However, enzyme turnover rates\nremain scarce across most organisms due to the high cost and complexity of\nexperimental measurements. To address this gap, we propose a multimodal\nframework for predicting the enzyme turnover rate by integrating enzyme\nsequences, substrate structures, and environmental factors. Our model combines\na pre-trained language model and a convolutional neural network to extract\nfeatures from protein sequences, while a graph neural network captures\ninformative representations from substrate molecules. An attention mechanism is\nincorporated to enhance interactions between enzyme and substrate\nrepresentations. Furthermore, we leverage symbolic regression via\nKolmogorov-Arnold Networks to explicitly learn mathematical formulas that\ngovern the enzyme turnover rate, enabling interpretable and accurate\npredictions. Extensive experiments demonstrate that our framework outperforms\nboth traditional and state-of-the-art deep learning approaches. This work\nprovides a robust tool for studying enzyme kinetics and holds promise for\napplications in enzyme engineering, biotechnology, and industrial biocatalysis.",
          "score": 0.24552474224139362,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.24261216746911798,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 7
    },
    {
      "question": "For the top 3 papers that showed strongest empirical results (per our earlier summaries), propose precise next actions to reproduce their reported best results: which repository/branch to check, which hyperparameters to assume if missing, an estimated compute budget (GPU type, hours), and a minimal reproduction protocol (dataset split, seed choices, metric reporting).",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.3557548561841214,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.3359213434076525,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.3184806076512926,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.29744756428214847,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 7
    },
    {
      "question": "Generate a short template email / issue text to request missing details from authors (targeting hyperparams, seeds, and code availability) with a checklist so we can paste it when contacting authors or opening issues.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.16970734622226677,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.16775164056351072,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.16715301120749396,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.15028471801116036,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 7
    },
    {
      "question": "Per-paper reproducibility checklist (for each of the 8 papers): optimizer, initial LR and LR schedule, batch size, epochs, weight decay, scheduler/warmup, gradient clipping, data augmentations, loss(es) and loss weights, random seeds (or 'missing'), initialization scheme, mixed precision/accumulation, and any training tricks. Mark any item not reported as 'missing'.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.2631008337375748,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.25625227861337396,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.24764036244616108,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.2443837259881703,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 8
    },
    {
      "question": "Per-paper model specification: total parameter count (or 'missing'), embedding dims, number of message-passing steps / equivariant blocks, attention heads, pooling method, and any architecture hyperparameters. Mark missing items.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.41588436760076897,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.3687599626728814,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.33850755571630686,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.289757598999363,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 8
    },
    {
      "question": "Per-paper compute & runtime: GPU type(s) and count, effective batch size per GPU, wall-clock training time (per epoch / total) or 'missing', and any reported FLOPs or cost estimates.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.31250296856277343,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.11782v1:summary",
          "paper_id": "arxiv:2509.11782v1",
          "text": "The enzyme turnover rate is a fundamental parameter in enzyme kinetics,\nreflecting the catalytic efficiency of enzymes. However, enzyme turnover rates\nremain scarce across most organisms due to the high cost and complexity of\nexperimental measurements. To address this gap, we propose a multimodal\nframework for predicting the enzyme turnover rate by integrating enzyme\nsequences, substrate structures, and environmental factors. Our model combines\na pre-trained language model and a convolutional neural network to extract\nfeatures from protein sequences, while a graph neural network captures\ninformative representations from substrate molecules. An attention mechanism is\nincorporated to enhance interactions between enzyme and substrate\nrepresentations. Furthermore, we leverage symbolic regression via\nKolmogorov-Arnold Networks to explicitly learn mathematical formulas that\ngovern the enzyme turnover rate, enabling interpretable and accurate\npredictions. Extensive experiments demonstrate that our framework outperforms\nboth traditional and state-of-the-art deep learning approaches. This work\nprovides a robust tool for studying enzyme kinetics and holds promise for\napplications in enzyme engineering, biotechnology, and industrial biocatalysis.",
          "score": 0.270864474771347,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.2423059067349818,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.2395080534845164,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 8
    },
    {
      "question": "Per-paper datasets & splits: exact dataset versions used, train/val/test sizes, split strategy (random/scaffold/time), preprocessing steps (conformer generation parameters, featurizers), and citations/links. Flag ambiguous or unreported splits.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.27400376939556237,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.25262851561951494,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.25162154815010074,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.2431418356159053,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 8
    },
    {
      "question": "Extract ablation table entries and precise experimental settings (what was held fixed, what varied). If only qualitative statements present, record that and list exact quantitative values where available.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2509.11782v1:summary",
          "paper_id": "arxiv:2509.11782v1",
          "text": "The enzyme turnover rate is a fundamental parameter in enzyme kinetics,\nreflecting the catalytic efficiency of enzymes. However, enzyme turnover rates\nremain scarce across most organisms due to the high cost and complexity of\nexperimental measurements. To address this gap, we propose a multimodal\nframework for predicting the enzyme turnover rate by integrating enzyme\nsequences, substrate structures, and environmental factors. Our model combines\na pre-trained language model and a convolutional neural network to extract\nfeatures from protein sequences, while a graph neural network captures\ninformative representations from substrate molecules. An attention mechanism is\nincorporated to enhance interactions between enzyme and substrate\nrepresentations. Furthermore, we leverage symbolic regression via\nKolmogorov-Arnold Networks to explicitly learn mathematical formulas that\ngovern the enzyme turnover rate, enabling interpretable and accurate\npredictions. Extensive experiments demonstrate that our framework outperforms\nboth traditional and state-of-the-art deep learning approaches. This work\nprovides a robust tool for studying enzyme kinetics and holds promise for\napplications in enzyme engineering, biotechnology, and industrial biocatalysis.",
          "score": 0.21743576308902035,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.17018v1:summary",
          "paper_id": "arxiv:2509.17018v1",
          "text": "We propose a machine learning (ML) architecture to better capture the\ndependency of thermodynamic properties on the independent states. When\npredicting state-dependent thermodynamic properties, ML models need to account\nfor both molecular structure and the thermodynamic state, described by\nindependent variables, typically temperature, pressure, and composition. Modern\nmolecular ML models typically include state information by adding it to\nmolecular fingerprint vectors or by embedding explicit (semi-empirical)\nthermodynamic relations. Here, we propose to rather split the information\nprocessing on the molecular structure and the dependency on states into two\nseparate network channels: a graph neural network and a multilayer perceptron,\nwhose output is combined by a dot product. We refer to our approach as\nDeepEOSNet, as this idea is based on the DeepONet architecture [Lu et al.\n(2021), Nat. Mach. Intell.]: instead of operators, we learn state dependencies,\nwith the possibility to predict equation of states (EOS). We investigate the\npredictive performance of DeepEOSNet by means of three case studies, which\ninclude the prediction of vapor pressure as a function of temperature, and\nmixture molar volume as a function of composition, temperature, and pressure.\nOur results show superior performance of DeepEOSNet for predicting vapor\npressure and comparable performance for predicting mixture molar volume\ncompared to state-of-research graph-based thermodynamic prediction models from\nour earlier works. In fact, we see large potential of DeepEOSNet in cases where\ndata is sparse in the state domain and the output function is structurally\nsimilar across different molecules. The concept of DeepEOSNet can easily be\ntransferred to other ML architectures in molecular context, and thus provides a\nviable option for property prediction.",
          "score": 0.21406353607497675,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.2041384002244316,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.19809558132239466,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 8
    },
    {
      "question": "Code and checkpoint availability: provide GitHub/Zenodo/other links, branch/commit if reported, license, and whether pretrained checkpoints are released. If no link is reported, list likely search terms to find repos.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.21884950807883144,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.19424000153183357,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.07887v1:summary",
          "paper_id": "arxiv:2509.07887v1",
          "text": "Graph Neural Networks (GNNs) have gained traction in the complex domain of\ndrug discovery because of their ability to process graph-structured data such\nas drug molecule models. This approach has resulted in a myriad of methods and\nmodels in published literature across several categories of drug discovery\nresearch. This paper covers the research categories comprehensively with recent\npapers, namely molecular property prediction, including drug-target binding\naffinity prediction, drug-drug interaction study, microbiome interaction\nprediction, drug repositioning, retrosynthesis, and new drug design, and\nprovides guidance for future work on GNNs for drug discovery.",
          "score": 0.18700435770034757,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.17305504927004506,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 8
    },
    {
      "question": "Randomness & reproducibility reporting: number of runs per experiment (mean/std) or single-run; any deterministic flags; reported variance across seeds or missing.",
      "answer": "- arXiv:2506.15792 (CheMeleon): The excerpt reports benchmark win rates but gives no information on number of runs, means/std, use of deterministic flags, or variance across random seeds — these details are missing in the provided text [arxiv:2506.15792v1].\n- arXiv:2509.11782 (enzyme turnover model): The excerpt says “extensive experiments” but does not report run counts, mean/std, deterministic settings, or seed variance — missing in the provided text [arxiv:2509.11782v1].\n- arXiv:2509.22028 (MCGM): Performance numbers are given, but the excerpt contains no mention of repeated runs, reporting of mean±std, deterministic flags, or variance over seeds — missing in the provided text [arxiv:2509.22028v1].\n- arXiv:2509.10871 (MPNN strategies): Describes models and analyses but the excerpt does not state number of runs, mean/std, deterministic options, or seed variance — missing in the provided text [arxiv:2509.10871v1].",
      "citations": [
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.2352471289469412,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.11782v1:summary",
          "paper_id": "arxiv:2509.11782v1",
          "text": "The enzyme turnover rate is a fundamental parameter in enzyme kinetics,\nreflecting the catalytic efficiency of enzymes. However, enzyme turnover rates\nremain scarce across most organisms due to the high cost and complexity of\nexperimental measurements. To address this gap, we propose a multimodal\nframework for predicting the enzyme turnover rate by integrating enzyme\nsequences, substrate structures, and environmental factors. Our model combines\na pre-trained language model and a convolutional neural network to extract\nfeatures from protein sequences, while a graph neural network captures\ninformative representations from substrate molecules. An attention mechanism is\nincorporated to enhance interactions between enzyme and substrate\nrepresentations. Furthermore, we leverage symbolic regression via\nKolmogorov-Arnold Networks to explicitly learn mathematical formulas that\ngovern the enzyme turnover rate, enabling interpretable and accurate\npredictions. Extensive experiments demonstrate that our framework outperforms\nboth traditional and state-of-the-art deep learning approaches. This work\nprovides a robust tool for studying enzyme kinetics and holds promise for\napplications in enzyme engineering, biotechnology, and industrial biocatalysis.",
          "score": 0.2333759091872551,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.21999994984679186,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.21871556475758494,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 8
    },
    {
      "question": "Cross-paper summary table (textual): list reproducibility items most commonly missing (ranked), and produce prioritized follow-ups: (1) immediate — code & exact hyperparameters, (2) medium — dataset splits & preprocessing, (3) optional — hardware/time estimates and exact seeds.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.2678475595718535,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.2580114395378981,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.25055642133804823,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.23653336666639174,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 8
    },
    {
      "question": "Identify the top-3 empirically strongest papers (based on our earlier summaries) and for each produce a minimal reproduction plan: repository/branch to check (or search hints), assumed hyperparameters if missing (conservative defaults), an estimated compute budget (GPU type + hours), and a minimal experiment protocol (split, seeds, metrics to report).",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.3415835044497852,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.3270410390150231,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.3199871464946659,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.30727108068326703,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 8
    },
    {
      "question": "Generate a short template email/issue to send to authors requesting missing reproducibility details (hyperparams, seeds, code/checkpoints, dataset splits), with a concise checklist so we can paste it into GitHub issues or emails.",
      "answer": "No answer returned.",
      "citations": [
        {
          "chunk_id": "arxiv:2509.22028v1:summary",
          "paper_id": "arxiv:2509.22028v1",
          "text": "Geometric graph neural networks (GNNs) excel at capturing molecular geometry,\nyet their locality-biased message passing hampers the modeling of long-range\ninteractions. Current solutions have fundamental limitations: extending cutoff\nradii causes computational costs to scale cubically with distance;\nphysics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific\nand lack generality; Fourier-space methods require careful tuning of multiple\nparameters (e.g., mesh size, k-space cutoff) with added computational overhead.\nWe introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,\nplug-and-play module that endows geometric GNNs with hierarchical global\ncontext through efficient clustering operations. MCGM builds a multi-resolution\nhierarchy of atomic clusters, distills global information via dynamic\nhierarchical clustering, and propagates this context back through learned\ntransformations, ultimately reinforcing atomic features via residual\nconnections. Seamlessly integrated into four diverse backbone architectures,\nMCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM\nachieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\\AA} for\nforces) while using 20% fewer parameters than Neural P3M. Code will be made\navailable upon acceptance.",
          "score": 0.21949301922974396,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2509.10871v1:summary",
          "paper_id": "arxiv:2509.10871v1",
          "text": "Strategies to improve the predicting performance of Message-Passing\nNeural-Networks for molecular property predictions can be achieved by\nsimplifying how the message is passed and by using descriptors that capture\nmultiple aspects of molecular graphs. In this work, we designed model\narchitectures that achieved state-of-the-art performance, surpassing more\ncomplex models such as those pre-trained on external databases. We assessed\ndataset diversity to complement our performance results, finding that\nstructural diversity influences the need for additional components in our MPNNs\nand feature sets.\n  In most datasets, our best architecture employs bidirectional message-passing\nwith an attention mechanism, applied to a minimalist message formulation that\nexcludes self-perception, highlighting that relatively simpler models, compared\nto classical MPNNs, yield higher class separability. In contrast, we found that\nconvolution normalization factors do not benefit the predictive power in all\nthe datasets tested. This was corroborated in both global and node-level\noutputs. Additionally, we analyzed the influence of both adding spatial\nfeatures and working with 3D graphs, finding that 2D molecular graphs are\nsufficient when complemented with appropriately chosen 3D descriptors. This\napproach not only preserves predictive performance but also reduces\ncomputational cost by over 50%, making it particularly advantageous for\nhigh-throughput screening campaigns.",
          "score": 0.20243052205594558,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2410.07981v2:summary",
          "paper_id": "arxiv:2410.07981v2",
          "text": "In this work, we propose a simple transformer-based baseline for multimodal\nmolecular representation learning, integrating three distinct modalities:\nSMILES strings, 2D graph representations, and 3D conformers of molecules. A key\naspect of our approach is the aggregation of 3D conformers, allowing the model\nto account for the fact that molecules can adopt multiple conformations-an\nimportant factor for accurate molecular representation. The tokens for each\nmodality are extracted using modality-specific encoders: a transformer for\nSMILES strings, a message-passing neural network for 2D graphs, and an\nequivariant neural network for 3D conformers. The flexibility and modularity of\nthis framework enable easy adaptation and replacement of these encoders, making\nthe model highly versatile for different molecular tasks. The extracted tokens\nare then combined into a unified multimodal sequence, which is processed by a\ndownstream transformer for prediction tasks. To efficiently scale our model for\nlarge multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision.\nDespite its simplicity, our approach achieves state-of-the-art results across\nmultiple datasets, demonstrating its effectiveness as a strong baseline for\nmultimodal molecular representation learning.",
          "score": 0.19434908214588179,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        },
        {
          "chunk_id": "arxiv:2506.15792v1:summary",
          "paper_id": "arxiv:2506.15792v1",
          "text": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.",
          "score": 0.17022888835855252,
          "metadata": {
            "section": "Summary",
            "page_from": 0,
            "page_to": 0
          }
        }
      ],
      "step": 8
    },
    {
      "question": "For each of the 8 papers (arXiv:2509.07887v1, arXiv:2509.10871v1, arXiv:2506.15792v1, arXiv:2410.07981v2, arXiv:2509.22468v1, arXiv:2509.22028v1, arXiv:2509.17018v1, arXiv:2509.11782v1) produce a reproducibility checklist entry containing: optimizer, initial learning rate(s) and exact LR schedule (including warmup and decay), batch size (per-GPU and effective), epochs (or total training steps), weight decay, gradient clipping, data augmentations/augment strategies, loss functions and weights, random seed(s) or explicit 'missing', initialization scheme, mixed precision/gradient accumulation, and any other training tricks. Mark items not reported as 'missing'.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.30494725146944746
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.30210849647320287
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.2939133365221847
        },
        {
          "paper_id": "arxiv:2509.11782v1",
          "chunk_id": "arxiv:2509.11782v1:summary",
          "section": "Summary",
          "score": 0.2817230339518475
        }
      ],
      "step": 9
    },
    {
      "question": "Per-paper model specification: total parameter count (or 'missing'), embedding dimensions, number of message-passing steps / equivariant block counts, attention heads, pooling/readout method, and other architecture hyperparameters (e.g., cutoff radii, basis functions). If items are absent, mark 'missing'.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.41558044365886854
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.36480497855090555
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.34809668941635813
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.29101106240200675
        }
      ],
      "step": 9
    },
    {
      "question": "Per-paper compute & runtime: reported GPU type(s) and count, effective batch size per device, wall-clock training time (per epoch or total) or 'missing', any reported FLOPs / cost estimates. If not reported, state 'missing' and provide conservative estimates where feasible.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.30966268817303666
        },
        {
          "paper_id": "arxiv:2509.11782v1",
          "chunk_id": "arxiv:2509.11782v1:summary",
          "section": "Summary",
          "score": 0.2537883411206706
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.24770925220816783
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.23935281726498117
        }
      ],
      "step": 9
    },
    {
      "question": "Per-paper datasets & splits: exact dataset names/versions, train/val/test sizes, split strategy (random/scaffold/time/fold), preprocessing steps (featurization, conformer generation parameters, augmentation), and citations/links. Flag ambiguous or unstated split strategies as 'missing/ambiguous'.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.32385719291202986
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.29438762127484114
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.2747733656729469
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.27368700995541667
        }
      ],
      "step": 9
    },
    {
      "question": "Extract ablation table entries and experimental settings per ablation: record numerical results for each ablation row, and explicitly document which hyperparameters were held fixed vs varied. If ablations are only qualitative, mark as such and copy any reported qualitative conclusions.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.23629074213478024
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.23397571570219294
        },
        {
          "paper_id": "arxiv:2509.11782v1",
          "chunk_id": "arxiv:2509.11782v1:summary",
          "section": "Summary",
          "score": 0.23144535126623492
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.22523257317976783
        }
      ],
      "step": 9
    },
    {
      "question": "Code & checkpoint availability: list GitHub/Zenodo/other links and reported branch/commit and license (if provided). If no link is reported, return 'no link reported' and propose 2–3 likely search queries (repo name keywords, paper title tokens, author GitHub handles) to find the code.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.07887v1",
          "chunk_id": "arxiv:2509.07887v1:summary",
          "section": "Summary",
          "score": 0.23334245774897044
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.21306912199651892
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.18767392383739795
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.16946150079884723
        }
      ],
      "step": 9
    },
    {
      "question": "Randomness & reproducibility reporting: number of runs per experiment (mean/std) or single-run; any deterministic flags or seed-setting instructions; whether results averaged across seeds and what variance reported. Mark missing or underreported items.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.24882115119744264
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.2387392184969823
        },
        {
          "paper_id": "arxiv:2509.11782v1",
          "chunk_id": "arxiv:2509.11782v1:summary",
          "section": "Summary",
          "score": 0.23445856442517307
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.22436140299646617
        }
      ],
      "step": 9
    },
    {
      "question": "Cross-paper textual summary (compact table): rank reproducibility items by how often they are missing (e.g., GPU hours, exact LR schedule, seeds, total params). For each high-frequency missing item propose prioritized follow-ups: (1) immediate — find code & exact hyperparams, (2) medium — clarify dataset splits & preprocessing, (3) optional — request hardware/time estimates and exact seeds.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.32038550716788916
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.29865192549484065
        },
        {
          "paper_id": "arxiv:2509.11782v1",
          "chunk_id": "arxiv:2509.11782v1:summary",
          "section": "Summary",
          "score": 0.29269828497488415
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.2885842568197356
        }
      ],
      "step": 9
    },
    {
      "question": "Identify the top-3 empirically strongest papers (based on our earlier performance summaries). For each selected paper produce a minimal reproduction plan: (a) repository/branch search hints to check first, (b) assumed hyperparameters (conservative defaults) for any missing items, (c) estimated compute budget (GPU type and approximate hours), and (d) minimal experiment protocol (dataset split to use, 3 seeds to run, metrics to report, checkpointing schedule).",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.3501450771053286
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.34465832211250735
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.33289407919258074
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.3061477593223948
        }
      ],
      "step": 9
    },
    {
      "question": "Prepare a short template email / GitHub issue text (<=200 words) we can paste when contacting authors requesting missing reproducibility details: include a concise checklist (hyperparams, seeds, code/checkpoints, dataset splits, compute/hardware used) and polite closing asking for links or supplementary files.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.23549264392395647
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.23058293796897716
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.18711419315789268
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.18433435877743057
        }
      ],
      "step": 9
    },
    {
      "question": "Cross-paper comparison: produce a compact side-by-side summary (one-paragraph or bullet per paper) of architecture (message-passing / MPNN, SE(3)-equivariant, transformer), key inductive biases (edge/angle features, spherical harmonics, equivariance), input modalities (2D graph, 3D coordinates, conformers), parameter counts, primary tasks/datasets (QM9, OC20, MoleculeNet, etc.), and headline performance numbers (best metric and baseline).",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.48304011574987316
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.47881391069448304
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.4658481764714022
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.39598756225039133
        }
      ],
      "step": 10
    },
    {
      "question": "Reproducibility consolidation: merge per-paper checklists into a cross-paper matrix that flags which reproducibility items are present vs missing (optimizer, initial LR, LR schedule, batch size, epochs, weight decay, warmup, gradient clipping, augmentations, loss weights, seeds, init scheme, mixed precision, GPU hours). For each missing item, propose a conservative default and label it 'assume' vs 'request'.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.3001528945982185
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.2697020229799888
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.25932967224851483
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.2591490338412395
        }
      ],
      "step": 10
    },
    {
      "question": "Code & checkpoint collection: for each of the 10 known paper IDs produce (a) reported code/checkpoint link(s) if present with license/commit when available, (b) if no link reported provide 2–3 concrete repo-search queries (keywords, likely repo names, author GitHub handles) to locate the implementation, and (c) a quick plausibility score (high/medium/low) for whether that repo corresponds to the reported results.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.07887v1",
          "chunk_id": "arxiv:2509.07887v1:summary",
          "section": "Summary",
          "score": 0.2166401338945063
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.19064701866393696
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.18726158777566207
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.18674649169245156
        }
      ],
      "step": 10
    },
    {
      "question": "Prioritized follow-ups: output an actionable prioritized list (Immediate / Medium / Optional) for missing reproducibility items across the corpus—Immediate = find code & exact hyperparams, Medium = clarify dataset splits & preprocessing, Optional = request hardware/time estimates and seed dumps. For Immediate items give the exact next search action (repo search query or author contact).",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.2150430695211953
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.21051544962741445
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.20543943025741798
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.19946724926592954
        }
      ],
      "step": 10
    },
    {
      "question": "Minimal reproduction plans (concise templates): for the top-3 empirically strongest papers produce a one-paragraph minimal reproduction protocol each covering: repo/branch search hints, assumed hyperparams (if missing), compute estimate (GPU type & hours), dataset split to use, 3 seeds, metrics to report, checkpointing schedule, and expected runtime per seed.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.33952214781714
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.33597669586200346
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.29822937356049467
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.29652109865137777
        }
      ],
      "step": 10
    },
    {
      "question": "Final recommendation sketch (outline): based on the synthesis, recommend (a) model family choice(s) for molecule property prediction vs force-field/energy tasks, (b) pretraining strategies to try (masked node/edge, denoising, contrastive, equivariant denoising), (c) a prioritized evaluation plan (benchmarks, metrics, baselines), and (d) a conservative compute/hardware plan for initial experiments.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.549149825654688
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.537834715603493
        },
        {
          "paper_id": "arxiv:2509.17018v1",
          "chunk_id": "arxiv:2509.17018v1:summary",
          "section": "Summary",
          "score": 0.49435311097064966
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.4896122668772729
        }
      ],
      "step": 10
    },
    {
      "question": "Produce a compact per-paper bullet (1 para or 4 bullets) for the 8 known papers (ids in state) covering: model family/architecture, key inductive biases (equivariance, angles, spherical harmonics, edge features), input modality (2D graph, 3D coords, conformers), parameter count (if available), primary tasks/datasets and top reported metrics.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.3708965339537107
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.3622337160034723
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.34974690583837864
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.30341238247021485
        }
      ],
      "step": 11
    },
    {
      "question": "Merge per-paper reproducibility checklists into a single cross-paper matrix that explicitly lists: optimizer, initial LR, LR schedule, batch size, epochs, weight decay, warmup, gradient clipping, augmentations, loss weights, seeds, init scheme, mixed precision, GPU hours. For each missing item propose a conservative default and label it 'assume' vs 'request'.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.29196839435693805
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.28509836754406354
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.25547567125783094
        },
        {
          "paper_id": "arxiv:2509.17018v1",
          "chunk_id": "arxiv:2509.17018v1:summary",
          "section": "Summary",
          "score": 0.25248995341905617
        }
      ],
      "step": 11
    },
    {
      "question": "For each of the 8 papers provide: (a) reported code/checkpoint link(s) with license/commit when present, (b) for missing links 2–3 repo-search queries (keywords, likely repo names, author GitHub handles), and (c) a plausibility score (high/medium/low) that a found repo matches reported results.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.07887v1",
          "chunk_id": "arxiv:2509.07887v1:summary",
          "section": "Summary",
          "score": 0.2422063453691486
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.18361426329796224
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.17895525982001356
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.17704365619504392
        }
      ],
      "step": 11
    },
    {
      "question": "Consolidate the prioritized follow-ups (Immediate / Medium / Optional) into a one-page action plan. For Immediate items include exact next search action (repo search query or author contact template) and target person (author handle if known).",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.15705220646654144
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.13759366649682359
        },
        {
          "paper_id": "arxiv:2509.07887v1",
          "chunk_id": "arxiv:2509.07887v1:summary",
          "section": "Summary",
          "score": 0.11550188855787592
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.10845686098286171
        }
      ],
      "step": 11
    },
    {
      "question": "Deliver minimal reproduction protocols (one paragraph each) for the top-3 empirically strongest papers: repo/branch hints, assumed hyperparams (with 'assume' vs 'request' tags), compute estimate (GPU type & hours), dataset split/preprocessing, seeds, metrics to report, checkpointing schedule, runtime per seed.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.35126897498415643
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.33373032904838323
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.3119414702898184
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.29812748048660576
        }
      ],
      "step": 11
    },
    {
      "question": "Produce a short final recommendation sketch (3–5 bullets) covering: model family(s) to prioritize for property prediction vs energy/force tasks, 2–3 pretraining strategies to try (with brief justification), prioritized evaluation plan (benchmarks, metrics, baselines), and a conservative compute/hardware plan for initial experiments (per-experiment GPU & hours).",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.40644144516216857
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.3926112287245771
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.388653751331995
        },
        {
          "paper_id": "arxiv:2509.17018v1",
          "chunk_id": "arxiv:2509.17018v1:summary",
          "section": "Summary",
          "score": 0.38066674093378144
        }
      ],
      "step": 11
    },
    {
      "question": "Per-paper compact bullets (1 paragraph or 4 bullets) for each known paper id: architecture (MPNN / SE(3)-equivariant / Transformer), key inductive biases (edge/angle features, spherical harmonics, equivariance), input modality (2D graph, 3D coords, number of conformers), parameter count (or estimate), primary tasks/datasets, top reported metrics, code/checkpoint link or repo-search hints, and a short reproducibility checklist summary (optimizer, init LR, schedule, batch, epochs, weight decay, warmup, grad clipping, augmentations, loss weights, seeds, mixed precision, GPU hours) with each missing item tagged 'assume' or 'request'.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.3916938226220219
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.37347604125840955
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.35899476308167266
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.3237055680682533
        }
      ],
      "step": 12
    },
    {
      "question": "Merge per-paper reproducibility checklists into a single cross-paper matrix: list each hyper/exp item as columns, fill reported values, and for missing entries propose a conservative default and tag 'assume' vs 'request'. Produce short justification for each chosen default (1 line).",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.2624929601825729
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.25572624785290127
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.24413215916396896
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.23615548272027534
        }
      ],
      "step": 12
    },
    {
      "question": "For papers lacking code/checkpoints: for each paper provide 2–3 targeted repo-search queries (keywords, likely repo name patterns, and probable author GitHub handles) and give a plausibility score (high/medium/low) that an identified repo matches reported results.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.07887v1",
          "chunk_id": "arxiv:2509.07887v1:summary",
          "section": "Summary",
          "score": 0.2522751949434321
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.21161890860972318
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.203361495160886
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.1881739915998278
        }
      ],
      "step": 12
    },
    {
      "question": "Minimal reproduction protocol (one paragraph each) for the top-3 empirically strongest papers: include suggested repo/branch hints, assumed hyperparams with 'assume'/'request' tags, dataset splits/preprocessing, seeds, metrics to report, checkpointing schedule, estimated compute (GPU type & hours) and runtime per seed.",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.37259315215063105
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.33627199555461784
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.32438488559849205
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.31901155386456886
        }
      ],
      "step": 12
    },
    {
      "question": "Short final recommendation sketch (3–5 bullets) covering: model family(s) to prioritize for property prediction vs energy/force tasks, 2–3 pretraining strategies to try (with brief justification), prioritized evaluation plan (benchmarks, metrics, baselines), and a conservative compute/hardware plan for initial experiments (per-experiment GPU & hours).",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.4281008264950187
        },
        {
          "paper_id": "arxiv:2506.15792v1",
          "chunk_id": "arxiv:2506.15792v1:summary",
          "section": "Summary",
          "score": 0.4246482383568097
        },
        {
          "paper_id": "arxiv:2509.17018v1",
          "chunk_id": "arxiv:2509.17018v1:summary",
          "section": "Summary",
          "score": 0.40863943016507737
        },
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.39623489139217943
        }
      ],
      "step": 12
    },
    {
      "question": "Prioritized follow-ups (Immediate / Medium / Optional): for each Immediate item include exact next search action (repo search query or author contact template) and a target person (author GitHub handle or contact if known).",
      "answer": "No answer returned.",
      "citations": [
        {
          "paper_id": "arxiv:2410.07981v2",
          "chunk_id": "arxiv:2410.07981v2:summary",
          "section": "Summary",
          "score": 0.14792125060662795
        },
        {
          "paper_id": "arxiv:2509.10871v1",
          "chunk_id": "arxiv:2509.10871v1:summary",
          "section": "Summary",
          "score": 0.14216091786926643
        },
        {
          "paper_id": "arxiv:2509.22028v1",
          "chunk_id": "arxiv:2509.22028v1:summary",
          "section": "Summary",
          "score": 0.1398968918685766
        },
        {
          "paper_id": "arxiv:2509.07887v1",
          "chunk_id": "arxiv:2509.07887v1:summary",
          "section": "Summary",
          "score": 0.13032983146955035
        }
      ],
      "step": 12
    }
  ],
  "reading_list": [
    {
      "paper_id": "arxiv:2510.07910v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2509.22468v1",
      "reason": "read"
    },
    {
      "paper_id": "arxiv:2509.22028v1",
      "reason": "read"
    },
    {
      "paper_id": "arxiv:2509.17018v1",
      "reason": "read"
    },
    {
      "paper_id": "arxiv:2509.11782v1",
      "reason": "read"
    },
    {
      "paper_id": "arxiv:2509.07887v1",
      "reason": "read"
    },
    {
      "paper_id": "arxiv:2503.07397v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2501.18876v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2412.01982v2",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2410.16975v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2410.07981v2",
      "reason": "read"
    },
    {
      "paper_id": "arxiv:2409.18070v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2510.11286v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2510.10954v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2510.10775v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2510.10341v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2510.10327v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2510.10116v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2509.10871v1",
      "reason": "read"
    },
    {
      "paper_id": "arxiv:2506.15792v1",
      "reason": "read"
    },
    {
      "paper_id": "arxiv:2505.18728v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2503.07378v5",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2508.20527v2",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2508.15015v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2508.07807v2",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2507.03474v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2507.03430v2",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2506.19862v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2410.18676v2",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2510.09854v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2510.08966v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2510.08450v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2510.07285v1",
      "reason": "discovered"
    },
    {
      "paper_id": "arxiv:2510.05995v2",
      "reason": "discovered"
    }
  ],
  "warnings": []
}